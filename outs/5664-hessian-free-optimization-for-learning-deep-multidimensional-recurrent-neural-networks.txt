Hessian-free Optimization for Learning

Deep Multidimensional Recurrent Neural Networks

Minhyung Cho

Chandra Shekhar Dhir

Jaehyung Lee

{mhyung.cho,shekhardhir}@gmail.com

Applied Research Korea, Gracenote Inc.

jaehyung.lee@kaist.ac.kr

Abstract

Multidimensional recurrent neural networks (MDRNNs) have shown a remark-
able performance in the area of speech and handwriting recognition. The perfor-
mance of an MDRNN is improved by further increasing its depth, and the dif-
ﬁculty of learning the deeper network is overcome by using Hessian-free (HF)
optimization. Given that connectionist temporal classiﬁcation (CTC) is utilized as
an objective of learning an MDRNN for sequence labeling, the non-convexity of
CTC poses a problem when applying HF to the network. As a solution, a convex
approximation of CTC is formulated and its relationship with the EM algorithm
and the Fisher information matrix is discussed. An MDRNN up to a depth of 15
layers is successfully trained using HF, resulting in an improved performance for
sequence labeling.

1

Introduction

Multidimensional recurrent neural networks (MDRNNs) constitute an efﬁcient architecture for
building a multidimensional context into recurrent neural networks [1]. End-to-end training of
MDRNNs in conjunction with connectionist temporal classiﬁcation (CTC) has been shown to
achieve a state-of-the-art performance in on/off-line handwriting and speech recognition [2, 3, 4].
In previous approaches, the performance of MDRNNs having a depth of up to ﬁve layers, which
is limited as compared to the recent progress in feedforward networks [5], was demonstrated. The
effectiveness of MDRNNs deeper than ﬁve layers has thus far been unknown.
Training a deep architecture has always been a challenging topic in machine learning. A notable
breakthrough was achieved when deep feedforward neural networks were initialized using layer-
wise pre-training [6]. Recently, approaches have been proposed in which supervision is added to
intermediate layers to train deep networks [5, 7]. To the best of our knowledge, no such pre-training
or bootstrapping method has been developed for MDRNNs.
Alternatively, Hesssian-free (HF) optimization is an appealing approach to training deep neural
networks because of its ability to overcome pathological curvature of the objective function [8].
Furthermore, it can be applied to any connectionist model provided that its objective function is
differentiable. The recent success of HF for deep feedforward and recurrent neural networks [8, 9]
supports its application to MDRNNs.
In this paper, we claim that an MDRNN can beneﬁt from a deeper architecture, and the application of
second order optimization such as HF allows its successful learning. First, we offer details of the de-
velopment of HF optimization for MDRNNs. Then, to apply HF optimization for sequence labeling
tasks, we address the problem of the non-convexity of CTC, and formulate a convex approximation.
In addition, its relationship with the EM algorithm and the Fisher information matrix is discussed.
Experimental results for ofﬂine handwriting and phoneme recognition show that an MDRNN with
HF optimization performs better as the depth of the network increases up to 15 layers.

1

2 Multidimensional recurrent neural networks

MDRNNs constitute a generalization of RNNs to process multidimensional data by replacing the
single recurrent connection with as many connections as the dimensions of the data [1]. The network
can access the contextual information from 2N directions, allowing a collective decision to be made
based on rich context information. To enhance its ability to exploit context information, long short-
term memory (LSTM) [10] cells are usually utilized as hidden units. In addition, stacking MDRNNs
to construct deeper networks further improves the performance as the depth increases, achieving the
state-of-the-art performance in phoneme recognition [4]. For sequence labeling, CTC is applied as
a loss function of the MDRNN. The important advantage of using CTC is that no pre-segmented
sequences are required, and the entire transcription of the input sample is sufﬁcient.

2.1 Learning MDRNNs

A d-dimensional MDRNN with M inputs and K outputs is regarded as a mapping from an input
sequence x ∈ RM×T1×···×Td to an output sequence a ∈ (RK)T of length T , where the input data for
M input neurons are given by the vectorization of d-dimensional data, and T1, . . . , Td is the length
of the sequence in each dimension. All learnable weights and biases are concatenated to obtain a
parameter vector θ ∈ RN . In the learning phase with ﬁxed training data, the MDRNN is formalized
as a mapping N : RN → (RK)T from the parameters θ to the output sequence a, i.e., a = N (θ).
The scalar loss function is deﬁned over the output sequence as L : (RK)T → R. Learning an
MDRNN is viewed as an optimization of the objective L(N (θ)) = L ◦ N (θ) with respect to θ.
The Jacobian JF of a function F : Rm → Rn is the n × m matrix where each element is a partial
derivative of an element of output with respect to an element of input. The Hessian HF of a scalar
function F : Rm → R is the m × m matrix of second-order partial derivatives of the output with
respect to its inputs. Throughout this paper, a vector sequence is denoted by boldface a, a vector at
time t in a is denoted by at, and the k-th element of at is denoted by at
k.

3 Hessian-free optimization for MDRNNs

The application of HF optimization to an MDRNN is straightforward if the matching loss func-
tion [11] for its output layer is adopted. However, this is not the case for CTC, which is necessarily
adopted for sequence labeling. Before developing an appropriate approximation to CTC that is com-
patible with HF optimization, we discuss two considerations related to the approximation. The ﬁrst
is obtaining a quadratic approximation of the loss function, and the second is the efﬁcient calculation
of the matrix-vector product used at each iteration of the conjugate gradient (CG) method.
HF optimization minimizes an objective by constructing a local quadratic approximation for the
objective function and minimizing the approximate function instead of the original one. The loss
function L(θ) needs to be approximated at each point θn of the n-th iteration:

δ(cid:62)
n Gδn,

1
2

Qn(θ) = L(θn) + ∇θL|(cid:62)

θn

δn +

(1)
where δn = θ − θn is the search direction, i.e., the parameters of the optimization, and G is a
local approximation to the curvature of L(θ) at θn, which is typically obtained by the generalized
Gauss-Newton (GGN) matrix as an approximation of the Hessian.
HF optimization uses the CG method in a subroutine to minimize the quadratic objective above for
utilizing the complete curvature information and achieving computational efﬁciency. CG requires
the computation of Gv for an arbitrary vector v, but not the explicit evaluation of G. For neural
networks, an efﬁcient way to compute Gv was proposed in [11], extending the study in [12]. In
section 3.2, we provide the details of the efﬁcient computation of Gv for MDRNNs.

3.1 Quadratic approximation of loss function
The Hessian matrix, HL◦N , of the objective L (N (θ)) is written as

HL◦N = J(cid:62)

N HLJN +

[JL]iH[N ]i,

(2)

KT(cid:88)

i=1

2

where JN ∈ RKT×N , HL ∈ RKT×KT , and [q]i denotes the i-th component of the vector q.
An indeﬁnite Hessian matrix is problematic for second-order optimization, because it deﬁnes an
unbounded local quadratic approximation [13]. For nonlinear systems, the Hessian is not necessarily
positive semideﬁnite, and thus, the GGN matrix is used as an approximation of the Hessian [11, 8].
The GGN matrix is obtained by ignoring the second term in Eq. (2), as given by

GL◦N = J(cid:62)

N HLJN .

(3)
The sufﬁcient condition for the GGN approximation to be exact is that the network makes a perfect
prediction for every given sample, that is, JL = 0, or [N ]i stays in the linear region for all i, that is,
H[N ]i = 0.
GL◦N has less rank than KT and is positive semideﬁnite provided that HL is. Thus, L is chosen to
be a convex function so that HL is positive semideﬁnite. In principle, it is best to deﬁne L and N
such that L performs as much of the computation as possible, with the positive semideﬁniteness of
HL as a minimum requirement [13]. In practice, a nonlinear output layer together with its matching
loss function [11], such as the softmax function with cross-entropy loss, is widely used.

3.2 Computation of matrix-vector product for MDRNN
The product of an arbitrary vector v by the GGN matrix, Gv = J(cid:62)
N HLJN v, amounts to the se-
quential multiplication of v by three matrices. First, the product JN v is a Jacobian times vector
and is therefore equal to the directional derivative of N (θ) along the direction of v. Thus, JN v can
be written using a differential operator JN v = Rv(N (θ)) [12] and the properties of the operator
can be utilized for efﬁcient computation. Because an MDRNN is a composition of differentiable
components, the computation of Rv(N (θ)) throughout the whole network can be accomplished by
repeatedly applying the sum, product, and chain rules starting from the input layer. The detailed
derivation of the R operator to LSTM, normally used as a hidden unit in MDRNNs, is provided in
appendix A.
Next, the multiplication of JN v by HL can be performed by direct computation. The dimension
of HL could at ﬁrst appear problematic, since the dimension of the output vector used by the loss
function L can be as high as KT , in particular, if CTC is adopted as an objective for the MDRNN.
If the loss function can be expressed as the sum of individual loss functions with a domain restricted
in time, the computation can be reduced signiﬁcantly. For example, with the commonly used cross-
entropy loss function, the KT × KT matrix HL can be transformed into a block diagonal matrix
with T blocks of a K × K Hessian matrix. Let HL,t be the t-th block in HL. Then, the GGN matrix
can be written as

(cid:88)

GL◦N =

J(cid:62)
Nt

HL,tJNt,

(4)

where JNt is the Jacobian of the network at time t.
Finally, the multiplication of a vector u = HLJN v by the matrix J(cid:62)
propagation through time algorithm by propagating u instead of the error at the output layer.

N is calculated using the back-

t

4 Convex approximation of CTC for application to HF optimization

Connectioninst temporal classiﬁcation (CTC) [14] provides an objective function of learning an
MDRNN for sequence labeling. In this section, we derive a convex approximation of CTC inspired
by the GGN approximation according to the following steps. First, the non-convex part of the
original objective is separated out by reformulating the softmax part. Next, the remaining convex
part is approximated without altering its Hessian, making it well matched to the non-convex part.
Finally, the convex approximation is obtained by reuniting the convex and non-convex parts.

4.1 Connectionist temporal classiﬁcation
CTC is formulated as the mapping from an output sequence of the recurrent network, a ∈ (RK)T ,
to a scalar loss. The output activations at time t are normalized using the softmax function

(cid:80)

yt
k =

exp(at
k)
k(cid:48) exp(at

k(cid:48))

,

3

(5)

k is the probability of label k given a at time t.

where yt
The conditional probability of the path π is calculated by the multiplication of the label probabilities
at each timestep, as given by

p(π|a) =

yt
πt

,

(6)

T(cid:89)

t=1

(cid:88)

π∈B−1(l)

where πt is the label observed at time t along the path π. The path π of length T is mapped to a
label sequence of length M ≤ T by an operator B, which removes the repeated labels and then
the blanks. Several mutually exclusive paths can map to the same label sequence. Let S be a set
containing every possible sequence mapped by B, that is, S = {s|s ∈ B(π) for some π} is the
image of B, and let |S| denote the cardinality of the set.
The conditional probability of a label sequence l is given by

p(l|a) =

p(π|a),

(7)

which is the sum of probabilities of all the paths mapped to a label sequence l by B.
The cross-entropy loss assigns a negative log probability to the correct answer. Given a target
sequence z, the loss function of CTC for the sample is written as

L(a) = − log p(z|a).

(8)

From the description above, CTC is composed of the sum of the product of softmax components.
The function − log(yt
k), corresponding to the softmax with cross-entropy loss, is convex [11].
Therefore, yt
k is log-concave. Whereas log-concavity is closed under multiplication, the sum of
log-concave functions is not log-concave in general [15]. As a result, the CTC objective is not
convex in general because it contains the sum of softmax components in Eq. (7).

4.2 Reformulation of CTC objective function

We reformulate the CTC objective Eq. (8) to separate out the terms that are responsible for the non-
convexity of the function. By reformulation, the softmax function is deﬁned over the categorical
label sequences.
By substituting Eq. (5) into Eq. (6), it follows that

where bπ =(cid:80)

as

p(π|a) =

exp(bπ)

(9)
πt. By substituting Eq. (9) into Eq. (7) and setting l = z, p(z|a) can be re-written

π(cid:48)∈all exp(bπ(cid:48))

,

t at

(cid:80)

(cid:80)
(cid:80)

p(z|a) =

π∈B−1(z) exp(bπ)

π∈all exp(bπ)

(cid:80)

=

exp(fz)
z(cid:48)∈S exp(fz(cid:48))

,

(cid:16)(cid:80)

π∈B−1(z) exp(bπ)

is the log-
where S is the set of every possible label sequence and fz = log
sum-exp function1, which is proportional to the probability of observing the label sequence z among
all the other label sequences.
With the reformulation above, the CTC objective can be regarded as the cross-entropy loss with the
softmax output, which is deﬁned over all the possible label sequences. Because the cross-entropy
loss function matches the softmax output layer [11], the CTC objective is convex, except the part
that computes fz for each of the label sequences. At this point, an obvious candidate for the convex
approximation of CTC is the GGN matrix separating the convex and non-convex parts.
Let the non-convex part be Nc and the convex part be Lc. The mapping Nc : (RK)T → R|S| is
deﬁned by

Nc(a) = F = [fz1, . . . , fz|S| ](cid:62),

(11)

(10)

(cid:17)

1f (x1, . . . , xn) = log(ex1 + ··· + exn ) is the log-sum-exp function deﬁned on Rn

4

where fz is given above, and |S| is the number of all the possible label sequences. For given F as
above, the mapping Lc : R|S| → R is deﬁned by

(cid:33)

(cid:32)(cid:88)

z(cid:48)∈S

(cid:80)

Lc(F ) = − log

exp(fz)
z(cid:48)∈S exp(fz(cid:48))

= −fz + log

exp(fz(cid:48))

,

(12)

where z is the label sequence corresponding to a. The ﬁnal reformulation for the loss function of
CTC is given by

L(a) = Lc ◦ Nc(a).

(13)

4.3 Convex approximation of CTC loss function

The GGN approximation of Eq. (13) immediately gives a convex approximation of the Hessian for
CTC as GLc◦Nc = J(cid:62)
HLcJNc. Although HLc has the form of a diagonal matrix plus a rank-1
Nc
matrix, i.e., diag(Y ) − Y Y (cid:62), the dimension of HLc is |S| × |S|, where |S| becomes exponentially
large as the length of the sequence increases. This makes the practical calculation of HLc difﬁcult.
On the other hand, removing the linear team −fz from Lc(F ) in Eq. (12) does not alter its Hessian.
and M = Lp ◦ Nc are the same, i.e., GLc◦Nc = GLp◦Nc. Therefore, their Hessian matrices are
approximations of each other. The condition that the two Hessian matrices, HL and HM, converges
to the same matrix is discussed below.

The resulting formula is Lp(F ) = log(cid:0)(cid:80)
Interestingly, M is given as a compact formula M(a) = Lp ◦ Nc(a) =(cid:80)

z(cid:48)∈S exp(fz(cid:48))(cid:1). The GGN matrices of L = Lc ◦ Nc

t log(cid:80)

k), where
k is the output unit k at time t. Its Hessian HM can be directly computed, resulting in a block
at
diagonal matrix. Each block is restricted in time, and the t-th block is given by

k exp(at

1, . . . , yt

HM,t = diag(Y t) − Y tY t(cid:62)
(14)
k is given in Eq. (5). Because the Hessian of each block is positive
where Y t = [yt
semideﬁnite, HM is positive semideﬁnite. A convex approximation of the Hessian of an MDRNN
using the CTC objective can be obtained by substituting HM for HL in Eq. (3). Note that the
resulting matrix is block diagonal and Eq. (4) can be utilized for efﬁcient computation.
Our derivation can be summarized as follows:

K](cid:62) and yt

,

1. HL = HLc◦Nc is not positive semideﬁnite.
2. GLc◦Nc = GLp◦Nc is positive semideﬁnite, but not computationally tractable.
3. HLp◦Nc is positive semideﬁnite and computationally tractable.

4.4 Sufﬁcient condition for the proposed approximation to be exact

From Eq. (2), the condition HLc◦Nc = HLp◦Nc holds if and only if (cid:80)KT
(cid:80)KT
i=1[JLc]iH[Nc]i =
i=1[JLp ]iH[Nc]i. Since JLc (cid:54)= JLp in general, we consider only the case of H[Nc]i = 0 for
all i, which corresponds to the case where Nc is a linear mapping.
[Nc]i contains a log-sum-exp function mapping from paths to a label sequence. Let l be the label
sequence corresponding to [Nc]i; then, [Nc]i = fl(. . . , bπ, . . . ) for π ∈ B−1(l). If the probability
of one path π(cid:48) is sufﬁciently large to ignore all the other paths, that is, exp(bπ(cid:48)) (cid:29) exp(bπ) for
π ∈ {B−1(l)\π(cid:48)}, it follows that fl(. . . , bπ(cid:48), . . . ) = bπ(cid:48). This is a linear mapping, which results in
H[Nc]i = 0.
In conclusion, the condition HLc◦Nc = HLp◦Nc holds if one dominant path π ∈ B−1(l) exists such
that fl(. . . , bπ, . . . ) = bπ for each label sequence l.

4.5 Derivation of the proposed approximation from the Fisher information matrix

The identity of the GGN and the Fisher information matrix [16] has been shown for the network
using the softmax with cross-entropy loss [17, 18]. Thus, it follows that the GGN matrix of Eq. (13)
is identical to the Fisher information matrix. Now, we show that the proposed matrix in Eq. (14)

5

is derived from the Fisher information matrix under the condition given in section 4.4. The Fisher
information matrix of an MDRNN using CTC is written as

(cid:34)(cid:18) ∂ log p(l|a)

(cid:19)(cid:62)(cid:18) ∂ log p(l|a)

(cid:19)(cid:35)

(cid:35)

(cid:34)

F = Ex

J(cid:62)
N El∼p(l|a)

∂a

∂a

JN

,

(15)

where a = a(x, θ) is the KT -dimensional output of the network N . CTC assumes output proba-
bilities at each timestep to be independent of those at other timesteps [1], and therefore, its Fisher
information matrix is given as the sum of every timestep. It follows that

(cid:34)(cid:88)

t

F = Ex

J(cid:62)
Nt

El∼p(l|a)

(cid:34)(cid:18) ∂ log p(l|a)

∂at

(cid:19)(cid:35)

(cid:35)

(cid:19)(cid:62)(cid:18) ∂ log p(l|a)
(cid:35)

∂at

(cid:34)(cid:88)

JNt

.

(16)

Under the condition in section 4.4, the Fisher information matrix is given by

F = Ex

J(cid:62)
Nt

(diag(Y t) − Y tY t(cid:62)

)JNt

,

(17)

which is the same form as Eqs. (4) and (14) combined. See appendix B for the detailed derivation.

t

4.6 EM interpretation of the proposed approximation

The goal of the Expectation-Maximization (EM) algorithm is to ﬁnd the maximum likelihood so-
lution for models having latent variables [19]. Given an input sequence x, and its corresponding
π∈B−1(z) p(π|x, θ),
where θ represents the model parameters. For each observation x, we have a corresponding latent
variable q which is a 1-of-k binary vector where k is the number of all the paths mapped to z. The
π∈B−1(z) qπ|x,z log p(π|x, θ). The
EM algorithm starts with an initial parameter ˆθ, and repeats the following process until convergence.

target label sequence z, the log likelihood of z is given by log p(z|x, θ) = log(cid:80)
log likelihood can be written in terms of q as log p(z, q|x, θ) =(cid:80)
Maximization step updates: ˆθ = argmaxθQ(θ), where Q(θ) =(cid:80)
of π and(cid:80)

In the context of CTC and RNN, p(π|x, θ) is given as p(π|a(x, θ)) as in Eq. (6), where a(x, θ) is
the KT -dimensional output of the neural network. Taking the second-order derivative of log p(π|a)
with respect to at gives diag(Y t)−Y tY t(cid:62)
, with Y t as in Eq. (14). Because this term is independent

π∈B−1(z) γπ|x,z = 1, the Hessian of Q with respect to at is given by

π∈B−1(z) γπ|x,z log p(π|x, θ).

Expectation step calculates: γπ|x,z =

(cid:80)

p(π|x,ˆθ)

π∈B−1(z) p(π|x,ˆθ)

.

HQ,t = diag(Y t) − Y tY t(cid:62)

,

(18)

which is the same as the convex approximation in Eq. (14).

5 Experiments

In this section, we present the experimental results for two different sequence labeling tasks, ofﬂine
handwriting recognition and phoneme recognition. The performance of Hessian-free optimization
for MDRNNs with the proposed matrix is compared with that of stochastic gradient descent (SGD)
optimization on the same settings.

5.1 Database and preprocessing

The IFN/ENIT Database [20] is a database of handwritten Arabic words, which consists of 32,492
images. The entire dataset has ﬁve subsets (a, b, c, d, e). The 25,955 images corresponding to the
subsets (b − e) were used for training. The validation set consisted of 3,269 images corresponding
to the ﬁrst half of the sorted list in alphabetical order (ae07 001.tif − ai54 028.tif) in set a. The
remaining images in set a, amounting to 3,268, were used for the test. The intensity of pixels was
centered and scaled using the mean and standard deviation calculated from the training set.

6

The TIMIT corpus [21] is a benchmark database for evaluating speech recognition performance.
The standard training, validation, and core datasets were used. Each set contains 3,696 sentences,
400 sentences, and 192 sentences, respectively. A mel spectrum with 26 coefﬁcients was used as
a feature vector with a pre-emphasis ﬁlter, 25 ms window size, and 10 ms shift size. Each input
feature was centered and scaled using the mean and standard deviation of the training set.

5.2 Experimental setup

For handwriting recognition, the basic architecture was adopted from that proposed in [3]. Deeper
networks were constructed by replacing the top layer with more layers. The number of LSTM cells
in the augmented layer was chosen such that the total number of weights between the different
networks was similar. The detailed architectures are described in Table 1, together with the results.
For phoneme recognition, the deep bidirectional LSTM and CTC in [4] was adopted as the basic
architecture. In addition, the memory cell block [10], in which the cells share the gates, was applied
for efﬁcient information sharing. Each LSTM block was constrained to have 10 memory cells.
According to the results, using a large value of bias for input/output gates is beneﬁcial for training
deep MDRNNs. A possible explanation is that the activation of neurons is exponentially decayed
by input/output gates during the propagation. Thus, setting large bias values for these gates may
facilitate the transmission of information through many layers at the beginning of the learning. For
this reason, the biases of the input and output gates were initialized to 2, whereas those of the forget
gates and memory cells were initialized to 0. All the other weight parameters of the MDRNN were
initialized randomly from a uniform distribution in the range [−0.1, 0.1].
The label error rate was used as the metric for performance evaluation, together with the average
loss of CTC in Eq. (8). It is deﬁned by the edit distance, which sums the total number of insertions,
deletions, and substitutions required to match two given sequences. The ﬁnal performance, shown
in Tables 1 and 2, was evaluated using the weight parameters that gave the best label error rate on
the validation set. To map output probabilities to a label sequence, best path decoding [1] was used
for handwriting recognition and beam search decoding [4, 22] with a beam width of 100 was used
for phoneme recognition. For phoneme recognition, 61 phoneme labels were used during training
and decoding, and then, mapped to 39 classes for calculating the phoneme error rate (PER) [4, 23].
For phoneme recognition, the regularization method suggested in [24] was used. We applied Gaus-
sian weight noise of standard deviation σ = {0.03, 0.04, 0.05} together with L2 regularization of
strength 0.001. The network was ﬁrst trained without noise, and then, it was initialized to the weights
that gave the lowest CTC loss on the validation set. Then, the network was retrained with Gaussian
weight noise [4]. Table 2 presents the best result for different values of σ.

5.2.1 Parameters

For HF optimization, we followed the basic setup described in [8], but different parameters were
utilized. Tikhonov damping was used together with Levenberg-Marquardt heuristics. The value of
the damping parameter λ was initialized to 0.1, and adjusted according to the reduction ratio ρ (mul-
tiplied by 0.9 if ρ > 0.75, divided by 0.9 if ρ < 0.25, and unchanged otherwise). The initial search
direction for each run of CG was set to the CG direction found by the previous HF optimization
iteration decayed by 0.7. To ensure that CG followed the descent direction, we continued to perform
a minimum 5 and maximum 30 of additional CG iterations after it found the ﬁrst descent direction.
We terminated CG at iteration i before reaching the maximum iteration if the following condition
was satisﬁed: (φ(xi) − φ(xi−5))/φ(xi) < 0.005 , where φ is the quadratic objective of CG with-
out offset. The training data were divided into 100 and 50 mini-batches for the handwriting and
phoneme recognition experiments, respectively, and used for both the gradient and matrix-vector
product calculation. The learning was stopped if any of two criteria did not improve for 20 epochs
and 10 epochs in handwriting and phoneme recognition, respectively.
For SGD optimization, the learning rate  was chosen from {10−4, 10−5, 10−6}, and the momentum
µ from {0.9, 0.95, 0.99}. For handwriting recognition, the best performance obtained using all the
possible combinations of parameters is presented in Table 1. For phoneme recognition, the best
parameters out of nine candidates for each network were selected after training without weight noise
based on the CTC loss. Additionally, the backpropagated error in LSTM layer was clipped to remain

7

in the range [−1, 1] for stable learning [25]. The learning was stopped after 1000 epochs had been
processed, and the ﬁnal performance was evaluated using the weight parameters that showed the best
label error rate on the validation set. It should be noted that in order to guarantee the convergence,
we selected a conservative criterion as compared to the study where the network converged after 85
epochs in handwriting recognition [3] and after 55-150 epochs in phoneme recognition [4].

5.3 Results

Table 1 presents the label error rate on the test set for handwriting recognition. In all cases, the
networks trained using HF optimization outperformed those using SGD. The advantage of using HF
is more pronounced as the depth increases. The improvements resulting from the deeper architecture
can be seen with the error rate dropping from 6.1% to 4.5% as the depth increases from 3 to 13.
Table 2 shows the phoneme error rate (PER) on the core set for phoneme recognition. The improved
performance according to the depth can be observed for both optimization methods. The best PER
for HF optimization is 18.54% at 15 layers and that for SGD is 18.46% at 10 layers, which are
comparable to that reported in [4], where the reported results are a PER of 18.6% from a network
with 3 layers having 3.8 million weights and a PER of 18.4% from a network with 5 layers having
6.8 million weights. The beneﬁt of a deeper network is obvious in terms of the number of weight
parameters, although this is not intended to be a deﬁnitive performance comparison because of
the different preprocessing. The advantage of HF optimization is not prominent in the result of
the experiments using the TIMIT database. One explanation is that the networks tend to overﬁt
to a relatively small number of the training data samples, which removes the advantage of using
advanced optimization techniques.

Table 1: Experimental results for Arabic ofﬂine handwriting recognition. The label error rate is
presented with the different network depths. AB denotes a stack of B layers having A hidden
LSTM cells in each layer. “Epochs” is the number of epochs required by the network using HF
optimization so that the stopping criteria are fulﬁlled.  is the learning rate and µ is the momentum.

NETWORKS
2-10-50
2-10-213
2-10-146
2-10-128
2-10-1011
2-10-913

DEPTH WEIGHTS HF (%)

EPOCHS

SGD (%)

3
5
8
10
13
15

159,369
157,681
154,209
154,153
150,169
145,417

6.10
5.85
4.98
4.95
4.50
5.69

77
90
140
109
84
84

9.57
9.19
9.67
9.25
10.63
12.29

{, µ}
{10−4,0.9}
{10−5,0.99}
{10−4,0.95}
{10−4,0.95}
{10−4,0.9}
{10−5,0.99}

Table 2: Experimental results for phoneme recognition using the TIMIT corpus. PER is presented
with the different MDRNN architectures (depth × block × cell/block). σ is the standard deviation
of Gaussian weight noise. The remaining parameters are the same as in Table 1.

WEIGHTS HF (%)
20.14
771,542
19.18
795,752
19.09
720,826
755,822
18.79
18.59
806,588
741,230
18.54

EPOCHS

22
30
29
60
93
50

{σ}
{0.03}
{0.05}
{0.05}
{0.04}
{0.05}
{0.04}

3.8M
6.8M

SGD (%)

20.96
20.82
19.68
18.46
18.49
19.09
18.6
18.4

{, µ, σ}
{10−5, 0.99, 0.05 }
{10−4, 0.9, 0.04 }
{10−4, 0.9, 0.04 }
{10−5, 0.95, 0.04 }
{10−5, 0.95, 0.04 }
{10−5, 0.95, 0.03 }
{10−4, 0.9, 0.075 }
{10−4, 0.9, 0.075 }

NETWORKS
3 × 20 × 10
5 × 15 × 10
8 × 11 × 10
10 × 10 × 10
13 × 9 × 10
15 × 8 × 10
3 × 250 × 1†
5 × 250 × 1†

† The results were reported by Graves in 2013 [4].
6 Conclusion

Hessian-free optimization as an approach for successful learning of deep MDRNNs, in conjunction
with CTC, was presented. To apply HF optimization to CTC, a convex approximation of its objective
function was explored. In experiments, improvements in performance were seen as the depth of the
network increased for both HF and SGD. HF optimization showed a signiﬁcantly better performance
for handwriting recognition than did SGD, and a comparable performance for speech recognition.

8

References
[1] Alex Graves. Supervised sequence labelling with recurrent neural networks, volume 385. Springer, 2012.
[2] Alex Graves, Marcus Liwicki, Horst Bunke, J¨urgen Schmidhuber, and Santiago Fern´andez. Uncon-
strained on-line handwriting recognition with recurrent neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 577–584, 2008.

[3] Alex Graves and J¨urgen Schmidhuber. Ofﬂine handwriting recognition with multidimensional recurrent

neural networks. In Advances in Neural Information Processing Systems, pages 545–552, 2009.

[4] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent

neural networks. In Proceedings of ICASSP, pages 6645–6649. IEEE, 2013.

[5] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. Fitnets: Hints for thin deep nets. CoRR, abs/1412.6550, 2014. URL http://arxiv.org/
abs/1412.6550.

[6] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural net-

works. Science, 313(5786):504–507, 2006.

[7] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015.

[8] James Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International

Conference on Machine Learning, pages 735–742, 2010.

[9] James Martens and Ilya Sutskever. Learning recurrent neural networks with Hessian-free optimization.

In Proceedings of the 28th International Conference on Machine Learning, pages 1033–1040, 2011.

[10] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–

1780, 1997.

[11] Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural

Computation, 14(7):1723–1738, 2002.

[12] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural Computation, 6(1):147–160, 1994.
[13] James Martens and Ilya Sutskever. Training deep and recurrent networks with Hessian-free optimization.

In Neural Networks: Tricks of the Trade, pages 479–535. Springer, 2012.

[14] Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal
classiﬁcation: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of
the 23rd International Conference on Machine Learning, pages 369–376, 2006.

[15] Stephen Boyd and Lieven Vandenberghe, editors. Convex Optimization. Cambridge University Press,

2004.

[16] Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):251–276,

1998.

[17] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks.

Conference on Learning Representations, 2014.

In International

[18] Hyeyoung Park, S-I Amari, and Kenji Fukumizu. Adaptive natural gradient learning algorithms for vari-

ous stochastic models. Neural Networks, 13(7):755–764, 2000.

[19] Christopher M. Bishop, editor. Pattern Recognition and Machine Learning. Springer, 2007.
[20] Mario Pechwitz, S Snoussi Maddouri, Volker M¨argner, Noureddine Ellouze, and Hamid Amiri.

IFN/ENIT-database of handwritten arabic words. In Proceedings of CIFED, pages 129–136, 2002.

[21] DARPA-ISTO. The DARPA TIMIT acoustic-phonetic continuous speech corpus (TIMIT). In speech disc

cd1-1.1 edition, 1990.

[22] Alex Graves. Sequence transduction with recurrent neural networks. In ICML Representation Learning

Workshop, 2012.

[23] Kai-Fu Lee and Hsiao-Wuen Hon. Speaker-independent phone recognition using hidden markov models.

IEEE Transactions on Acoustics, Speech and Signal Processing, 37(11):1641–1648, 1989.

[24] Alex Graves. Practical variational inference for neural networks.

Processing Systems, pages 2348–2356, 2011.

In Advances in Neural Information

[25] Alex Graves. Rnnlib: A recurrent neural network library for sequence learning problems, 2008.

9

