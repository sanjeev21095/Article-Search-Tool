A class of network models recoverable by spectral

clustering

Yali Wan

Department of Statistics
University of Washington

Seattle, WA 98195-4322, USA
yaliwan@washington.edu

Marina Meil˘a

Department of Statistics
University of Washington

Seattle, WA 98195-4322, USA

mmp@stat.washington.edu

Abstract

Finding communities in networks is a problem that remains difﬁcult, in spite of the
amount of attention it has recently received. The Stochastic Block-Model (SBM)
is a generative model for graphs with “communities” for which, because of its
simplicity, the theoretical understanding has advanced fast in recent years. In par-
ticular, there have been various results showing that simple versions of spectral
clustering using the Normalized Laplacian of the graph can recover the commu-
nities almost perfectly with high probability. Here we show that essentially the
same algorithm used for the SBM and for its extension called Degree-Corrected
SBM, works on a wider class of Block-Models, which we call Preference Frame
Models, with essentially the same guarantees. Moreover, the parametrization we
introduce clearly exhibits the free parameters needed to specify this class of mod-
els, and results in bounds that expose with more clarity the parameters that control
the recovery error in this model class.

1

Introduction

There have been many recent advances in the recovery of communities in networks, under “block-
model” assumptions [19, 18, 9].
In particular, advances in recovering communities by spectral
clustering algorithms. These have been extended to models including node-speciﬁc propensities.
In this paper, we argue that one can further expand the model class for which recovery by spectral
clustering is possible, and describe a model that subsumes a number of existing models, which we
call the PFM. We show that under the PFM model, the communities can be recovered with small
error, w.h.p. Our results correspond to what [6] termed the “weak recovery” regime, in which w.h.p.
the fraction of nodes that are mislabeled is o(1) when n → ∞.

2 The Preference Frame Model of graphs with communities

This model embodies the assumption that interactions at the community level (which we will also
call macro level) can be quantiﬁed by meaningful parameters. This general assumption underlies
the (p, q) and the related parameterizations of the SBM as well. We deﬁne a preference frame to
be a graph with K nodes, one for each community, that encodes the connectivity pattern at the
community level by a (non-symmetric) stochastic matrix R. Formally, given [K] = {1, . . . K}, a
K × K matrix R (det(R) (cid:54)= 0) representing the transition matrix of a reversible Markov chain on
[K], the weighted graph H = ([K], R), with edge set supp R (edges correspond to entries in R not
being 0) is called a K-preference frame. Requiring reversibility is equivalent to requiring that there
is a set of symmetric weights on the edges from which R can be derived ([17]). We note that without
the reversibility assumption, we would be modeling directed graphs, which we will leave for future

1

work. We denote by ρ the left principal eigenvector of R, satisfying ρT R = ρT . W.l.o.g. we can
assume the eigenvalue 1 or R has multiplicity 11 and therefore we call ρ the stationary distribution
of R.
We say that a deterministic weighted graph G = (V, S) with weight matrix S (and edge set supp S)
admits a K-preference frame H = ([K], R) if and only if there exists a partition C of the nodes V
into K clusters C = {C1, . . . Ck} of sizes n1, . . . , nK, respectively, so that the Markov chain on V
with transition matrix P determined by S satisﬁes the linear constraints

(cid:88)

j∈Cm

Pij = Rlm for all i ∈ Cl, and all cluster indices l, m ∈ {1, 2, . . . k}.

(1)

i=1 Sij.

The matrix P is obtained from S by the standard row-normalization P = D−1S where D =

diag{d1:n}, di =(cid:80)n
pairs i (cid:54)= j. We denote a realization from this process by A. Furthermore, let ˆdi =(cid:80)

A random graph family over node set V admits a K-preference frame H, and is called a Preference
Frame Model (PFM), if the edges i, j, i < j are sampled independently from Bernoulli distributions
with parameters Sij. It is assumed that the edges obtained are undirected and that Sij ≤ 1 for all
j∈V Aij and
in general, throughout this paper, we will denote computable quantities derived from the observed
A with the same letter as their model counterparts, decorated with the “hat” symbol. Thus, ˆD =
diag ˆd1:n, ˆP = ˆD−1A, and so on.
One question we will study is under what conditions the PFM model can be estimated from a given A
by a standard spectral clustering algorithms. Evidently, the difﬁcult part in this estimation problem is
recovering the partition C. If this is obtained correctly, the remaining parameters are easily estimated
in a Maximum Likelihood framework.
But another question we elucidate refers to the parametrization itself. It is known that in the SBM
and Degree Corrected-SBM (DC-SBM) [18], in spite of their simplicity, there are dependencies
between the community level “intensive” parameters and the graph level “extensive”parameters, as
we will show below. In the parametrization of the PFM , we can explicitly show which are the free
parameters and which are the dependent ones.
Several network models in wide use admit a preference frame. For example, the SBM(B) model,
which we brieﬂy describe here. This model has parameters the cluster sizes (n1:K) and the con-
nectivity matrix B ∈ [0, 1]K×K. For two nodes i, j ∈ V, the probability of an edge (i, j) is Bkl
iff i ∈ Ck and j ∈ Cl. The matrix B needs not be symmetric. When Bkk = p, Bkl = q for
k, l ∈ [K], k (cid:54)= l, the model is denoted SBM(p, q). It is easy to verify that the SBM admits a
preference frame. For instance, in the case of SBM(p, q), we have

di = p(nl − 1) + q(n − nl) ≡ dCl , for i ∈ Cl,
qnm
dCl

if l (cid:54)= m, Rl,l =

p(nl − 1)

In the above we have introduced the notation dCl = (cid:80)

Rl,m =

dCl

, for l, m ∈ {1, 2, . . . , k}.

di. One particular realization of the
PFM is the Homogeneous K-Preference Frame model (HPFM). In a HPFM, each node i ∈ V is
characterized by a weight, or propensity to form ties wi. For each pair of communities l, m with
l ≤ m and for each i ∈ Cl, j ∈ Cm we sample Aij with probability Sij given by

j∈Cl

Sij =

Rmlwiwj

ρl

.

(2)

This formulation ensures detail balance in the edge expectations, i.e. Sij = Sji. The HPFM is
virtually equivalent to what is known as the “degree model” [8] or “DC-SBM”, up to a reparam-
eterization2. Proposition 1 relates the node weights to the expected node degrees di. We note
that the main result we prove in this paper uses independent sampling of edges only to prove the
concentration of the laplacian matrix. The PFM model can be easily extended to other graph models

1Otherwise the networks obtained would be disconnected.
2Here we follow the customary deﬁnition of this model, which does not enforce Sii = 0, even though this

implies a non-zero probability of self-loops.

2

with dependent edges if one could prove concentration and eigenvalue separation. For example,
when R has rational entries, the subgraph induced by each block of A can be represented by a
random d-regular graph with a speciﬁed degree.

Proposition 1 In a HPFM di = wi

(cid:80)K

l=1 Rkl

whenever i ∈ Ck and k ∈ [K].

wCl
ρl

Equivalent statements that the expected degrees in each cluster are proportional to the weights exist
in [7, 19] and they are instrumental in analyzing this model. This particular parametrization imme-
diately implies in what case the degrees are globally proportional to the weights. This is, obviously,
the situation when wCl ∝ ρl for all l ∈ [K].
As we see, the node degrees in a HPFM are not directly determined by the propensities wi, but
depend on those by a multiplicative constant that varies with the cluster. This type of interaction
between parameters has been observed in practically all extensions of the Stochastic Block-Model
that we are aware of, making parameter interpretation more difﬁcult. Our following result establishes
what are the free parameters of the PFM and of their subclasses. As it will turn out, these parameters
and their interactions are easily interpretable.

Proposition 2 Let (n1, . . . nK) be a partition of n (assumed to represent the cluster sizes of C =
{C1, . . . CK} a partition of node set V), R a non-singular K × K stochastic matrix, ρ its left
principal eigenvector, and πC1 ∈ [0, 1]n1, . . . πCK ∈ [0, 1]nK probability distributions over C1:K.
Then, there exists a PFM consistent with H = ([K], R), with clustering C, and whose node degrees
are given by

whenever i ∈ Ck, where dtot = (cid:80)

Assumption 2.

di = dtotρkπCk,i,

(3)

i∈V di is a user parameter which is only restricted above by

The proof of this result is constructive, and can be found in the extended version.
The parametrization shows to what extent one can specify independently the degree distribution of a
network model, and the connectivity parameters R. Moreover, it describes the pattern of connection
of a node i as a composition of a macro-level pattern, which gives the total probability of i to
form connections with a cluster l, and the micro-level distribution of connections between i and the
members of Cl. These parameters are meaningful on their own and can be speciﬁed or estimated
separately, as they have no hidden dependence on each other or on n, K.
The PFM enjoys a number of other interesting properties. As this paper will show, almost all the
properties that make SBM’s popular and easy to understand hold also for the much more ﬂexible
PFM. In the remainder of this paper we derive recovery guarantees for the PFM. As an additional
goal, we will show that in the frame we set with the PFM, the recovery conditions become clearer,
more interpretable, and occasionally less restrictive than for other models.
As already mentioned, the PFM includes many models that have been found useful by previous
authors. Yet, the PFM class is much more ﬂexible than those individual models, in the sense that
it allows other unexplored degrees of freedom (or, in other words, achieves the same advantages as
previously studied models with fewer constraints on the data). Note that there is an inﬁnite number
of possible random graphs G with the same parameters (d1:n, n1:k, R) satisfying the constraints (1)
and Proposition 2, yet for reliable community detection we do not need to control S fully, but only

aggregate statistics like(cid:80)

j∈C Aij.

3 Spectral clustering algorithm and main result

Now, we address the community recovery problem from a random graph (V, A) sampled from
the PFM deﬁned as above. We make the standard assumption that K is known. Our analysis is

3

: Graph (V, A) with |V| = n and A ∈ {0, 1}n×n, number of clusters K

based on a very common spectral clustering algorithm used in [13] and described also in [14, 21].
Input
Output: Clustering C
1. Compute ˆD = diag( ˆd1,··· , ˆdn) and Laplacian

ˆL = ˆD−1/2A ˆD−1/2

(4)
2. Calculate the K eigenvectors ˆY1,··· , ˆYK associated with the K eigenvalues |ˆλ1| ≥ ··· ≥ |ˆλK|
of ˆL. Normalize the eigenvectors to unit length. We denote them as the ﬁrst K eigenvectors in the
following text;
3. Set ˆVi = ˆD−1/2 ˆYi, i = 1,··· , K. Form matrix ˆV = [ ˆV1 ··· ˆVK];
4. Treating each row of ˆV as a point in K dimensions, cluster them by the K-means algorithm to
obtain the clustering ˆC.

Algorithm 1: Spectral Clustering

Note that the vectors ˆV are the ﬁrst K eigenvectors of P . The K-means algorithm is assumed to ﬁnd
the global optimum. For more details on good initializations for K-means in step 4 see [16].
We quantify the difference between ˆC and the true clusterings C by the mis-clustering rate perr,
which is deﬁned as

perr = 1 − 1
n

max

φ:[K]→[K]

|Cφ(k) ∩ ˆCk|.

(5)

(cid:88)

k

Theorem 3 (Mis-clustering rate bound for HPFM and PFM) Let the n × n matrix S admit a
PFM, and w1:n, R, ρ, P, A, d1:n have the usual meaning, and let λ1:n be the eigenvalues of P ,
with |λi| ≥ |λi+1|. Let dmin = min d1:n be the minimum expected degree, ˆdmin = min ˆdi, and
dmax = maxij nSij. Let γ ≥ 1, η > 0 be arbitrary numbers. Assume:
Assumption 1 S admits a HPFM model and (2) holds.
Assumption 2 Sij ≤ 1
Assumption 3 ˆdmin ≥ log(n)
Assumption 4 dmin ≥ log(n)
Assumption 5 ∃κ > 0, dmax ≤ κ log n
Assumption 6 grow > 0, where grow is deﬁned in Proposition 4.
Assumption 7 λ1:K are the eigenvalues of R, and |λK| − |λK+1| = σ > 0.

We also assume that we run Algorithm 1 on S and that K-means ﬁnds the optimal solution. Then,
for n sufﬁciently large, the following statements hold with probability at least 1 − e−γ.
PFM Assumptions 2 - 7 imply

perr ≤ Kdtot

ndmingrow

HPFM Assumptions 1 - 6 imply

perr ≤ Kdtot

ndmingrow
where C0 is a constant depending on κ and γ.

(cid:20) C0γ4
(cid:20) C0γ4

σ2 log n

λ2
K log n

(cid:21)
(cid:21)

+

4(log n)η

ˆdmin

+

4(log n)η

ˆdmin

(6)

(7)

Note that perr decreases at least as 1/ log(n) when ˆdmin = dmin = log(n). This is because ˆdmin
and dmin help with the concentration of L. Using Proposition 4, the distances between rows of V ,

4

i.e, the true centers of the k-means step, are lower bounded by grow/dtot. After plugging in the
assumptions for dmin, ˆdmin, dmax, we obtain

(cid:20) C0γ4

σ2 log n

perr ≤ Kκ

grow

(cid:21)

+

4

(log n)(1−η)

.

(8)

When n is small, the ﬁrst component on the right hand side dominates because of the constant C0,
while the second part dominates when n is very large. This shows that perr decreases almost as
1/ log n. Of the remaining quantities, κ controls the spread of the degrees di. Notice that λK and
σ are eigengaps in HPFM model and PFM model respectively and depend only on the preference
frame, and likewise for grow. The eigengaps ensure the stability of principal spaces and the sepa-
ration from the spurious eigenvalues, as shown in Proposition 6. The term containing (log n)η is
designed to control the difference between di and ˆdi with η a small positive constant.

3.1 Proof outline, techniques and main concepts

The proof of Theorem 3 (given in the extended version of the paper) relies on three steps, which
are to be found in most results dealing with spectral clustering. First, concentration bounds of
the empirical Laplacian ˆL w.r.t L are obtained. There are various conditions under which these
can be obtained, and ours are most similar to the recent result of [9]. The other tools we use are
Hoeffding bounds and tools from linear algebra. Second, one needs to bound the perturbation of
the eigenvectors Y as a function of the perturbation in L. This is based on the pivotal results of
Davis and Kahan, see e.g [18]. A crucial ingredient in these type of theorems is the size of the
eigengap between the invariant subspace Y and its orthogonal complement. This is a condition that
is model-dependent, and therefore we discuss the techniques we introduce for solving this problem
in the PFM in the next subsection.
The third step is to bound the error of the K-means clustering algorithm. This is done by a counting
argument. The crux of this step is to ensure the separation of the K distinct rows of V . This, again, is
model dependent and we present our result below. The details and proof are in the extended version.
All proofs are for the PFM; to specialize to the HPFM, one replaces σ with |λK|

3.2 Cluster separation and bounding the spurious eigenvalues in the PFM

volume dCk = (cid:80)

Proposition 4 (Cluster separation) Let V, ρ, d1:n have the usual meaning and deﬁne the cluster
. Let i, j ∈ V be nodes belonging
respectively to clusters k, m with k (cid:54)= m. Then,

di, and cmax, cmin as maxk, mink

i∈Ck

dCk
nρk

(cid:18) 1
(cid:20) 1
(cid:17) − 1√

cmax

ρk

(cid:19)
(cid:16) 1

1
ρm

+

−

1√
ρkρm

(cid:17)(cid:105)

− 1

cmax

||Vi: − Vj:||2 ≥ 1
dtot

(cid:104) 1

(cid:16) 1

ρk

(cid:18) 1

(cid:19)(cid:21)

− 1
cmax

cmin

=

grow
dtot

,

(9)

where grow =
normalized to length 1, the above result holds by replacing dtotcmax,min with max, mink

. Moreover, if the columns of V are

+ 1
ρm

ρkρm

cmax

cmin

.

nk
ρk

In the square brackets, cmax,min depend on the cluster-level degree distribution, while all the other
quantitities depend only of the preference frame. Hence, this expression is invariant with n, and as
long as it is strictly positive, we have that the cluster separation is Ω(1/dtot).
The next theorem is crucial in proving that L has a constant eigengap. We express the eigengap of P
in terms of the preference frame H and the mixing inside each of the clusters Ck. For this, we resort
to generalized stochastic matrices, i.e. rectangular positive matrices with equal row sums, and we
relate their properties to the mixing of Markov chains on bipartite graphs.
These tools are introduced here, for the sake of intuition, toghether with the main spectral result,
while the rest of the proofs are in the extended version.
Given C, for any vector x ∈ Rn, we denote by xk, k = 1, . . . K, the block of x indexed by elements
of cluster k of C. Similarly, for any square matrix A ∈ Rn×n, we denote by Akl = [Aij]i∈k,j∈l the
block with rows indexed by i ∈ k, and columns indexed by j ∈ l.

5

Denote by ρ, λ1:K, ν1:K ∈ RK respectively the stationary distribution, eigenvalues3, and eigenvec-
tors of R.
We are interested in block stochastic matrices P for which the eigenvalues of R are the principal
eigenvalues. We call λK+1 . . . λn spurious eigenvalues. Theorem 6 below is a sufﬁcient condition
that bounds |λK+1| whenever each of the K 2 blocks of P is ”homogeneous” in a sense that will be
deﬁned below.
When we consider the matrix L = D−1/2SD−1/2 partitioned according to C, it will be convenient
to consider the off-diagonal blocks in pairs. This is why the next result describes the properties of
matrices consisting of a pair of off-diagonal blocks.

Proposition 5 (Eigenvalues for the off-diagonal blocks) Let M be the square matrix

M =

(cid:20) 0 B
(cid:21)
(cid:20) x1

A 0

x2

(cid:21)

(10)

, x1,2 ∈ Cn1,2 be an eigenvector of M

where A ∈ Rn2×n1 and B ∈ Rn1×n2, and let x =
with eigenvalue λ. Then

Bx2 = λx1
Ax1 = λx2

ABx2 = λ2x2
BAx1 = λ2x1

(cid:20) BA

(cid:21)

0
AB

(11)
(12)

0

M 2 =

1 − xT

(13)
Moreover, if M is symmetric, i.e B = AT , then λ is a singular value of A, x is real, and −λ is
2 ]T . Assuming n2 ≤ n1, and that A is full rank,
also an eigenvalue of M with eigenvector [xT
one can write A = V ΛU T with V ∈ Rn2×n2, U ∈ Rn1×n2 orthogonal matrices, and Λ a diagonal
matrix of non-zero singular values.
Theorem 6 (Bounding the spurious eigenvalues of L) Let C, L, P, D, S, R, ρ be deﬁned as above,
and let λ be an eigenvalue of P . Assume that (1) P is block-stochastic with respect to C; (2) λ1:K are
the eigenvalues of R, and |λK| > 0; (3) λ is not an eigenvalue of R; (4) denote by λkl
3 (λkk
2 ) the third
|λkl
3 |
λmax(Mkl) ≤ c < 1
(second) largest in magnitude eigenvalue of block Mkl (Lkk) and assume that
λmax(Lkk) ≤ c). Then, the spurious eigenvalues of P are bounded by c times a constant that
(
depends only on R.

|λkk
2 |

rkk +

(cid:88)

l(cid:54)=k

√

rklrlk

|λ| ≤ c max

k=1:K

Remarks: The factor that multiplies c can be further bounded denoting a = [
√
[

rlk]T

l=1:K



K(cid:88)

(14)

rkl]T

l=1:K, b =

√

rlk

(15)

(16)

(cid:118)(cid:117)(cid:117)(cid:116) K(cid:88)

rkl

rlk =

l=1

l=1

(cid:118)(cid:117)(cid:117)(cid:116) K(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) K(cid:88)

rlk

l=1

l=1

(cid:88)

l(cid:54)=k

√

rkk +

In other words,

rklrlk = aT b ≤ ||a||||b|| =

|λ| ≤ c
2

max
k=1:K

The maximum column sum of a stochastic matrix is 1 if the matrix is doubly stochastic and larger
K. However, one must remember that the interesting R
than 1 otherwise, and can be as large as
matrices have “large” eigenvalues. In particular we will be interested in λK > c. It is expected that
under these conditions, the factor depending on R to be close to 1.

√

3Here too, eigenvalues will always be ordered in decreasing order of their magnitudes, with positive values

preceeding negatives one of the same magnitude. Consequently, for any stochastic matrix, λ1 = 1 always

6

The second remark is on the condition (3), that all blocks have small spurious eigenvalues. This
condition is not merely a technical convenience. If a block had a large eigenvalue, near 1 or −1
(times its λmax), then that block could itself be broken into two distinct clusters. In other words, the
clustering C would not accurately capture the cluster structure of the matrix P . Hence, condition (3)
amounts to requiring that no other cluster structure is present, in other words that within each block,
the Markov chain induced by P mixes well.

4 Related work

Previous results we used The Laplacian concentration results use a technique introduced recently
by [9], and some of the basic matrix theoretic results are based on [14] which studied the P and L
matrix in the context of spectral clustering. As any of the many works we cite, we are indebted to
the pioneering work on the perturbation of invariant subspaces of Davis and Kahan [18, 19, 20].

4.1 Previous related models

The conﬁguration model for regular random graphs [4, 11] and for graphs with general ﬁxed degrees
[10, 12] is very well known. It can be shown by a simple calculation that the conﬁguration model
also admits a K-preference frame. In the particular case when the diagonal of the R matrix is 0 and
the connections between clusters are given by a bipartite conﬁguration model with ﬁxed degrees,
K-preference frames have been studied by [15] under the name “equitable graphs”; the object there
was to provide a way to calculate the spectrum of the graph.
Since the PFM is itself an extension of the SBM, many other extensions of the latter will bear
resemblance to PFM. Here we review only a subset of these, a series of strong relatively recent
advances, which exploit the spectral properties of the SBM and extend this to handle a large range
of degree distributions [7, 19, 5]. The PFM includes each of these models as a subclass4.
In [7] the authors study a model that coincides (up to some multiplicative constants) with the HPFM.
The paper introduces an elegant algorithm that achieves partial recovery or better, which is based
on the spectral properties of a random Laplacian-like matrix, and does not require knowledge of the
partition size K.
The PFM also coincides with the model of [1] and [8] called the expected degree model w.r.t the
distribution of intra-cluster edges, but not w.r.t the ambient edges, so the HPFM is a subclass of this
model.
A different approach to recovery The papers [5, 18, 9] propose regularizing the normalized Lapla-
cian with respect to the inﬂuence of low degrees, by adding the scaled unit matrix τ I to the incidence
matrix A, and thereby they achieve recovery for much more imbalanced degree distributions than
us. Currently, we do not see an application of this interesting technique to the PFM, as the diagonal
regularization destroys the separation of the intracluster and intercluster transitions, which guaran-
tee the clustering property of the eigenvectors. Therefore, currently we cannot break the n log n
limit into the ultra-sparse regime, although we recognize that this is an important current direction
of research.
Recovery results like ours can be easily extended to weighted, non-random graphs, and in this sense
they are relevant to the spectral clustering of these graphs, when they are assumed to be noisy
versions of a G that admits a PFM.

4.2 An empirical comparison of the recovery conditions

As obtaining general results in comparing the various recovery conditions in the literature would be
a tedious task, here we undertake to do a numerical comparison. While the conclusions drawn from
this are not universal, they illustrate well the stringency of various conditions, as well as the gap
between theory and actual recovery. For this, we construct HPFM models, and verify numerically if
they satisfy the various conditions. We have also clustered random graphs sampled from this model,
with good results (shown in the extended version).

4In particular, the models proposed in [7, 19, 5] are variations of the DC-SBM and thus forms of the

homogeneous PFM.

7

We generate S from the HPFM model with K = 5, n = 5000. Each wi is uniformly generated
from (0.5, 1). n1:K = (500, 1000, 1500, 1000, 1000), grow > 0, λ1:K = (1, 0.8, 0.6, 0.4, 0.2). The

matrix R is given below; note its last row in which r55 <(cid:80)4



.80
.04
.01
.01
.13

R =

.07
.52
.20
.08
.21

.02
.24
.65
.12
.02

.02
.12
.15
.70
.32

.09
.08
.00
.08
.33

l=1 r5l.

 ρ = (.25, .44, .54, .65, .17).

(17)

dmin

The conditions we are verifying include besides ours, those obtained by [18], [19], [3] and [5];
since the original S is a perfect case for spectral clustering of weighted graphs, we also verify the
theoretical recovery conditions for spectral clustering in [2] and [16].
Our result Theorem 3 Assumption 1 and 2 automatically hold from the construction of the data.
By simulating the data, We ﬁnd that dmin = 77.4, ˆdmin = 63, both of which are bigger than
log n = 8.52. Therefore Assumption 3 and 4 hold. dmax = 509.3, grow = 1.82 > 0, thus Assump-
tion 5 and 6 hold. After running Algorithm 1, the mis-clustering rate is r = 0.0008, which satisﬁes
the theoretical bound. In conclusion, the dataset ﬁts into both the assumptions and conclusion of
Theorem 3.
λK ≥
Qin and Rohe[18] This paper has an assumption on the lower bound on λK, that is
, so that the concentration bound holds with probability (1 − ). We set  = 0.1 and

(cid:113) K(ln(K/)

√
1
8

3

n , and requires τ 2

1 ≥ maxi1,i2∈{1,··· ,K}(cid:80)

2),  =(cid:112)K(K − 1)1 + K2

obtain λK ≥ 12.3, which is impossible to hold since λK is upper bounded by 15.
Rohe, Chatterjee, Yu[19] Here, one deﬁnes τn = dmin
n log n > 2 to ensure the
concentration of L. To meet this assumption, with n = 5000, dmin ≥ 2422. While in our case
dmin = 77.4. The assumption requires a very dense graph and is not satisﬁed in this dataset.
Balcan, Borgs Braverman, Chayes[3]Their theorem is based on self-determined community struc-
ture. It requires all the nodes to be more connected within their own cluster. However, in our graph,
1296 out of 5000 nodes have more connections to outside nodes than to nodes in their own cluster.
Ng, Jordan, Weiss[16] require λ2 < 1 − δ, where δ > (2 + 2
2,
)1/2.
On the given data, we ﬁnd that  ≥ 36.69, and δ ≥ 125.28, which is impossible to hold since δ
needs to be smaller than 1.
Chaudhuri, Chung, Tsiatas[5] The recovery theorem of this paper requires di ≥ 128
9 ln(6n/δ),
so that when all the assumptions hold, it recovers the clustering correctly with probability at least
1 − 6δ. We set δ = 0.01, and obtain that di = 77.40, 128
9 ln(6n/δ) = 212.11. Therefore the
assumption fails as well.
For our method, the hardest condition to satisfy, and the most different from the others, was Assump-
tion 6. We repeated this experiment with the other weights distributions for which this assumption
fails. The assumptions in the related papers continued to be violated. In [Qin and Rohe], we obtain
λK ≥ 17.32. In [Rohe, Chatterjee, Yu], we still needs dmin ≥ 2422. In [Balcan, Borgs Braverman,
Chayes], we get 1609 points more connected to the outside nodes of its cluster. In [Balakrishnan,
Xu, Krishnamurthy, Singh], we get σ = 0.172 and needs to satisfy σ = o(0.3292). In [Ng, Jordan,
Weiss], we obtain δ ≥ 175.35. Therefore, the assumptions in these papers are all violated as well.

, 2 ≥ maxi∈{1,··· ,K}

((cid:80)

k:k∈Si
ˆdj

j∈Ci1

k∈Ci2

k,l∈Si

A2
kl
ˆdk
ˆdl

(cid:80)

A2
jk
ˆdj ˆdk

√

(cid:80)

5 Conclusion

In this paper, we have introduced the preference frame model, which is more ﬂexible and subsumes
many current models including SBM and DC-SBM. It produces state-of-the art recovery rates com-
parable to existing models. To accomplish this, we used a parametrization that is clearer and more
intuitive. The theoretical results are based on the new geometric techniques which control the eigen-
gaps of the matrices with piecewise constant eigenvectors.
We note that the main result theorem 3 uses independent sampling of edges only to prove the concen-
tration of the laplacian matrix. The PFM model can be easily extended to other graph models with
dependent edges if one could prove concentration and eigenvalue separation. For example, when
R has rational entries, the subgraph induced by each block of A can be represented by a random
d-regular graph with a speciﬁed degree.

5To make λ ≤ 1 possible, one needs dmin ≥ 11718.

8

References
[1] Sanjeev Arora, Rong Ge, Sushant Sachdeva, and Grant Schoenebeck. Finding overlapping
communities in social networks: toward a rigorous approach. In Proceedings of the 13th ACM
Conference on Electronic Commerce, pages 37–54. ACM, 2012.

[2] Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise thresholds
for spectral clustering. In Advances in Neural Information Processing Systems, pages 954–962,
2011.

[3] Maria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, and Shang-Hua

Teng. Finding endogenously formed communities. arxiv preprint arXiv:1201.4899v2, 2012.

[4] Bela Bollobas. Random Graphs. Cambridge University Press, second edition, 2001.
[5] K. Chaudhuri, F. Chung, and A. Tsiatas. Spectral clustering of graphs with general degrees in
extended planted partition model. Journal of Machine Learning Research, pages 1–23, 2012.
[6] Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and
submatrix localization with a growing number of clusters and submatrices. arXiv preprint
arXiv:1402.1267, 2014.

[7] Amin Coja-Oghlan and Andre Lanka. Finding planted partitions in random graphs with general

degree distributions. SIAM Journal on Discrete Mathematics, 23:1682–1714, 2009.
[8] M. O. Jackson. Social and Economic Networks. Princeton University Press, 2008.
[9] Can M. Le and Roman Vershynin. Concentration and regularization of random graphs. 2015.
[10] Brendan McKay. Asymptotics for symmetric 0-1 matrices with prescribed row sums. Ars

Combinatoria, 19A:15–26, 1985.

[11] Brendan McKay and Nicholas Wormald. Uniform generation of random regular graphs of

moderate degree. Journal of Algorithms, 11:52–67, 1990.

[12] Brendan McKay and Nicholas Wormald. Asymptotic enumeration by degree sequence of

graphs with degrees o(n1/2. Combinatorica, 11(4):369–382, 1991.

[13] Marina Meil˘a and Jianbo Shi. Learning segmentation by random walks. In T. K. Leen, T. G.
Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems, vol-
ume 13, pages 873–879, Cambridge, MA, 2001. MIT Press.

[14] Marina Meil˘a and Jianbo Shi. A random walks view of spectral segmentation. In T. Jaakkola

and T. Richardson, editors, Artiﬁcial Intelligence and Statistics AISTATS, 2001.

[15] M.E.J. Newman and Travis Martin. Equitable random graphs. 2014.
[16] Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis and an

algorithm. Advances in neural information processing systems, 2:849–856, 2002.

[17] J.R. Norris. Markov Chains. Cambridge University Press, 1997.
[18] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic
blockmodel. In Advances in Neural Information Processing Systems, pages 3120–3128, 2013.
[19] Karl Rohe, Sourav Chatterjee, Bin Yu, et al. Spectral clustering and the high-dimensional

stochastic blockmodel. The Annals of Statistics, 39(4):1878–1915, 2011.

[20] Gilbert W Stewart, Ji-guang Sun, and Harcourt Brace Jovanovich. Matrix perturbation theory,

volume 175. Academic press New York, 1990.

[21] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–

416, 2007.

9

