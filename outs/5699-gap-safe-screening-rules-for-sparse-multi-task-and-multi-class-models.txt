GAP Safe screening rules for sparse multi-task and

multi-class models

Eugene Ndiaye Olivier Fercoq Alexandre Gramfort

Joseph Salmon

firstname.lastname@telecom-paristech.fr

LTCI, CNRS, T´el´ecom ParisTech, Universit´e Paris-Saclay

Paris, 75013, France

Abstract

High dimensional regression beneﬁts from sparsity promoting regularizations.
Screening rules leverage the known sparsity of the solution by ignoring some
variables in the optimization, hence speeding up solvers. When the procedure
is proven not to discard features wrongly the rules are said to be safe. In this paper
we derive new safe rules for generalized linear models regularized with (cid:96)1 and
(cid:96)1{(cid:96)2 norms. The rules are based on duality gap computations and spherical safe
regions whose diameters converge to zero. This allows to discard safely more vari-
ables, in particular for low regularization parameters. The GAP Safe rule can cope
with any iterative solver and we illustrate its performance on coordinate descent
for multi-task Lasso, binary and multinomial logistic regression, demonstrating
signiﬁcant speed ups on all tested datasets with respect to previous safe rules.

1

Introduction

The computational burden of solving high dimensional regularized regression problem has lead to a
vast literature in the last couple of decades to accelerate the algorithmic solvers. With the increasing
popularity of (cid:96)1-type regularization ranging from the Lasso [18] or group-Lasso [24] to regularized
logistic regression and multi-task learning, many algorithmic methods have emerged to solve the
associated optimization problems. Although for the simple (cid:96)1 regularized least square a speciﬁc
algorithm (e.g., the LARS [8]) can be considered, for more general formulations, penalties, and
possibly larger dimension, coordinate descent has proved to be a surprisingly efﬁcient strategy [12].
Our main objective in this work is to propose a technique that can speed-up any solver for such
learning problems, and that is particularly well suited for coordinate descent method, thanks to
active set strategies.
The safe rules introduced by [9] for generalized (cid:96)1 regularized problems, is a set of rules that allows
to eliminate features whose associated coefﬁcients are proved to be zero at the optimum. Relaxing
the safe rule, one can obtain some more speed-up at the price of possible mistakes. Such heuristic
strategies, called strong rules [19] reduce the computational cost using an active set strategy, but
require difﬁcult post-precessing to check for features possibly wrongly discarded. Another road to
speed-up screening method has been the introduction of sequential safe rules [21, 23, 22]. The idea
is to improve the screening thanks to the computations done for a previous regularization parameter.
This scenario is particularly relevant in machine learning, where one computes solutions over a
grid of regularization parameters, so as to select the best one (e.g., to perform cross-validation).
Nevertheless, such strategies suffer from the same problem as strong rules, since relevant features
can be wrongly disregarded: sequential rules usually rely on theoretical quantities that are not known
by the solver, but only approximated. Especially, for such rules to work one needs the exact dual
optimal solution from the previous regularization parameter.

1

Recently, the introduction of safe dynamic rules [6, 5] has opened a promising venue by letting
the screening to be done not only at the beginning of the algorithm, but all along the iterations.
Following a method introduced for the Lasso [11], we generalize this dynamical safe rule, called
GAP Safe rules (because it relies on duality gap computation) to a large class of learning problems
with the following beneﬁts:

• a uniﬁed and ﬂexible framework for a wider family of problems,
• easy to insert in existing solvers,
• proved to be safe,
• more efﬁcient that previous safe rules,
• achieves fast true active set identiﬁcation.

We introduce our general GAP Safe framework in Section 2. We then specialize it to important
machine learning use cases in Section 3. In Section 4 we apply our GAP Safe rules to a multi-
task Lasso problem, relevant for brain imaging with magnetoencephalography data, as well as to
multinomial logistic regression regularized with (cid:96)1{(cid:96)2 norm for joint feature selection.

2 GAP Safe rules

2.1 Model and notations
We denote by rds the set t1, . . . , du for any integer d P N, and by QJ the transpose of a matrix
Q. Our observation matrix is Y P Rnˆq where n represents the number of samples, and q the
number of tasks or classes. The design matrix X “ rxp1q, . . . , xppqs “ rx1, . . . , xnsJ P Rnˆp has
p explanatory variables (or features) column-wise, and n observations row-wise. The standard (cid:96)2
norm is written } ¨ }2, the (cid:96)1 norm } ¨ }1, the (cid:96)8 norm } ¨ }8. The (cid:96)2 unit ball is denoted by B2 (or
simply B) and we write Bpc, rq the (cid:96)2 ball with center c and radius r. For a matrix B P Rpˆq, we
denote by }B}2
j,k the Frobenius norm, and by x¨,¨y the associated inner product.
We consider the general optimization problem of minimizing a separable function with a group-
Lasso regularization. The parameter to recover is a matrix B P Rpˆq, and for any j in Rp, Bj,: is the
j-th row of B, while for any k in Rq, B:,k is the k-th column. We would like to ﬁnd

k“1 B2

ř

ř

2 “

p
j“1

q

pBpλq P arg min

BPRpˆq

nÿ
loooooooooooomoooooooooooon
fipxJ
i Bq ` λΩpBq

i“1

,

(1)

ř

PλpBq

p

ř
where fi : R1ˆq ÞÑ R is a convex function with 1{γ-Lipschitz gradient. So F : B Ñ
i Bq
is also convex with Lipschitz gradient. The function Ω : Rpˆq ÞÑ R` is the (cid:96)1{(cid:96)2 norm ΩpBq “
j“1 }Bj,:}2 promoting a few lines of B to be non-zero at a time. The λ parameter is a non-negative
constant controlling the trade-off between data ﬁtting and regularization.
Some elements of convex analysis used in the following are introduced here. For a convex function
f : Rd Ñ r´8,`8s the Fenchel-Legendre transform1 of f, is the function f˚ : Rd Ñ r´8,`8s
deﬁned by f˚puq “ supzPRdxz, uy ´ fpzq. The sub-differential of a function f at a point x is
denoted by Bfpxq. The dual norm of Ω is the (cid:96)8{(cid:96)2 norm and reads Ω˚pBq “ maxjPrps }Bj,:}2.
Remark 1. For the ease of reading, all groups are weighted with equal strength, but extension
of our results to non-equal weights as proposed in the original group-Lasso [24] paper would be
straightforward.

i“1 fipxJ

n

2.2 Basic properties

First we recall the associated Fermat’s condition and a dual formulation of the optimization problem:
Theorem 1. Fermat’s condition (see [3, Proposition 26.1] for a more general result)
For any convex function f : Rn Ñ R:

x P arg min
xPRn

fpxq ô 0 P Bfpxq.

(2)

1this is also often referred to as the (convex) conjugate of a function

2

´ nÿ
Theorem 2 ([9]). A dual formulation of (1) is given by
looooooooomooooooooon
i p´λΘi,:q
f˚

pΘpλq “ arg max

ΘP∆X

i“1

DλpΘq

.

(3)

pΘ
xpjqJpΘpλq P

where ∆X “ tΘ P Rnˆq : @j P rps,}xpjqJ
and dual solutions are linked by

@i P rns,
Furthermore, Fermat’s condition reads:

Θ}2 ď 1u “ tΘ P Rnˆq : Ω˚pXJΘq ď 1u. The primal
pBpλqq{λ.
pλq
i,: “ ´∇fipxJ
*
ifpB
ifpB
Then the primal/dual link can be writtenpΘpλq “ ´GpXpBpλqq{λ .

Remark 2. Contrarily to the primal, the dual problem has a unique solution under our assumption
on fi. Indeed, the dual function is strongly concave, hence strictly concave.
Remark 3. For any Θ P Rnˆq let us introduce GpΘq “ r∇f1pΘ1,:qJ, . . . ,∇fnpΘn,:qJs P Rnˆq.

pλq
j,: ‰ 0,
pλq
j,: “ 0.

ˆBλ
j,;
j,;}2
} ˆBλ
B2,

@j P rps,

$&%

"

(5)

(4)

i

,

2.3 Critical parameter: λmax

For λ large enough the solution of the primal problem is simply 0. Thanks to the Fermat’s rule (2),
0 is optimal if and only if ´∇Fp0q{λ P BΩp0q. Thanks to the property of the dual norm Ω˚, this is
equivalent to Ω˚p∇Fp0q{λq ď 1 where Ω˚ is the dual norm of Ω. Since ∇Fp0q “ XJGp0q, 0 is a
primal solution of Pλ if and only if λ ě λmax :“ maxjPrps }xpjqJ
This development shows that for λ ě λmax, Problem (1) is trivial. So from now on, we will only
focus on the case where λ ď λmax.

Gp0q}2 “ Ω˚pXJGp0qq.

2.4 Screening rules description

Safe screening rules rely on a simple consequence of the Fermat’s condition:

Stated in such a way, this relation is useless becausepΘpλq is unknown (unless λ ą λmax). However,

it is often possible to construct a set R Ă Rnˆq, called a safe region, containing it. Then, note that

pλq
j,: “ 0 .

(6)

}xpjqJpΘpλq}2 ă 1 ñpB
Θ}2 ă 1 ñpB

ΘPR }xpjqJ

max

pλq
j,: “ 0 .

(7)

previous test is satisﬁed, sincepB

The so called safe screening rules consist in removing the variable j from the problem whenever the
is then guaranteed to be zero. This property leads to considerable
speed-up in practice especially with active sets strategies, see for instance [11] for the Lasso case. A
natural goal is to ﬁnd safe regions as narrow as possible: smaller safe regions can only increase the
number of screened out variables. However, complex regions could lead to a computational burden
limiting the beneﬁt of screening. Hence, we focus on constructing R satisfying the trade-off:

pλq
j,:

• R is as small as possible and containspΘpλq.
• Computing maxΘPR }xpjqJ

Θ}2 is cheap.

Spheres as safe regions

2.5
Various shapes have been considered in practice for the set R such as balls (referred to as spheres)
[9], domes [11] or more reﬁned sets (see [23] for a survey). Here we consider the so-called
“sphere regions” choosing a ball R “ Bpc, rq as a safe region. One can easily obtain a control

3

Θ}2 by extending the computation of the support function of a ball [11, Eq.
ΘPBpc,rq}xpjqJ

on maxΘPBpc,rq }xpjqJ
(9)] to the matrix case: max
Note that here the center c is a matrix in Rpˆq. We can now state the safe sphere test:
pλq
j,: “ 0.

c}2 ` r}xpjq}2 ă 1,

then pB

c}2 ` r}xpjq}2 .

Θ}2 ď }xpjqJ

Sphere test:

}xpjqJ

(8)

If

2.6 GAP Safe rule description

i

In this section we derive a GAP Safe screening rule extending the one introduced in [11]. For this,
we rely on the strong convexity of the dual objective function and on weak duality.
Finding a radius: Remember that @i P rns, fi is differentiable with a 1{γ-Lipschitz gradient.
As a consequence, @i P rns, f˚
is γ-strongly convex [14, Theorem 4.2.2, p. 83] and so Dλ is
γλ2-strongly concave:
@pΘ1, Θ2q P Rnˆq ˆ Rnˆq, DλpΘ2q ď DλpΘ1q ` x∇DλpΘ1q, Θ2 ´ Θ1y ´ γλ2
2

Specifying the previous inequality for Θ1 “ pΘpλq, Θ2 “ Θ P ∆X, one has
DλpΘq ď DλppΘpλqq ` x∇DλppΘpλqq, Θ ´pΘpλqy ´ γλ2
By deﬁnition,pΘpλq maximizes Dλ on ∆X, so we have: x∇DλppΘpλqq, Θ´pΘpλqy ď 0. This implies
}pΘpλq ´ Θ}2.
DλpΘq ď DλppΘpλqq ´ γλ2
By weak duality @B P Rpˆq, DλppΘpλqq ď PλpBq, so : @B P Rpˆq,@Θ P ∆X , DλpΘq ď PλpBq ´
2 }pΘpλq ´ Θ}2, and we deduce the following theorem:
d

}pΘpλq ´ Θ}2.

}Θ1 ´ Θ2}2.

γλ2

2

2

Theorem 3.

2pPλpBq ´ DλpΘqq

γλ2

“: ˆrλpB, Θq.

(9)

@B P Rpˆq,@Θ P ∆X ,

(cid:13)(cid:13)(cid:13)pΘpλq ´ Θ

(cid:13)(cid:13)(cid:13)2

ď

Provided one knows a dual feasible point Θ P ∆X and a B P Rpˆq , it is possible to construct a safe
sphere with radius ˆrλpB, Θq centered on Θ. We now only need to build a (relevant) dual point to
center such a ball. Results from Section 2.3, ensure that ´Gp0q{λmax P ∆X, but it leads to a static
rule, a introduced in [9]. We need a dynamic center to improve the screening as the solver proceeds.

Finding a center: Remember thatpΘpλq “ ´GpXpBpλqq{λ. Now assume that one has a converging
algorithm for the primal problem, i.e., Bk Ñ pBpλq. Hence, a natural choice for creating a dual

feasible point Θk is to choose it proportional to ´GpXBkq, for instance by setting:

Θk “

Rk
λ ,
Ω˚pXJRkq ,

Rk

if Ω˚pXJRkq ď λ,
otherwise.

(10)
A reﬁned method consists in solving the one dimensional problem: arg maxΘP∆XXSpanpRkq DλpΘq.
In the Lasso and group-Lasso case [5, 6, 11] such a step is simply a projection on the intersection of
a line and the (polytope) dual set and can be computed efﬁciently. However for logistic regression
the computation is more involved, so we have opted for the simpler solution in Equation (10). This
still provides converging safe rules (see Proposition 1).

where Rk “ ´GpXBkq .

#

Dynamic GAP Safe rule summarized

We can now state our dynamical GAP Safe rule at the k-th step of an iterative solver:

1. Compute Bk, and then obtain Θk and ˆrλpBk, Θkq using (10).

4

Θk}2 ` ˆrλpBk, Θkq}xpjq}2 ă 1, then setpB

2. If }xpjqJ

pλq
j,: “ 0 and remove xpjq from X.

Dynamic safe screening rules are more efﬁcient than existing methods in practice because they can
increase the ability of screening as the algorithm proceeds. Since one has sharper and sharper dual
regions available along the iterations, support identiﬁcation is improved. Provided one relies on a
primal converging algorithm, one can show that the dual sequence we propose is converging too.
The convergence of the primal is unaltered by our GAP Safe rule: screening out unnecessary coefﬁ-
cients of Bk can only decrease its distance with its original limits. Moreover, a practical consequence
is that one can observe surprising situations where lowering the tolerance of the solver can reduce
the computation time. This can happen for sequential setups.

Proposition 1. Let Bk be the current estimate of pBpλq and Θk deﬁned in Eq. (10) be the current
estimate ofpΘpλq. Then limkÑ`8 Bk “pBpλq implies limkÑ`8 Θk “ pΘpλq.

Note that if the primal sequence is converging to the optimal, our dual sequence is also converging.
But we know that the radius of our safe sphere is p2pPλpBkq ´ DλpΘkqq{pγλ2qq1{2. By strong
duality, this radius converges to 0, hence we have certiﬁed that our GAP Safe regions sequence
BpΘk, ˆrλpBk, Θkqq is a converging safe rules (in the sense introduced in [11, Deﬁnition 1]).
Remark 4. The active set obtained by our GAP Safe rule (i.e., the indexes of non screened-out

variables) converges to the equicorrelation set [20] Eλ :“ tj P p : }xpjqJpΘpλq}2 “ 1u, allowing us

to early identify relevant features (see Proposition 2 in the supplementary material for more details).

3 Special cases of interest

We now specialize our results to relevant supervised learning problems, see also Table 1.

n

ř
3.1 Lasso
In the Lasso case q “ 1, the parameter is a vector: B “ β P Rp, Fpβq “ 1{2}y ´ Xβ}2
i“1pyi ´ xJ
3.2

i βq2, meaning that fipzq “ pyi ´ zq2{2 and Ωpβq “ }β}1.

(cid:96)1{(cid:96)2 multi-task regression

2}Y ´XB}2

ř
In the multi-task Lasso, which is a special case of group-Lasso, we assume that the observation is
Y P Rnˆq, FpBq “ 1
2 (i.e., fipzq “ }Yi,:´z}2{2) and ΩpBq “
j“1 }Bj,:}2. In signal processing, this model is also referred to as Multiple Measurement Vector
(MMV) problem. It allows to jointly select the same features for multiple regression tasks [1, 2].
Remark 5. Our framework could encompass easily the case of non-overlapping groups with var-
ious size and weights presented in [6]. Since our aim is mostly for multi-task and multinomial
applications, we have rather presented a matrix formulation.

i“1 }Yi,:´xJ

2 “ 1

i B}2

2 “

ř

n

p

2

3.3

(cid:96)1 regularized logistic regression

`

`

`

˘˘˘

´yixJ

Here, we consider the formulation given in [7, Chapter 3] for the two classes logistic regression. In
such a context, one observes for each i P rns a class label ci P t1, 2u. This information can be recast
as yi “ 1tci“1u, and it is then customary to minimize (1) where
1 ` exp

(11)
with B “ β P Rp (i.e., q “ 1), fipzq “ ´yiz ` logp1 ` exppzqq and the penalty is simply the (cid:96)1
norm: Ωpβq “ }β}1. Let us introduce Nh, the (binary) negative entropy function deﬁned by 2:

Fpβq “ nÿ
"
x logpxq ` p1 ´ xq logp1 ´ xq,
`8,
i pziq “ Nhpzi ` yiq and γ “ 4.
Then, one can easily check that f˚
2with the convention 0 logp0q “ 0

if x P r0, 1s ,
otherwise .

i β ` log

Nhpxq “

xJ
i β

(12)

i“1

,

5

Lasso
pyi´zq2

2

Multi-task regr.

}Yi,:´z}2

2

Logistic regr.
logp1 ` ezq ´ yiz

log

fipzq
i puq
f˚
ΩpBq

λmax
GpΘq

γ

pyi´uq2´y2

i

2

}β}1

}XJy}8
θ ´ y

1

}Yi,:´u}2´}Yi,:}2

2

pÿ

2

}Bj,:}2

j“1
Ω˚pXJY q
Θ ´ Y

1

Yi,kzk

Multinomial regr.

` qÿ

˘

´ qÿ

ezk

k“1
k“1
pÿ
NHpu ` Yi,:q
}Bj,:}2

j“1

Nhpu ` yiq

}β}1

}XJp1n{2 ´ yq}8

ez

1`ez ´ y

4

Ω˚pXJp1nˆq{q ´ Y qq
RowNormpeΘq ´ Y

1

Table 1: Useful ingredients for computing GAP Safe rules. We have used lower case to indicate
when the parameters are vectorial (i.e., q “ 1). The function RowNorm consists in normalizing a
(non-negative) matrix row-wise, such that each row sums to one.

3.4

(cid:96)1{(cid:96)2 multinomial logistic regression

We adapt the formulation given in [7, Chapter 3] for the multinomial regression. In such a context,
one observes for each i P rns a class label ci P t1, . . . , qu. This information can be recast into a
matrix Y P Rnˆq ﬁlled by 0’s and 1’s: Yi,k “ 1tci“ku. In the same spirit as the multi-task Lasso, a
matrix B P Rpˆq is formed by q vectors encoding the hyperplanes for the linear classiﬁcation. The
multinomial (cid:96)1{(cid:96)2 regularized regression reads:
ř
ř
´Yi,kxJ
k“1
k“1 ´Yi,kzk ` logp

(13)
k“1 exppzkqq to recover the formulation as in (1). Let us

with fipzq “
introduce NH, the negative entropy function deﬁned by (still with the convention 0 logp0q “ 0)

i B:,k ` log

˘¸¸

xJ
i B:,k

qÿ

˜

˜

k“1

`

exp

,

q

i“1

qÿ

FpBq “ nÿ
"ř
i“1 xi logpxiq,
`8,

q

q

ř
i“1 xi “ 1u,

q

if x P Σq “ tx P Rq` :
otherwise.

NHpxq “

(14)

: @i P rns,´λΘ1

i pzq “ NHpz ` Yi,:q and γ “ 1.

Again, one can easily check that f˚
Remark 6. For multinomial logistic regression, Dλ implicitly encodes the additional constraint
Θ P dom Dλ “ tΘ1
i,: ` Yi,: P Σqu where Σq is the q dimensional simplex, see
(14). As 0 and Rk{λ both belong to this set, any convex combination of them, such as Θk deﬁned
in (10), satisﬁes this additional constraint.
Remark 7. The intercept has been neglected in our models for simplicity. Our GAP Safe framework
can also handle such a feature at the cost of more technical details (by adapting the results from [15]
for instance). However, in practice, the intercept can be handled in the present formulation by adding
a constant column to the design matrix X. The intercept is then regularized. However, if the constant
is set high enough, regularization is small and experiments show that it has little to no impact for
high-dimensional problems. This is the strategy used by the Liblinear package [10].

4 Experiments

In this section we present results obtained with the GAP Safe rule. Results are on high dimensional
data, both dense and sparse. Implementation have been done in Python and Cython for low critical
parts. They are based on the multi-task Lasso implementation of Scikit-Learn [17] and coordinate
descent logistic regression solver in the Lightning software [4]. In all experiments, the coordinate
descent algorithm used follows the pseudo code from [11] with a screening step every 10 iterations.

6

Figure 1: Experiments on MEG/EEG brain imaging dataset (dense data with n “ 360, p “ 22494
and q “ 20). On the left: fraction of active variables as a function of λ and the number of iterations
K. The GAP Safe strategy has a much longer range of λ with (red) small active sets. On the right:
Computation time to reach convergence using different screening strategies.

Note that we have not performed comparison with the sequential screening rule commonly acknowl-
edge as the state-of-the-art “safe” screening rule (such as th EDDP+ [21]), since we can show that
this kind of rule is not safe. Indeed, the stopping criterion is based on dual gap accuracy, and com-
parisons would be unfair since such methods sometimes do not converge to the prescribed accuracy.
This is backed-up by a counter example given in the supplementary material. Nevertheless, modiﬁ-
cations of such rules, inspired by our GAP Safe rules, can make them safe. However the obtained
sequential rules are still outperformed by our dynamic strategies (see Figure 2 for an illustration).

4.1

(cid:96)1{(cid:96)2 multi-task regression

To demonstrate the beneﬁt of the GAP Safe screening rule for a multi-task Lasso problem we used
neuroimaging data. Electroencephalography (EEG) and magnetoencephalography (MEG) are brain
imaging modalities that allow to identify active brain regions. The problem to solve is a multi-task
regression problem with squared loss where every task corresponds to a time instant. Using a multi-
task Lasso one can constrain the recovered sources to be identical during a short time interval [13].
This corresponds to a temporal stationary assumption. In this experiment we used a joint MEG/EEG
data with 301 MEG and 59 EEG sensors leading to n “ 360. The number of possible sources is
p “ 22, 494 and the number of time instants q “ 20. With a 1 kHz sampling rate it is equivalent to
say that the sources stay the same for 20 ms.
Results are presented in Figure 1. The GAP Safe rule is compared with the dynamic safe rule
from [6]. The experimental setup consists in estimating the solutions of the multi-task Lasso problem
for 100 values of λ on a logarithmic grid from λmax to λmax{103. For the experiments on the left
a ﬁxed number of iterations from 2 to 211 is allowed for each λ. The fraction of active variables
is reported. Figure 1 illustrates that the GAP Safe rule screens out much more variables than the
compared method, as well as the converging nature of our safe regions. Indeed, the more iterations
performed the more the rule allows to screen variables. On the right, computation time conﬁrms the
effective speed-up. Our rule signiﬁcantly improves the computation time for all duality gap tolerance
from 10´2 to 10´8, especially when accurate estimates are required, e.g., for feature selection.

4.2

(cid:96)1 binary logistic regression

Results on the Leukemia dataset are reported in Figure 2. We compare the dynamic strategy of GAP
Safe to a sequential and non dynamic rule such as Slores [22]. We do not compare to the actual
Slores rule as it requires the previous dual optimal solution, which is not available. Slores is indeed
not a safe method (see Section B in the supplementary materials). Nevertheless one can observe that
dynamic strategies outperform pure sequential one, see Section C in the supplementary material).

7

Figure 2: (cid:96)1 regularized binary logistic regression on the Leukemia dataset (n = 72 ; m = 7,129 ;
q = 1). Simple sequential and full dynamic screening GAP Safe rules are compared. On the left:
fraction of the variables that are active. Each line corresponds to a ﬁxed number of iterations for
which the algorithm is run. On the right: computation times needed to solve the logistic regression
path to desired accuracy with 100 values of λ.

(cid:96)1{(cid:96)2 multinomial logistic regression

4.3
We also applied GAP Safe to an (cid:96)1{(cid:96)2 multinomial logistic regression problem on a sparse dataset.
Data are bag of words features extracted from the News20 dataset (TF-IDF removing English stop
words and words occurring only once or more than 95% of the time). One can observe on Figure 3
the dynamic screening and its beneﬁt as more iterations are performed. GAP Safe leads to a sig-
niﬁcant speedup: to get a duality gap smaller than 10´2 on the 100 values of λ, we needed 1,353 s
without screening and only 485 s when GAP Safe was activated.

Figure 3: Fraction of the variables that are ac-
tive for (cid:96)1{(cid:96)2 regularized multinomial logistic
regression on 3 classes of the News20 dataset
(sparse data with n = 2,757 ; m = 13,010 ;
q = 3). Computation was run on the best 10%
of the features using χ2 univariate feature se-
lection [16]. Each line corresponds to a ﬁxed
number of iterations for which the algorithm is
run.

5 Conclusion

This contribution detailed new safe rules for accelerating algorithms solving generalized linear mod-
els regularized with (cid:96)1 and (cid:96)1{(cid:96)2 norms. The rules proposed are safe, easy to implement, dynamic
and converging, allowing to discard signiﬁcantly more variables than alternative safe rules. The
positive impact in terms of computation time was observed on all tested datasets and demonstrated
here on a high dimensional regression task using brain imaging data as well as binary and multiclass
classiﬁcation problems on dense and sparse data. Extensions to other generalized linear model,
e.g., Poisson regression, are expected to reach the same conclusion. Future work could investigate
optimal screening frequency, determining when the screening has correctly detected the support.

Acknowledgment

We acknowledge the support from Chair Machine Learning for Big Data at T´el´ecom ParisTech and
from the Orange/T´el´ecom ParisTech think tank phi-TAB. This work beneﬁted from the support of
the ”FMJH Program Gaspard Monge in optimization and operation research”, and from the support
to this program from EDF.

8

No screeningGAP Safe (sequential)GAP Safe (dynamic)No screeningGAP Safe (sequential)GAP Safe (dynamic)References
[1] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In NIPS, pages 41–48,

2006.

[2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learn-

ing, 73(3):243–272, 2008.

[3] H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert

spaces. Springer, New York, 2011.

[4] M. Blondel, K. Seki, and K. Uehara. Block coordinate descent algorithms for large-scale

sparse multiclass classiﬁcation. Machine Learning, 93(1):31–52, 2013.

[5] A. Bonnefoy, V. Emiya, L. Ralaivola, and R. Gribonval. A dynamic screening principle for the

lasso. In EUSIPCO, 2014.

[6] A. Bonnefoy, V. Emiya, L. Ralaivola, and R. Gribonval. Dynamic Screening: Accelerat-
IEEE Trans. Signal Process.,

ing First-Order Algorithms for the Lasso and Group-Lasso.
63(19):20, 2015.

[7] P. B¨uhlmann and S. van de Geer. Statistics for high-dimensional data. Springer Series in

Statistics. Springer, Heidelberg, 2011. Methods, theory and applications.

[8] B. Efron, T. Hastie, I. M. Johnstone, and R. Tibshirani. Least angle regression. Ann. Statist.,

32(2):407–499, 2004. With discussion, and a rejoinder by the authors.

[9] L. El Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learn-

ing. J. Paciﬁc Optim., 8(4):667–698, 2012.

[10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large

linear classiﬁcation. J. Mach. Learn. Res., 9:1871–1874, 2008.

[11] O. Fercoq, A. Gramfort, and J. Salmon. Mind the duality gap: safer rules for the lasso. In

ICML, pages 333–342, 2015.

[12] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models

via coordinate descent. Journal of statistical software, 33(1):1, 2010.

[13] A. Gramfort, M. Kowalski, and M. H¨am¨al¨ainen. Mixed-norm estimates for the M/EEG inverse

problem using accelerated gradient methods. Phys. Med. Biol., 57(7):1937–1961, 2012.

[14] J.-B. Hiriart-Urruty and C. Lemar´echal. Convex analysis and minimization algorithms. II,

volume 306. Springer-Verlag, Berlin, 1993.

[15] K. Koh, S.-J. Kim, and S. Boyd. An interior-point method for large-scale l1-regularized logistic

regression. J. Mach. Learn. Res., 8(8):1519–1555, 2007.

[16] C. D. Manning and H. Sch¨utze. Foundations of Statistical Natural Language Processing. MIT

Press, Cambridge, MA, USA, 1999.

[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res.,
12:2825–2830, 2011.

[18] R. Tibshirani. Regression shrinkage and selection via the lasso. JRSSB, 58(1):267–288, 1996.
[19] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. J. Tibshirani. Strong

rules for discarding predictors in lasso-type problems. JRSSB, 74(2):245–266, 2012.

[20] R. J. Tibshirani. The lasso problem and uniqueness. Electron. J. Stat., 7:1456–1490, 2013.
[21] J. Wang, P. Wonka, and J. Ye. Lasso screening rules via dual polytope projection. arXiv

preprint arXiv:1211.3966, 2012.

[22] J. Wang, J. Zhou, J. Liu, P. Wonka, and J. Ye. A safe screening rule for sparse logistic regres-

sion. In NIPS, pages 1053–1061, 2014.

[23] Z. J. Xiang, Y. Wang, and P. J. Ramadge. Screening tests for lasso problems. arXiv preprint

arXiv:1405.4897, 2014.

[24] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

JRSSB, 68(1):49–67, 2006.

9

