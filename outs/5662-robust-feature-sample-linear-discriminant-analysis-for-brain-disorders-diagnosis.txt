Robust Feature-Sample Linear Discriminant Analysis

for Brain Disorders Diagnosis

Ehsan Adeli-Mosabbeb, Kim-Han Thung, Le An, Feng Shi, Dinggang Shen, for the ADNI∗

Department of Radiology and BRIC

University of North Carolina at Chapel Hill, NC, 27599, USA

{eadeli,khthung,le_an,fengshi,dgshen}@med.unc.edu

Abstract

A wide spectrum of discriminative methods is increasingly used in diverse appli-
cations for classiﬁcation or regression tasks. However, many existing discrimi-
native methods assume that the input data is nearly noise-free, which limits their
applications to solve real-world problems. Particularly for disease diagnosis, the
data acquired by the neuroimaging devices are always prone to different sources
of noise. Robust discriminative models are somewhat scarce and only a few at-
tempts have been made to make them robust against noise or outliers. These
methods focus on detecting either the sample-outliers or feature-noises. More-
over, they usually use unsupervised de-noising procedures, or separately de-noise
the training and the testing data. All these factors may induce biases in the learn-
ing process, and thus limit its performance. In this paper, we propose a classiﬁ-
cation method based on the least-squares formulation of linear discriminant anal-
ysis, which simultaneously detects the sample-outliers and feature-noises. The
proposed method operates under a semi-supervised setting, in which both labeled
training and unlabeled testing data are incorporated to form the intrinsic geometry
of the sample space. Therefore, the violating samples or feature values are iden-
tiﬁed as sample-outliers or feature-noises, respectively. We test our algorithm on
one synthetic and two brain neurodegenerative databases (particularly for Parkin-
son’s disease and Alzheimer’s disease). The results demonstrate that our method
outperforms all baseline and state-of-the-art methods, in terms of both accuracy
and the area under the ROC curve.

1

Introduction

Discriminative methods pursue a direct mapping from the input to the output space for a classi-
ﬁcation or a regression task. As an example, linear discriminant analysis (LDA) aims to ﬁnd the
mapping that reduces the input dimensionality, while preserving the most class discriminatory in-
formation. Discriminative methods usually achieve good classiﬁcation results compared to the gen-
erative models, when there are enough number of training samples. But they are limited when there
are small number of labeled data, as well as when the data is noisy. Various efforts have been made
to add robustness to these methods. For instance, [17] and [9] proposed robust Fisher/linear discrim-
inant analysis methods, and [19] introduced a worst-case LDA, by minimizing the upper bound of
the LDA cost function. These methods are all robust to sample-outliers. On the other hand, some
methods were proposed to deal with the intra-sample-outliers (or feature-noises), such as [12, 15].
∗Parts of the data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimag-
ing Initiative (ADNI) database (http://adni.loni.ucla.edu). The investigators within the ADNI con-
tributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or
writing of this paper. A complete listing of ADNI investigators can be found at: http://adni.loni.
ucla.edu/wp-content/uploads/howtoapply/ADNIAcknowledgementList.pdf.

1

min

βββ

(cid:107)H(Ytr − βββXtr)(cid:107)2
F,

As in many previous works, de-noising the training and the testing data are often conducted sepa-
rately. This might induce a bias or inconsistency to the whole learning process. Besides, for many
real-world applications, it is a cumbersome task to acquire enough training samples to perform a
proper discriminative analysis. Hence, we propose to take advantage of the unlabeled testing data
available, to build a more robust classiﬁer. To this end, we introduce a semi-supervised discrimi-
native classiﬁcation model, which, unlike previous works, jointly estimates the noise model (both
sample-outliers and feature-noises) on the whole labeled training and unlabeled testing data and
simultaneously builds a discriminative model upon the de-noised training data.
In this paper, we introduce a novel classiﬁcation model based on LDA, which is robust against
both sample-outliers and feature-noises, and hence, here, it is called robust feature-sample linear
discriminant analysis (RFS-LDA). LDA ﬁnds the mapping between the sample space and the label
space through a linear transformation matrix, maximizing a so-called Fisher discriminant ratio [17].
In practice, the major drawback of the original LDA is the small sample size problem, which arises
when the number of available training samples is less than the dimensionality of the feature space
[18]. A reformulation of LDA based on the reduced-rank least-squares problem (known LS-LDA)
[10] tackles this problem. LS-LDA ﬁnds the mapping βββ ∈ Rl×d by solving the following problem1:
(1)
where Ytr ∈ Rl×Ntr is a binary class label indicator matrix, for l different classes (or labels), and
Xtr ∈ Rd×Ntr is the matrix containing Ntr d-dimensional training samples. H is a normalization
factor deﬁned as H = (YtrY(cid:62)
tr)−1/2 that compensates for the different number of samples in each
class [10]. As a result, the mapping βββ is a reduced rank transformation matrix [10, 15], which could
be used to project a test data xtst ∈ Rd×1 onto a l dimensional space. The class label could therefore
be simply determined using a k-NN strategy.
To make LDA robust against noisy data, Fidler et al. [12] proposed to construct a basis, which con-
tains complete discriminative information for classiﬁcation. In the testing phase, the estimated basis
identiﬁes the outliers in samples (images in their case) and then is used to calculate the coefﬁcients
using a subsampling approach. On the other hand, Huang et al. [15] proposed a general formulation
for robust regression (RR) and classiﬁcation (robust LDA or RLDA). In the training stage, they de-
noise the feature values using a strategy similar to robust principle component analysis (RPCA) [7]
and build the above LS-LDA model using the de-noised data. In the testing stage, they de-noise the
data by performing a locally compact representation of the testing samples from the de-noised train-
ing data. This separate de-noising procedure could not effectively form the underlying geometry of
sample space to de-noise the data. Huang et al. [15] only account for feature-noise by imposing a
sparse noise model constraint on the features matrix. On the other hand, the data ﬁtting term in (1)
is vulnerable to large sample-outliers. Recently, in robust statistics, it is found that (cid:96)1 loss functions
are able to make more reliable estimations [2] than (cid:96)2 least-squares ﬁtting functions. This has been
adopted in many applications, including robust face recognition [28] and robust dictionary learning
[22]. Reformulating the objective in (1), using this idea, would yield to this problem:

(cid:107)H(Ytr − βββXtr)(cid:107)1.

min

βββ

(2)

We incorporate this ﬁtting function in our formulation to deal with the sample-outliers by iteratively
re-weighting each single sample, while simultaneously de-noising the data from feature-noises. This
is done through a semi-supervised setting to take advantage of all labeled and unlabeled data to build
the structure of the sample space more robustly. Semi-supervised learning [8, 34] has long been of
great interest in different ﬁelds, because it can make use of unlabeled or poorly labeled data. For
instance, Joulin and Bach [16] introduced a convex relaxation and use the model in different semi-
supervised learning scenarios. In another work, Cai et al. [5] proposed a semi-supervised discrim-
inant analysis, where the separation between different classes is maximized using the labeled data
points, while the unlabeled data points estimate the structure of the data. In contrast, we incorporate
the unlabeled testing data to form the intrinsic geometry of the sample space and de-noise the data,
whilst building the discriminative model.

1Bold capital letters denote matrices (e.g., D). All non-bold letters denote scalar variables. dij is the scalar
in the row i and column j of D. (cid:104)d1, d2(cid:105) denotes the inner product between d1 and d2. (cid:107)d(cid:107)2
2 and (cid:107)d(cid:107)1
represent the squared Euclidean Norm and the (cid:96)1 norm of d, respectively. (cid:107)D(cid:107)2
ij dij and
(cid:107)D(cid:107)∗ designate the squared Frobenius Norm and the nuclear norm (sum of singular values) of D, respectively.

F = tr(D(cid:62)D) =(cid:80)

2



X = [Xtr Xtst] ∈ Rd×N

x11 x12 . . . x1Ntr
x21 x22 . . . x2Ntr
x31 x32 . . . x3Ntr

x1Ntr +1 . . . x1N
x2Ntr +1 . . . x2N
x3Ntr +1 . . . x3N

.
.
.

.
.
.

. . .

.
.
.

xd1 xd2 . . . xdNtr

xdNtr +1 . . . xdN

.
.
.

. . .

.
.
.



=




D = [Dtr Dtst] ∈ Rd×N

d11 d12 . . . d1Ntr
d21 d22 . . . d2Ntr
d31 d32 . . . d3Ntr

d1Ntr +1 . . . d1N
d2Ntr +1 . . . d2N
d3Ntr +1 . . . d3N

.
.
.

.
.
.

. . .

.
.
.

.
.
.

. . .

.
.
.

dd1 dd2 . . . ddNtr

ddNtr +1 . . . ddN





+

E ∈ Rd×N

e11 e12 . . . e1N
e21 e22 . . . e2N
e31 e32 . . . e3N

.
.
.

.
.
.

. . .

.
.
.

ed1 ed2 . . . edN





Mapping βββ

y11 y12 . . . y1Ntr

.
.
.

. . .

.
.
.
.
.
.
yl1 yl2 . . . ylNtr
Ytr ∈ Rl×Ntr

Figure 1: Outline of the proposed method: The original data matrix, X, is composed of both labeled
training and unlabeled testing data. Our method decomposes this matrix to a de-noised data matrix,
D, and an error matrix, E, to account for feature-noises. Simultaneously, we learn a mapping from
the de-noised training samples in D (Dtr) through a robust (cid:96)1 ﬁtting function, dealing with the
sample-outliers. The same learned mapping on the testing data, Dtst, leads to the test labels.

We apply our method for the diagnosis of neurodegenerative brain disorders. The term neurodegen-
erative disease is an umbrella term for debilitating and incurable conditions related to progressive
degeneration or death of the cells in the brain nervous system. Although neurodegenerative diseases
manifest with diverse pathological features, the cellular level processes resemble similar structures.
For instance, Parkinson’s disease (PD) mainly affects the basal ganglia region and the substansia
nigra sub-region of the brain, leading to decline in generation of a chemical messenger, dopamine.
Lack of dopamine yields loss of ability to control body movements, along with some non-motor
problems (e.g., depression, anxiety) [35].
In Alzheimer’s disease (AD), deposits of tiny protein
plaques yield into brain damage and progressive loss of memory [26]. These diseases are often
incurable and thus, early diagnosis and treatment are crucial to slow down the progression of the
disease in its initial stages. In this study, we use two popular databases: PPMI and ADNI. The
former aims at investigating PD and its related disorders, while the latter is designed for diagnosing
AD and its prodormal stage, known as mild cognitive impairment (MCI).
Contributions: The contribution of this paper would therefore be multi-fold: (1) We propose an
approach to deal with the sample-outliers and feature-noises simultaneously, and build a robust dis-
criminative classiﬁcation model. The sample-outliers are penalized through an (cid:96)1 ﬁtting function,
by re-weighing the samples based on their prediction power, while discarding the feature-noises.
(2) Our proposed model operates under a semi-supervised setting, where the whole data (labeled
training and unlabeled testing samples) are incorporated to build the intrinsic geometry of the sam-
ple space, which leads to better de-noising the data. (3) We further select the most discriminative
features for the learning process through regularizing the weights matrix with an (cid:96)1 norm. This is
speciﬁcally of great interest for the neurodegenerative disease diagnosis, where the features from
different regions of the brain are extracted, but not all the regions are associated with a certain dis-
ease. Therefore, the most discriminative regions in the brain that utmost affect the disease would be
identiﬁed, leading to a more reliable diagnosis model.

2 Robust Feature-Sample Linear Discriminant Analysis (RFS-LDA)

Let’s assume we have Ntr training and Ntst testing samples, each with a d-dimensional feature
vector, which leads to a set of N = Ntr + Ntst total samples. Let X ∈ Rd×N denote the set of all
samples (both training and testing), in which each column indicates a single sample, and yi ∈ R1×N
their corresponding ith labels. In general, with l different labels, we can deﬁne Y ∈ Rl×N . Thus,
X and Y are composed by stacking up the training and testing data as: X = [Xtr Xtst] and
Y = [Ytr Ytst]. Our goal is to determine the labels of the test samples, Ytst ∈ Rl×Ntst.
Formulation: An illustration of the proposed method is depicted in Fig 1. First, all the samples
(labeled or unlabeled) are arranged into a matrix, X. We are interested in de-noising this matrix.
Following [14, 21], this could be done by assuming that X can be spanned on a low-rank subspace
and therefore should be rank-deﬁcient. This assumption supports the fact that samples from same
classes should be more correlated [14, 15]. Therefore, the original matrix X is decomposed into two

3

counterparts, D and E, which represent the de-noised data matrix and the error matrix, respectively,
similar to RPCA [7]. The de-noised data matrix shall hold the low-rank assumption and the error
matrix is considered to be sparse. But, this process of de-noising does not incorporate the label
information and is therefore unsupervised. Nevertheless, note that we also seek a mapping between
the de-noised training samples and their respective labels. So, matrix D should be spanned on a
low-rank subspace, which also leads to a good classiﬁcation model of its sub-matrix, Dtr.
To ensure the rank-deﬁciency of the matrix D, like in many previous works [7, 14, 21], we approx-
imate the rank function using the nuclear norm (the sum of the singular values of the matrix). The
noise is modeled using the (cid:96)1 norm of the matrix, which ensures a sparse noise model on the feature
values. Accordingly, the objective function for RFS-LDA under a semi-supervised setting would be:

min

βββ,D, ˆD,E

(cid:107)H(Ytr − βββ ˆD)(cid:107)1 + (cid:107)D(cid:107)∗ + λ1(cid:107)E(cid:107)1 + λ2R(βββ),

η
2

s.t. D = X + E, ˆD = [Dtr; 1(cid:62)],

(3)

where the ﬁrst term is the (cid:96)1 regression model introduced in (2). This term only operates on the de-
noised training samples from matrix D with a row of all 1s is added to it, to ensure an appropriate
linear classiﬁcation model. The second and the third terms together with the ﬁrst constraint are
similar to the RPCA formulation [7]. They de-noise the labeled training and unlabeled testing data
together.
In combination with the ﬁrst term, we ensure that the de-noised data also provides a
favorable regression/classiﬁcation model. The last term is a regularization on the learned mapping
coefﬁcients to ensure the coefﬁcients do not get trivial or unexpectedly large values. The parameters
η, λ1 and λ2 are constant regularization parameters, which are discussed in more details later.
The regularization on the coefﬁcients could be posed as a simple norm of the βββ matrix. But, in many
applications like ours (disease diagnosis) many of the features in the feature vectors are redundant. In
practice, features from different brain regions are often extracted, but not all the regions contribute
to a certain disease. Therefore, it is desirable to determine which features (regions) are the most
relevant and the most discriminative to use. Following [11, 26, 28], we are looking for a sparse set
of weights that ensures incorporating the least and the most discriminative features. We propose a
regularization on the weights vector as a combination of the (cid:96)1 and Frobenius norms:

R(βββ) = (cid:107)βββ(cid:107)1 + γ(cid:107)βββ(cid:107)F.

(4)

Evidently, the solution to the objective function in (3) is not easy to achieve, since the ﬁrst term
contains a quadratic term and minimization of the (cid:96)1 ﬁtting function is not straightforward (because
of its indifferentiability). To this end, we formalize the solution with a similar strategy as in iter-
atively re-weighted least squares (IRLS) [2]. The (cid:96)1 minimization problem is approximated by a
conventional (cid:96)2 least-squares, in which each of the samples in the ˆD matrix are weighted with the
reverse of their regression residual. Therefore the new problem would be:

min

βββ,D, ˆD,E

(cid:107)H(Ytr − βββ ˆD)ˆααα(cid:107)2

F + (cid:107)D(cid:107)∗ + λ1(cid:107)E(cid:107)1 + λ2R(βββ),

η
2

s.t. D = X + E, ˆD = [Dtr; 1(cid:62)].

where ˆααα is a diagonal matrix, the ith diagonal element of which is the ith sample’s weight:

(cid:113)
(yi − βββ ˆdi)2 + δ, ∀ i, j ∈ {0, . . . , Ntr}, i (cid:54)= j, ˆαααij = 0,

ˆαααii = 1/

(5)

(6)

where δ is a very small positive number (equal to 0.0001 in our experiments). In the next subsection,
we introduce an algorithm to solve this optimization problem.
Our work is closely related to the RR and RLDA formulations in [15], where the authors impose a
low-rank assumption on the training data feature values and an (cid:96)1 assumption on the noise model.
The discriminant model is learned similar to LS-LDA, as illustrated in (1), while a sample-weighting
strategy is employed to achieve a more robust model. On the other hand, our model operates under a
semi-supervised learning setting, where both the labeled training and the unlabeled testing samples
are de-noised simultaneously. Therefore, the geometry of the sample space is better modeled on the
low-dimensional subspace, by interweaving both labeled training and unlabeled testing data. In ad-
dition, our model further selects the most discriminative features to learn the regression/classiﬁcation
model, by regularizing the mapping weights vector and enforcing an sparsity condition on them.

4

Algorithm 1 RFS-LDA optimization algorithm.

1 =

(cid:46) Main optimization loop
(cid:46) Update βββ

Input: X = [Xtr Xtst], Ytr, parameters η, λ1, λ2, ρ and γ.
Initialization: D0 = [Xtr Xtst], ˆD0 = [Xtr; 1(cid:62)], βββ0 = Ytr( ˆD0)(cid:62)( ˆD0( ˆD0)(cid:62) + γI), E0 = 0, L 0
X/(cid:107)X(cid:107)2, L 0

3 = βββ0/(cid:107)βββ0(cid:107)2, µ1 = dN

2 = Xtr/(cid:107)Xtr(cid:107)2, L 0

4 (cid:107)Xtr(cid:107)1, µ3 = dc

4 (cid:107)X(cid:107)1, µ2 = dNtr

4 (cid:107)βββ0(cid:107)1.

i − ˆβββt ˆdk

( ˆDk)(cid:62) + µ3(Bk − L k

t ← 0, ˆβββ0 = βββk
repeat

(cid:113)
( ˆDk)(cid:62) + γI(cid:1), t ← t + 1
∀ i, j ∈ {0, . . . , Ntr − 1}, i (cid:54)= j, ˆαααij ← 0 and ˆαααii ← 1/
(cid:62)
tr; 1(cid:62)](cid:1)
ˆDk+1](1:Ntr ,:) 0(cid:3)(cid:1)

3 )(cid:1)(cid:0) ˆDk ˆαααˆααα
2 I(cid:1)−1(cid:0)η ˆααα
1 (X − Ek) +(cid:2)[L k

(βββk+1)(cid:62)Ytr − L k
2 + µk
2

ˆβββt+1 ←(cid:0)Ytr ˆαααˆααα
ˆDk+1 ←(cid:0)η ˆααα

1: k ← 0
2: repeat
3:
4:
5:
6:
7:
8:
(βββk+1)(cid:62)βββk+1 ˆααα + µk
9:
10:
1 + µk
2 )
(X − Dk+1 + L k
11:
(βββk+1 + L k
12:
3 )
1 (X − Dk+1 − Ek+1)
13:
2 ( ˆD − [Dk+1
14:
tr
2 ← min(ρµk
15:
1 , 109), µk+1
16:
17: until (cid:107)X − Dk − Ek(cid:107)F/(cid:107)X(cid:107)F < 10−8 and (cid:107) ˆDk − [Dk

until (cid:107)ˆβββt−1 − ˆβββt(cid:107)F/((cid:107)ˆβββt−1(cid:107)F × (cid:107)ˆβββt(cid:107)F) < 0.001 or t > 100
βββk+1 ← ˆβββt.
(cid:62)
Dk+1 ← D1/(µk
Ek+1 ← Sλ1/µk
Bk+1 ← Sλ2/µk
1 ← L k
L k+1
1 + µk
2 ← L k
L k+1
2 + µk
1 ← min(ρµk
µk+1
k ← k + 1

; 1(cid:62)]), L k+1

(cid:0)L k

i )2 + 0.0001

1 + µk

1 )
1 /µk

3 (βββ − B)
3 + µk
3 ← min(ρµk

3 ← L k
2 , 109), µk+1
tr ; 1(cid:62)](cid:107)F/(cid:107) ˆDk(cid:107)F < 10−8 and (cid:107)βββk − Bk(cid:107)F/(cid:107)βββk(cid:107)F < 10−8

3 , 109)

2 [Dk

2 + µk

(cid:46) Update ˆD
(cid:46) Update D
(cid:46) Update E
(cid:46) Update B
(cid:46) Update multipliers and parameters

(yk

(cid:62)

(cid:62)

1

3

Output: βββ, D, E and Ytst = βββXtst.

Optimization: Problem (5) could be efﬁciently solved using the augmented Lagrangian multipliers
(ALM) approach. Hence, we introduce the Lagrangian multipliers, L1 ∈ Rd×N , L2 ∈ R(d+1)×Ntr
and L3 ∈ Rl×(d+1), an auxiliary variable, B ∈ Rl×(d+1), and write the Lagrangian function as:

L(βββ, B, D, ˆD, E) =

(cid:107)H(Ytr − βββ ˆD)ˆααα(cid:107)2
η
2
+ (cid:104)L1, X − D − E(cid:105) +
(cid:107) ˆD − [Dtr; 1(cid:62)](cid:107)2

+

µ2
2

F + (cid:107)D(cid:107)∗ + λ1(cid:107)E(cid:107)1 + λ2((cid:107)B(cid:107)1 + γ(cid:107)βββ(cid:107)F)

(cid:107)X − D − E(cid:107)2
µ1
2
F + (cid:104)L3, βββ − B(cid:105) +

F + (cid:104)L2, ˆD − [Dtr; 1(cid:62)](cid:105)
µ3
2

(cid:107)βββ − B(cid:107)2
F,

(7)

where µ1, µ2 and µ3 are penalty parameters. There are ﬁve variables (βββ, B, D, ˆD and E) contribut-
ing to the problem. We alternatively optimize for each variable, while ﬁxing the others. Except for
the matrix βββ, all the variables have straightforward or closed-form solutions. βββ is calculated through
IRLS [2], by iteratively calculating the weights in ˆααα and solving the conventional least-squares
problem, until convergence.
The detailed optimization steps are given in Algorithm 1. The normalization factor H is omitted
in this algorithm, for easier readability. In this algorithm, I is the identity matrix and the operators
Dτ (.) and Sκ(.) are deﬁned in the following. Dτ (A) = UDτ (ΣΣΣ)V∗ applies singular value thresh-
olding algorithm [6] on the intermediate matrix ΣΣΣ, as Dτ (ΣΣΣ) = diag({(σi − τ )+}), where UΣΣΣV∗
is the singular values decomposition (SVD) of A and σis are the singular values. Additionally,
Sκ(a) = (a − κ)+ − (−a − κ)+ is the soft thresholding operator or the proximal operator for the
(cid:96)1 norm [3]. Note that s+ is the positive part of s, deﬁned as s+ = max(0, s).
Algorithm analysis: The solution for each of the matrices B, D, ˆD, E is a convex function, while
all the other variables are ﬁxed. For βββ, the solution is achieved via the IRLS approach, in an iterative
manner. Both the (cid:96)1 ﬁtting function and the approximated re-weighted least-squares functions are
convex. We only need to ensure that the minimization of the latter is numerically better tractable
than the minimization of the former. This is discussed in depth and the convergence is proved in [2].
To estimate the computational complexity of the algorithm, we need to investigate the complexity of
the sub-procedures of the algorithm. The two most computationally expensive steps in the loop are
the iterative update of βββ (Algorithm 1, Steps 4-7) and the SVT operation (Algorithm 1, Step 10). The
former includes solving a least-squares iteratively, which is O(d2N ) in each iteration and the latter
has the SVD operation as the most computational intensive operation, which is of O(d2N + N 3).

5

By considering the maximum number of iterations for the ﬁrst sub-procedure equal to tmax = 100,
the overall computational complexity of the algorithm in each iteration would be O(100d2N + N 3).
The number of iterations of the whole algorithm until convergence is dependent on the choice of
{µ}s. If µ penalty parameters are increasing smoothly in each iteration (as in Step 15, Algorithm 1),
the overall algorithm would be Q-linearly convergent. A reasonable choice for the sequence of all
{µ}s yields in a decrease in the number of required SVD operations [1, 21].

3 Experiments

We compare our method with several baseline and state-of-the-art methods in three different scenar-
ios. The ﬁrst experiment is on synthetic data, which highlights how the proposed method is robust
against sample-outliers or feature-noises, separately or when they occur at the same time. The next
two experiments are conducted for neurodegenerative brain disorders diagnosis. We use two popular
databases, one for Parkinson’s disease (PD) and the other for Alzheimer’s disease (AD).
We compare our results with different baseline methods, including: Conventional LS-LDA [10],
RLDA [15], RPCA on the X matrix separately to de-noise and then LS-LDA for the classiﬁca-
tion (denoted as RPCA+LS-LDA) [15], linear support vector machines (SVM), and sparse feature
selection with SVM (SFS+SVM) or with RLDA (SFS+RLDA). Except for RPCA+LDA, the other
methods in comparison do not incorporate the testing data. In order to have a fair set of comparisons,
we also compare against the transductive matrix completion (MC) approach [14]. Additionally, to
also evaluate the effect of the regularization on matrix βββ, we report results for RFS-LDA when regu-
larized by only γ(cid:107)βββ(cid:107)F (denoted as RFS-LDA∗), instead of the term introduced in (4). Moreover, we
also train our proposed RFS-LDA in a fully supervised setting, i.e., not involving any testing data
in the training process, to show the effect of the established semi-supervised learning framework in
our proposed method. This is simply done by replacing variable X in (3) with Xtr and solving the
problem correspondingly. This method, referred to as S-RFS-LDA, only uses the training data to
form the geometry of the sample space and, therefore, only cleans the training feature-noises.
For the choice of parameters, the best parameters are selected through an inner 10-fold cross valida-
set with a same strategy as in [15]: λ1 = Λ1/((cid:112)min(d, N )), λ2 = Λ2/
tion on the training data, for all the competing methods. For the proposed method, the parameters are
d, ηk = Λ3(cid:107)X(cid:107)∗/(cid:107)Ytr − βββk ˆDk(cid:107)2
F,
and ρ (controlling the {µ}s in the algorithm) is set to 1.01. We have set Λ1, Λ2, Λ3 and γ through
inner cross validation, and found that all set to 1 yields to reasonable results across all datasets.
Synthetic Data: We construct two independent 100-dimensional subspaces, with bases U1 and U2
(same as described in [21]). U1 ∈ R100×100 is a random orthogonal matrix and U2 = TU1, in
which T is a random rotation matrix. Then, 500 vectors are sampled from each subspace through
Xi = UiQi, i = {1, 2}, with Qi, a 100 × 500 matrix, independent and identically distributed
(i.i.d.) from N (0, 1). This leads to a binary classiﬁcation problem. We gradually add additional
noisy samples and features to the data, drawn i.i.d from N (0, 1), and evaluate our proposed method.
The accuracy means and standard deviations of three different runs are illustrated in Fig. 2. This
experiment is conducted under three settings: (1) First, we analyze the behavior of the method
against gradually added noise to some of the features (feature-noises), illustrated in Fig. 2a. (2)
We randomly add some noisy samples to the aforementioned noise-free samples and evaluate the
methods in the sole presence of sample-outliers. Results are depicted in Fig. 2b. (3) Finally, we
simultaneously add noisy features and samples. Fig. 2c shows the mean±std accuracy as a function
of the additional number of noisy features and samples. Note that all the reported results are obtained
through 10-fold cross-validation. As can be seen, our method is able to select a better subset of
features and samples and achieve superior results compared to RLDA and conventional LS-LDA
approaches. Furthermore, our method behaves more robust against the increase in the noise factor.
Brain neurodegenrative disease diagnosis databases: The ﬁrst set of data used in this paper is
obtained from the Parkinson’s progression markers initiative (PPMI) database2 [23]. PPMI is the
ﬁrst substantial study for identifying the PD progression biomarkers to advance the understanding
of the disease. In this research, we use the MRI data acquired by the PPMI study, in which a T1-
weighted, 3D sequence (e.g., MPRAGE or SPGR) is acquired for each subject using 3T SIEMENS
MAGNETOM TrioTim syngo scanners. We use subjects scanned using MPRAGE sequence to

√

2http://www.ppmi-info.org/data

6

100

)

%

(

y
c
a
r
u
c
c
A

90

80

70

RFS-LDA

RLDA
LS-LDA

100

80

60

100

80

60

0
# of added noisy features

100

200

0
# of added noisy samples

100

200

0

100

200

# of added noisy samples and features

(a) Only added noisy features

(b) Only added noisy samples

(c) Added noisy samples & features

Figure 2: Results comparisons on synthetic data, for three different runs (mean±std).

Table 1: The accuracy (ACC) and area under ROC curve (AUC) of the PD/NC classiﬁcation on
PPMI database, compared to the baseline methods.

RFS-LDA RFS-LDA∗
84.1
0.87

78.3
0.81

ACC
AUC

Method

S-RFS-LDA RLDA SFS+RLDA RPCA+LS-LDA LS-LDA
56.6
0.59

73.4
0.80

75.8
0.80

71.0
0.79

59.4
0.64

SVM SFS+SVM
55.2
61.5
0.59
0.56

MC
61.5
68.8

minimize the effect of different scanning protocols. The T1-weighted images were acquired for
176 sagittal slices with the following parameters: repetition time = 2300 ms, echo time = 2.98 ms,
ﬂip angle = 9◦, and voxel size = 1 × 1 × 1 mm3. All the MR images were preprocessed by skull
stripping [29], cerebellum removal, and then segmented into white matter (WM), gray matter (GM),
and cerebrospinal ﬂuid (CSF) tissues [20]. The anatomical automatic labeling atlas [27], parcellated
with 90 predeﬁned regions of interest (ROI), was registered using HAMMER3 [25, 30] to each
subject’s native space. We further added 8 more ROIs in basal ganglia and brainstem regions, which
are clinically important ROIs for PD. We then computed WM, GM and CSF tissue volumes in each
of the 98 ROIs as features. 56 PD and 56 normal control (NC) subjects are used in our experiments.
The second dataset is from Alzheimer’s disease neuroimaging initiative (ADNI) study4, including
MRI and FDG-PET data. For this experiment, we used 93 AD patients, 202 MCI patients and 101
NC subjects. To process the data, same tools employed in [29] and [32] are used, including spatial
distortion, skull-stripping, and cerebellum removal. The FSL package [33] was used to segment
each MR image into three different tissues, i.e., GM, WM, and CSF. Then, 93 ROIs are parcellated
for each subject [25] with atlas warping. The volume of GM tissue in each ROI was calculated as
the image feature. For FDG-PET images, a rigid transformation was employed to align it to the
corresponding MR image and the mean intensity of each ROI was calculated as the feature. All
these features were further normalized in a similar way, as in [32].
Results: The ﬁrst experiment is set up on the PPMI database. Table 1 shows the diagnosis accuracy
of the proposed technique (RFS-LDA) in comparisons with different baseline and state-of-the-art
methods, using a 10-fold cross-validation strategy. As can be seen, the proposed method outperforms
all others. This could be because our method deals with both feature-noises and sample-outliers.
Note that, subjects and their corresponding feature vectors extracted from MRI data are quite prone
to noise, because of many possible sources of noise (e.g. the patient’s body movements, RF emission
due to thermal motion, overall MR scanner measurement chain, or preprocessing artifacts). There-
fore, some samples might not be useful (sample-outliers) and some might be contaminated by some
amounts of noise (feature-noises). Our method deals with both types and achieves good results.
The goal for the experiments on ADNI database is to discriminate both MCI and AD patients from
NC subjects, separately. Therefore, NC subjects form our negative class, while the positive class
is deﬁned as AD in one experiment and MCI in the other. The diagnosis results of the AD vs. NC
and MCI vs. NC experiments are reported in Tables 2. As it could be seen, in comparisons with
the state-of-the-art, our method achieves good results in terms of both accuracy and the area under
curve. This is because we successfully discard the sample-outliers and detect the feature-noises.

3Could be downloaded at http://www.nitrc.org/projects/hammerwml
4http://www.loni.ucla.edu/ADNI

7

Table 2: The accuracy (ACC) and the area under ROC curve (AUC) of the Alzheimer’s disease
classiﬁcation on ADNI database, compared to the baseline methods.

AD/NC ACC
AUC
MCI/NC ACC
AUC

RFS-LDA RFS-LDA∗
91.8
0.98
89.8
0.93

89.1
0.96
85.6
0.90

Method

S-RFS-LDA RLDA SFS+RLDA RPCA+LS-LDA LS-LDA
70.9
0.81
68.9
0.75

90.1
0.98
88.1
0.92

87.6
0.93
84.5
0.87

86.3
0.95
84.5
0.90

88.7
0.96
85.0
0.87

SVM SFS+SVM
76.3
72.1
0.83
0.80
76.1
70.1
0.79
0.80

MC
78.2
0.82
74.3
0.78

Figure 3: The top selected ROIs for AD vs. NC (left) and MCI vs. NC (right) classiﬁcation problems.

Discussions: In medical imaging applications, many sources of noise (e.g. patient’s movement,
radiations and limitation of imaging devices, preprocessing artifacts) contribute to the acquired data
[13], and therefore methods that deal with noise and outliers are of great interest. Our method
enjoys from a single optimization objective that can simultaneously suppress sample-outliers and
feature-noises, which compared to the competing methods, exhibits a good performance. One of
the interesting functions of the proposed method is the regularization on the mapping coefﬁcients
with the (cid:96)1 norm, which would select a compact set of features to contribute to the learned mapping.
The magnitude of the coefﬁcients would show the level of contribution of that speciﬁc feature to the
learned model. In our application, the features from the whole brain regions are extracted, but only a
small number of regions are associated with the disease (e.g., AD, MCI or PD). Using this strategy,
we can determine which brain regions are highly associated with a certain disease.
Fig. 3 shows the top regions selected by our algorithm in AD vs. NC and MCI vs. NC classiﬁcation
scenarios. These regions, including middle temporal gyrus, medial front-orbital gyrus, postcentral
gyrus, caudate nucleus, cuneus, and amygdala have been reported to be associated with AD and
MCI in the literature [24, 26]. The ﬁgures show the union of regions selected for both MRI and
FDG-PET features. The most frequently used regions for the PD/NC experiment are the substantial
nigra (left and right), putamen (right), middle frontal gyrus (right), superior temporal gyrus (left),
which are also consistent with the literature [4, 31]. This selection of brain regions could be further
incorporated for future clinical analysis.
The semi-supervised setting of the proposed method is also of great interest in the diagnosis of
patients. When new patients ﬁrst arrive and are to be diagnosed, the previous set of the patients with
no certain diagnosis so far (not labeled yet), could still be used to build a more reliable classiﬁer.
In other words, the current testing samples could contribute the diagnosis of future subjects, as
unlabeled samples.

4 Conclusion

In this paper, we proposed an approach for discriminative classiﬁcation, which is robust against
both sample-outliers and feature-noises. Our method enjoys a semi-supervised setting, where all
the labeled training and the unlabeled testing data are used to detect outliers and are de-noised,
simultaneously. We have applied our method to the interesting problem of neurodegenerative brain
disease diagnosis and directly applied it for the diagnosis of Parkinson’s and Alzheimer’s diseases.
The results show that our method outperforms all competing methods. As a direction for the future
work, one can develop a multi-task learning reformulation of the proposed method to incorporate
multiple modalities for the subjects, or extend the method for the incomplete data case.

8

References
[1] E. Adeli-Mosabbeb and M. Fathy. Non-negative matrix completion for action detection. Image Vision

Comput., 39:38 – 51, 2015.

[2] N. Bissantz, L. D¨umbgen, A. Munk, and B. Stratmann. Convergence analysis of generalized iteratively
reweighted least squares algorithms on convex function spaces. SIAM Optimiz., 19(4):1828–1845, 2009.
[3] S. Boyd and et al.. Distributed optimization and statistical learning via the alternating direction method

of multipliers. Found. Trends Mach. Learn., 3(1):1–122, 2011.

[4] Heiko Braak, Kelly Tredici, Udo Rub, Rob de Vos, Ernst Jansen Steur, and Eva Braak. Staging of brain

pathology related to sporadic parkinsons disease. Neurobio. of Aging, 24(2):197 – 211, 2003.

[5] D. Cai, X. He, and J. Han. Semi-supervised discriminant analysis. In CVPR, 2007.
[6] J.-F. Cai, E. Cand`es, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Optimiz., 20(4):1956–1982, 2010.

[7] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 58(3), 2011.
[8] O. Chapelle, B. Sch¨olkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, 2006.
[9] C. Croux and C. Dehon. Robust linear discriminant analysis using s-estimators. Canadian J. of Statistics,

29(3):473–493, 2001.

[10] F. De la Torre. A least-squares framework for component analysis. IEEE TPAMI, 34(6):1041–1055, 2012.
[11] E. Elhamifar and R. Vidal. Robust classiﬁcation using structured sparse representation. In CVPR, 2011.
[12] S. Fidler, D. Skocaj, and A. Leonardis. Combining reconstructive and discriminative subspace methods

for robust classiﬁcation and regression by subsampling. IEEE TPAMI, 28(3):337–350, 2006.

[13] V. Fritsch, G. Varoquaux, B. Thyreau, J.-B. Poline, and B. Thirion. Detecting outliers in high-dimensional

neuroimaging datasets with robust covariance estimators. Med. Image Anal., 16(7):1359 – 1370, 2012.

[14] A. Goldberg, X. Zhu, B. Recht, J.-M. Xu, and R. Nowak. Transduction with matrix completion: Three

birds with one stone. In NIPS, pages 757–765, 2010.

[15] D. Huang, R. Cabral, and F. De la Torre. Robust regression. In ECCV, pages 616–630, 2012.
[16] A. Joulin and F. Bach. A convex relaxation for weakly supervised classiﬁers. In ICML, 2012.
[17] S. Kim, A. Magnani, and S. Boyd. Robust Fisher discriminant analysis. In NIPS, pages 659–666, 2005.
[18] H. Li, T. Jiang, and K. Zhang. Efﬁcient and robust feature extraction by maximum margin criterion. In

NIPS, pages 97–104, 2003.

[19] H. Li, C. Shen, A. van den Hengel, and Q. Shi. Worst-case linear discriminant analysis as scalable

semideﬁnite feasibility problems. IEEE TIP, 24(8), 2015.

[20] K.O. Lim and A. Pfefferbaum. Segmentation of MR brain images into cerebrospinal ﬂuid spaces, white

and gray matter. J. of Computer Assisted Tomography, 13:588–593, 1989.

[21] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. Robust recovery of subspace structures by low-rank

representation. IEEE TPAMI, 35(1):171–184, 2013.

[22] C. Lu, J. Shi, and J. Jia. Online robust dictionary learning. In CVPR, pages 415–422, June 2013.
[23] K. Marek and et al.. The parkinson progression marker initiative (PPMI). Prog. Neurobiol., 95(4):629 –

635, 2011.

[24] B. Pearce, A. Palmer, D. Bowen, G. Wilcock, M. Esiri, and A. Davison. Neurotransmitter dysfunction

and atrophy of the caudate nucleus in alzheimer’s disease. Neurochem Pathol., 2(4):221–32, 1985.

[25] D. Shen and C. Davatzikos. HAMMER: Hierarchical attribute matching mechanism for elastic registra-

tion. IEEE TMI, 21:1421–1439, 2002.

[26] K.-H. Thung, C.-Y. Wee, P.-T. Yap, and D. Shen. Neurodegenerative disease diagnosis using incomplete

multi-modality data via matrix shrinkage and completion. NeuroImage, 91:386–400, 2014.

[27] N. Tzourio-Mazoyer and et al.. Automated anatomical labeling of activations in SPM using a macro-
scopic anatomical parcellation of the MNI MRI single-subject brain. NeuroImage, 15(1):273–289, 2002.
[28] A. Wagner, J. Wright, A. Ganesh, Zihan Zhou, and Yi Ma. Towards a practical face recognition system:

Robust registration and illumination by sparse representation. In CVPR, pages 597–604, 2009.

[29] Y. Wang, J. Nie, P.-T. Yap, G. Li, F. Shi, X. Geng, L. Guo, D. Shen, ADNI, et al. Knowledge-guided
robust MRI brain extraction for diverse large-scale neuroimaging studies on humans and non-human
primates. PLOS ONE, 9(1):e77810, 2014.

[30] Y. Wang, J. Nie, P.-T. Yap, F. Shi, L. Guo, and D. Shen. Robust deformable-surface-based skull-stripping

for large-scale studies. In MICCAI, volume 6893, pages 635–642, 2011.

[31] A. Worker and et al.. Cortical thickness, surface area and volume measures in parkinson’s disease,

multiple system atrophy and progressive supranuclear palsy. PLOS ONE, 9(12), 2014.

[32] D. Zhang, Y. Wang, L. Zhou, H. Yuan, D. Shen, ADNI, et al. Multimodal classiﬁcation of Alzheimer’s

disease and mild cognitive impairment. NeuroImage, 55(3):856–867, 2011.

[33] Y. Zhang, M. Brady, and S. Smith. Segmentation of brain MR images through a hidden Markov random

ﬁeld model and the expectation-maximization algorithm. IEEE TMI, 20(1):45–57, 2001.

[34] Xiaojin Zhu. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences,

[35] D. Ziegler and J. Augustinack. Harnessing advances in structural MRI to enhance research on Parkinson’s

University of Wisconsin-Madison, 2005.

disease. Imaging Med., 5(2):91–94, 2013.

9

