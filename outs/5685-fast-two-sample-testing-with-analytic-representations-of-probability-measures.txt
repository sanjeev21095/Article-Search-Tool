Fast Two-Sample Testing with Analytic
Representations of Probability Measures

Kacper Chwialkowski

Gatsby Computational Neuroscience Unit, UCL

kacper.chwialkowski@gmail.com

Aaditya Ramdas

Dept. of EECS and Statistics, UC Berkeley

aramdas@cs.berkeley.edu

Dino Sejdinovic

Dept of Statistics, University of Oxford
dino.sejdinovic@gmail.com

Arthur Gretton

Gatsby Computational Neuroscience Unit, UCL

arthur.gretton@gmail.com

Abstract

We propose a class of nonparametric two-sample tests with a cost linear in the
sample size. Two tests are given, both based on an ensemble of distances be-
tween analytic functions representing each of the distributions. The ﬁrst test uses
smoothed empirical characteristic functions to represent the distributions, the sec-
ond uses distribution embeddings in a reproducing kernel Hilbert space. Analytic-
ity implies that differences in the distributions may be detected almost surely at a
ﬁnite number of randomly chosen locations/frequencies. The new tests are consis-
tent against a larger class of alternatives than the previous linear-time tests based
on the (non-smoothed) empirical characteristic functions, while being much faster
than the current state-of-the-art quadratic-time kernel-based or energy distance-
based tests. Experiments on artiﬁcial benchmarks and on challenging real-world
testing problems demonstrate that our tests give a better power/time tradeoff than
competing approaches, and in some cases, better outright power than even the
most expensive quadratic-time tests. This performance advantage is retained even
in high dimensions, and in cases where the difference in distributions is not ob-
servable with low order statistics.

1 Introduction

Testing whether two random variables are identically distributed without imposing any parametric
assumptions on their distributions is important in a variety of scientiﬁc applications. These include
data integration in bioinformatics [6], benchmarking for steganography [20] and automated model
checking [19]. Such problems are addressed in the statistics literature via two-sample tests (also
known as homogeneity tests).
Traditional approaches to two-sample testing are based on distances between representations of the
distributions, such as density functions, cumulative distribution functions, characteristic functions or
mean embeddings in a reproducing kernel Hilbert space (RKHS) [27, 26]. These representations are
inﬁnite dimensional objects, which poses challenges when deﬁning a distance between distributions.
Examples of such distances include the classical Kolmogorov-Smirnov distance (sup-norm between
cumulative distribution functions); the Maximum Mean Discrepancy (MMD) [9], an RKHS norm
of the difference between mean embeddings, and the N-distance (also known as energy distance)
[34, 31, 4], which is an MMD-based test for a particular family of kernels [25] . Tests may also be
based on quantities other than distances, an example being the Kernel Fisher Discriminant (KFD)
[12], the estimation of which still requires calculating the RKHS norm of a difference of mean
embeddings, with normalization by an inverse covariance operator.

1

In contrast to consistent two-sample tests, heuristics based on pseudo-distances, such as the dif-
ference between characteristic functions evaluated at a single frequency, have been studied in the
context of goodness-of-ﬁt tests [13, 14]. It was shown that the power of such tests can be maximized
against fully speciﬁed alternative hypotheses, where test power is the probability of correctly reject-
ing the null hypothesis that the distributions are the same. In other words, if the class of distributions
being distinguished is known in advance, then the tests can focus only at those particular frequen-
cies where the characteristic functions differ most. This approach was generalized to evaluating the
empirical characteristic functions at multiple distinct frequencies by [8], thus improving on tests
that need to know the single “best” frequency in advance (the cost remains linear in the sample size,
albeit with a larger constant). This approach still fails to solve the consistency problem, however:
two distinct characteristic functions can agree on an interval, and if the tested frequencies fall in that
interval, the distributions will be indistinguishable.
In Section 2 of the present work, we introduce two novel distances between distributions, which
both use a parsimonious representation of the probability measures. The ﬁrst distance builds on
the notion of differences in characteristic functions with the introduction of smooth characteristic
functions, which can be though of as the analytic analogues of the characteristics functions. A
distance between smooth characteristic functions evaluated at a single random frequency is almost
surely a distance (Deﬁnition 1 formalizes this concept) between these two distributions. In other
words, there is no need to calculate the whole inﬁnite dimensional representation - it is almost
surely sufﬁcient to evaluate it at a single random frequency (although checking more frequencies will
generally result in more powerful tests). The second distance is based on analytic mean embeddings
of two distributions in a characteristic RKHS; again, it is sufﬁcient to evaluate the distance between
mean embeddings at a single randomly chosen point to obtain almost surely a distance. To our
knowledge, this representation is the ﬁrst mapping of the space of probability measures into a ﬁnite
dimensional Euclidean space (in the simplest case, the real line) that is almost surely an injection,
and as a result almost surely a metrization. This metrization is very appealing from a computational
viewpoint, since the statistics based on it have linear time complexity (in the number of samples)
and constant memory requirements.
We construct statistical tests in Section 3, based on empirical estimates of differences in the analytic
representations of the two distributions. Our tests have a number of theoretical and computational
advantages over previous approaches. The test based on differences between analytic mean embed-
dings is a.s. consistent for all distributions, and the test based on differences between smoothed
characteristic functions is a.s. consistent for all distributions with integrable characteristic func-
tions (contrast with [8], which is only consistent under much more onerous conditions, as discussed
above). This same weakness was used by [1] in justifying a test that integrates over the entire fre-
quency domain (albeit at cost quadratic in the sample size), for which the quadratic-time MMD is
a generalization [9]. Compared with such quadratic time tests, our tests can be conducted in linear
time – hence, we expect their power/computation tradeoff to be superior.
We provide several experimental benchmarks (Section 4) for our tests. First, we compare test power
as a function of computation time for two real-life testing settings: amplitude modulated audio
samples, and the Higgs dataset, which are both challenging multivariate testing problems. Our
tests give a better power/computation tradeoff than the characteristic function-based tests of [8],
the previous sub-quadratic-time MMD tests [11, 32], and the quadratic-time MMD test. In terms
of power when unlimited computation time is available, we might expect worse performance for
the new tests, in line with ﬁndings for linear- and sub-quadratic-time MMD-based tests [15, 9, 11,
32]. Remarkably, such a loss of power is not the rule: for instance, when distinguishing signatures
of the Higgs boson from background noise [3] (’Higgs dataset’), we observe that a test based on
differences in smoothed empirical characteristic functions outperforms the quadratic-time MMD.
This is in contrast to linear- and sub-quadratic-time MMD-based tests, which by construction are less
powerful than the quadratic-time MMD. Next, for challenging artiﬁcial data (both high-dimensional
distributions, and distributions for which the difference is very subtle), our tests again give a better
power/computation tradeoff than competing methods.

2 Analytic embeddings and distances

In this section we consider mappings from the space of probability measures into a sub-space of
real valued analytic functions. We will show that evaluating these maps at J randomly selected

2

points is almost surely injective for any J > 0. Using this result, we obtain a simple (randomized)
metrization of the space of probability measures. This metrization is used in the next section to
construct linear-time nonparametric two-sample tests.
To motivate our approach, we begin by recalling an integral family of distances between distribu-
tions, denoted Maximum Mean Discrepancies (MMD) [9]. The MMD is deﬁned as

MMD(P, Q) = sup

f2BkZE

f dP  ZE

f dQ  ,

(1)

(3)

(4)

where P and Q are probability measures on E, and Bk is the unit ball in the RKHS Hk associated
with a positive deﬁnite kernel k : E ⇥ E ! R. A popular choice of k is the Gaussian kernel
k(x, y) = exp( kx  yk2/ 2) with bandwidth parameter  . It can be shown that the MMD is equal
to the RKHS distance between so called mean embeddings,
(2)

MMD(P, Q) = kµP   µQkHk ,
where µP is an embedding of the probability measure P to Hk,
µP (t) =ZE

k(x, t)dP (x),

and k·k Hk denotes the norm in the RKHS Hk. When k is translation invariant, i.e., k (x, y) =
(x   y), the squared MMD can be written [27, Corollary 4]

MMD2(P, Q) =ZRd |'P (t)   'Q(t)|2 F  1(t)dt,

where F denotes the Fourier transform, F  1 is the inverse Fourier transform, and 'P , 'Q are
the characteristic functions of P , Q, respectively. From [27, Theorem 9], a kernel k is called
characteristic when the MMD for Hk satisﬁes

MMD(P, Q) = 0 iff P = Q.

(5)
Any bounded, continuous, translation-invariant kernel whose inverse Fourier transform is almost
everywhere non-zero is characteristic [27]. By representation (2), it is clear that the MMD with a
characteristic kernel is a metric.
Pseudometrics based on characteristic functions. A practical limitation when using the MMD
in testing is that an empirical estimate is expensive to compute, this being the sum of two U-statistics
and an empirical average, with cost quadratic in the sample size [9, Lemma 6]. We might instead
consider a ﬁnite dimensional approximation to the MMD, achieved by estimating the integral (4),
with the random variable

d2
',J (P, Q) =

1
J

JXj=1

|'P (Tj)   'Q(Tj)|2,

(6)

where {Tj}J
j=1 are sampled independently from the distribution with a density function F  1. This
type of approximation is applied to various kernel algorithms under the name of random Fourier
features [21, 17]. In the statistical testing literature, the quantity d',J (P, Q) predates the MMD by
a considerable time, and was studied in [13, 14, 8], and more recently revisited in [33]. Our ﬁrst
proposition is that d2
',J (P, Q) can be a poor choice of distance between probability measures, as it
fails to distinguish a large class of measures. The following result is proved in the Appendix.
Proposition 1. Let J 2 N and let {Tj}J
j=1 be a sequence of real valued i.i.d. random variables
with a distribution which is absolutely continuous with respect to the Lebesgue measure. For any
0 <✏< 1, there exists an uncountable set A of mutually distinct probability measures (on the real
line) such that for any P, Q 2A , P d2

We are therefore motivated to ﬁnd distances of the form (6) that can distinguish larger classes of
distributions, yet remain efﬁcient to compute. These distances are characterized as follows:
Deﬁnition 1 (Random Metric). A random process d with values in R, indexed with pairs from the
set of probability measures M, i.e., d = {d(P, Q) : P, Q 2M} , is said to be a random metric
if it satisﬁes all the conditions for a metric with qualiﬁcation ‘almost surely’. Formally, for all
P, Q, R 2M , random variables d(P, Q), d(P, R), d(R, Q) must satisfy

',J (P, Q) = 0    1   ✏.

3

1. d(P, Q)   0 a.s.
2. if P = Q, then d(P, Q) = 0 a.s, if P 6= Q then d(P, Q) 6= 0 a.s.
3. d(P, Q) = d(Q, P ) a.s.
4. d(P, Q)  d(P, R) + d(R, Q) a.s. 1

From the statistical testing point of view, the coincidence axiom of a metric d, d(P, Q) = 0 if and
only if P = Q, is key, as it ensures consistency against all alternatives. The quantity d',J (P, Q) in
(6) violates the coincidence axiom, so it is only a random pseudometric (other axioms are trivially
satisﬁed). We remedy this problem by replacing the characteristic functions by smooth characteristic
functions:
Deﬁnition 2. A smooth characteristic function  P (t) of a measure P is a characteristic function of
P convolved with an analytic smoothing kernel l, i.e.

 P (t) =ZRd

'P (w)l(t   w)dw,

t 2 Rd.

(7)

Proposition 3 shows that smooth characteristic function can be estimated in a linear time. The
analogue of d',J (P, Q) for smooth characteristic functions is simply

d2
 ,J (P, Q) =

1
J

JXj=1

| P (Tj)    Q(Tj)|2,

(8)

where {Tj}J
j=1 are sampled independently from the absolutely continuous distribution (returning
to our earlier example, this might be F  1(t) if we believe this to be an informative choice). The
following theorem, proved in the Appendix, demonstrates that the smoothing greatly increases the
class of distributions we can distinguish.
Theorem 1. Let l be an analytic, integrable kernel with an inverse Fourier transform that is non-
zero almost everywhere. Then, for any J > 0, d ,J is a random metric on the space of probability
measures with integrable characteristic functions, and  P is an analytic function.

This result is primarily a consequence of analyticity of smooth characteristic functions and the fact
that analytic functions are ’well behaved’. There is an additional, practical advantage to smoothing:
when the variability in the difference of the characteristic functions is high, and these differences are
local, smoothing distributes the difference in CFs more broadly in the frequency domain (a simple
illustration is in Fig. A.1, Appendix), making it easier to ﬁnd by measurement at a small number of
randomly chosen points. This accounts for the observed improvements in test power in Section 4,
over differences in unsmoothed CFs.
Metrics based on mean embeddings. The key step which leads us to the construction of a random
metric d ,J is the convolution of the original characteristic functions with an analytic smoothing ker-
nel. This idea need not be restricted to the representations of probability measures in the frequency
domain. We may instead directly convolve the probability measure with a positive deﬁnite kernel k
(that need not be translation invariant), yielding its mean embedding into the associated RKHS,

µP (t) =ZE

k(x, t)dP (x).

(9)

We say that a positive deﬁnite kernel k : RD⇥RD ! R is analytic on its domain if for all x 2 RD,
the feature map k(x,·) is an analytic function on RD. By using embeddings with characteristic and
analytic kernels, we obtain particularly useful representations of distributions. As for the smoothed
CF case, we deﬁne

(µP (Tj)   µQ(Tj))2.
The following theorem ensures that dµ,J (P, Q) is also a random metric.

d2
µ,J (P, Q) =

1
J

JXj=1

(10)

1 Note that this does not imply that realizations of d are distances on M, but it does imply that they are

almost surely distances for all arbitrary ﬁnite subsets of M.

4

Theorem 2. Let k be an analytic, integrable and characteristic kernel. Then for any J > 0, dµ,J is
a random metric on the space of probability measures (and µP is an analytic function).

Note that this result is stronger than the one presented in Theorem 1, since it is not restricted to the
class of probability measures with integrable characteristic functions. Indeed, the assumption that
the characteristic function is integrable implies the existence and boundedness of a density. Recall-
ing the representation of MMD in (2), we have proved that it is almost always sufﬁcient to measure
difference between µP and µQ at a ﬁnite number of points, provided our kernel is characteristic
and analytic. In the next section, we will see that metrization of the space of probability measures
using random metrics dµ,J, d ,J is very appealing from the computational point of view. It turns
out that the statistical tests that arise from these metrics have linear time complexity (in the number
of samples) and constant memory requirements.
3 Hypothesis Tests Based on Distances Between Analytic Functions

In this section, we provide two linear-time two-sample tests: ﬁrst, a test based on analytic mean
embeddings, and next a test based on smooth characteristic functions. We further describe the
relation with competing alternatives. Proofs of all propositions are in Appendix B.
Difference in analytic functions In the previous section we described the random metric based
JPJ
on a difference in analytic mean embeddings, d2
j=1(µP (Tj)   µQ(Tj))2. If we
nPn
replace µP with the empirical mean embedding ˆµP = 1
i=1 k(Xi,·) it can be shown that for any
j=1, under the null hypothesis, as n ! 1,
sequence of unique {tj}J
(ˆµP (tj)   ˆµQ(tj))2

µ,J (P, Q) = 1

pn

(11)

JXj=1

j=1, it is
converges in distribution to a sum of correlated chi-squared variables. Even for ﬁxed {tj}J
very computationally costly to obtain quantiles of this distribution, since this requires a bootstrap
or permutation procedure. We will follow a different approach based on Hotelling’s T 2-statistic
[16]. The Hotelling’s T 2-squared statistic of a normally distributed, zero mean, Gaussian vector
W = (W1,··· , WJ ), with a covariance matrix ⌃, is T 2 = W ⌃ 1W . The compelling property of
the statistic is that it is distributed as a  2-random variable with J degrees of freedom. To see a link
between T 2 and equation (11), consider a random variablePJ
j : this is also distributed as a
sum of correlated chi-squared variables. In our case W is replaced with a difference of normalized
empirical mean embeddings, and ⌃ is replaced with the empirical covariance of the difference of
mean embeddings. Formally, let Zi denote the vector of differences between kernels at tests points
Tj,

i=j W 2

Zi = (k(Xi, T1)   k(Yi, T1),··· , k(Xi, TJ )   k(Yi, TJ )) 2 RJ .

(12)
i=1 Zi, and its covariance matrix

We deﬁne the vector of mean empirical differences Wn = 1
⌃n = 1

nPi(Zi   Wn)(Zi   Wn)T . The test statistic is

nPn

n Wn.

Sn = nWn⌃ 1

(13)
The computation of Sn requires inversion of a J ⇥ J matrix ⌃n, but this is fast and numerically
stable: J will typically be small, and is less than 10 in our experiments. The next proposition
demonstrates the use of Sn as a two-sample test statistic.
Proposition 2 (Asymptotic behavior of Sn). Let d2
i=1 and {Yi}n
µ,J (P, Q) = 0 a.s. and let {Xi}n
i=1
be i.i.d. samples from P and Q respectively. If ⌃ 1
n exists for n large enough, then the statistic Sn
is a.s. asymptotically distributed as a  2-random variable with J degrees of freedom (as n ! 1
with d ﬁxed). If d2

µ,J (P, Q) > 0 a.s., then a.s. for any ﬁxed r, P(Sn > r) ! 1 as n ! 1 .

We now apply the above proposition to obtain a statistical test.
Test 1 (Analytic mean embedding ). Calculate Sn. Choose a threshold r↵ corresponding to the 1 ↵
quantile of a  2 distribution with J degrees of freedom, and reject the null hypothesis whenever Sn
is larger than r↵.

5

j=1 to evaluate the differences

i=1 and {Yi}n

There are a number of valid sampling schemes for the test points {Tj}J
in mean embeddings: see Section 4 for a discussion.
Difference in smooth characteristic functions From the convolution deﬁnition of a smooth char-
acteristic function (7) it is not immediately obvious how to calculate its estimator in linear time. In
the next proposition, however, we show that a smooth characteristic function is an expected value of
some function (with respect to the given measure), which can be estimated in a linear time.
Proposition 3. Let k be an integrable translation-invariant kernel and f its inverse Fourier trans-

form. Then the smooth characteristic function of P can be written as  P (t) =RRd eit>xf (x)dP (x).

It is now clear that a test based on the smooth characteristic functions is similar to the test based on
mean embeddings. The main difference is in the deﬁnition of the vector of differences Zi:
Zi = (f (Xi) sin(XiT1) f (Yi) sin(YiT1), f (Xi) cos(XiT1) f (Yi) cos(YiT1),··· ) 2 R2J (14)
The imaginary and real part of the ep 1T >j Xif (Xi)  ep 1T >j Yif (Yi) are stacked together, in order
to ensure that Wn, ⌃n and Sn as all real-valued quantities.
Proposition 4. Let d2
i=1 be i.i.d. samples from P and
Q respectively. Then the statistic Sn is almost surely asymptotically distributed as a  2-random
variable with 2J degrees of freedom (as n ! 1 with J ﬁxed). If d2
 ,J (P, Q) > 0 , then almost
surely for any ﬁxed r, P (Sn > r) ! 1 as n ! 1.
Other tests. The test [8] based on empirical characteristic functions was constructed originally
for one test point and then generalized to many points - it is quite similar to our second test, but
does not perform smoothing (it is also based on a T 2-Hotelling statistic). The block MMD [32] is
a sub-quadratic test, which can be trivially linearized by ﬁxing the block size, as presented in the
Appendix. Finally, another alternative is the MMD, an inherently quadratic time test. We scale
MMD to linear time by sub-sampling our data set, and choosing only pn points, so that the MMD
complexity becomes O(n). Note, however, that the true complexity of MMD involves a permutation
calculation of the null distribution at cost O(bnn), where the number of permutations bn grows with
n. See Appendix C for a detailed description of alternative tests.

 ,J (P, Q) = 0 and let {Xi}n

4 Experiments

In this section we compare two-sample tests on both artiﬁcial benchmark data and on real-world
data. We denote the smooth characteristic function test as ‘Smooth CF’, and the test based on the
analytic mean embeddings as ‘Mean Embedding’. We compare against several alternative testing ap-
proaches: block MMD (‘Block MMD’), a characteristic functions based test (‘CF’), a sub-sampling
MMD test (‘MMD(pn)’), and the quadratic-time MMD test (‘MMD(n)’).
Experimental setup. For all the experiments, D is the dimensionality of samples in a dataset, n
is a number of samples in the dataset (sample size) and J is number of test frequencies. Parameter
selection is required for all the tests. The table summarizes the main choices of the parameters made
for the experiments. The ﬁrst parameter is the test function, used to calculate the particular statistic.
The scalar   represents the length-scale of the observed data. Notice that for the kernel tests we
recover the standard parameterization exp( k x
). The original CF test
was proposed without any parameters, hence we added   to ensure a fair comparison - for this test
varying   is equivalent to adjusting the variance of the distribution of frequencies Tj. For all tests,
the value of the scaling parameter   was chosen so as to minimize a p-value estimate on a held-out
training set: details are described in Appendix D. We chose not to optimize the sampling scheme
for the Mean Embedding and Smooth CF tests, since this would give them an unfair advantage over
the Block MMD, MMD(pn) and CF tests. The block size in the Block MMD test and the number
of test frequencies in the Mean Embedding, Smooth CF, and CF tests, were always set to the same
value (not greater than 10) to maintain exactly the same time complexity. Note that we did not use
the popular median heuristic for kernel bandwidth choice (MMD and B-test), since it gives poor
results for the Blobs and AM Audio datasets [11]. We do not run MMD(n) test for ’Simulation 1’
or ’Amplitude Modulated Music’, since the sample size is 10000, and too large for a quadratic-time
test with permutation sampling for the test critical value.

 k2) = exp( kx yk2

    y

 2

6

Figure 1: Higgs dataset. Left: Test power vs. sample size. Right: Test power vs. execution time.

It is important to verify that Type I error is indeed at the design level, set at ↵ = 0.05 in this paper.
This is veriﬁed in the Appendix, Figure A.2. Also shown in the plots is the 95% percent conﬁdence
intervals for the results, as averaged over 4000 runs.

Test
Mean Embedding
Smooth CF
MMD(n),MMD(pn)
Block MMD
CF

Test Function

exp(it>  1x   k  1x   tk2)

exp( k  1(x   t)k2)
exp( k  1(x   t)k2)
exp( k  1(x   t)k2)

exp(it>  1x)

Sampling scheme
Tj ⇠ N (0D, ID)
Tj ⇠ N (0D, ID)
not applicable
not applicable
Tj ⇠ N (0D, ID)

Other parameters

J - no. of test frequencies
J - no. of test frequencies

b -bootstraps
B-block size

J - no. of test frequencies

Real Data 1: Higgs dataset, D = 4, n varies, J = 10. The ﬁrst experiment we consider is on
the UCI Higgs dataset [18] described in [3] - the task is to distinguish signatures of processes that
produce Higgs bosons from background processes that do not. We consider a two-sample test on
certain extremely low-level features in the dataset - kinematic properties measured by the particle
detectors, i.e., the joint distributions of the azimuthal angular momenta ' for four particle jets. We
denote by P the jet '-momenta distribution of the background process (no Higgs bosons), and by
Q the corresponding distribution for the process that produces Higgs bosons (both are distributions
on R4). As discussed in [3, Fig. 2], '-momenta, unlike transverse momenta pT , carry very little
discriminating information for recognizing whether Higgs bosons were produced. Therefore, we
would like to test the null hypothesis that the distributions of angular momenta P (no Higgs boson
observed) and Q (Higgs boson observed) might yet be rejected. The results for different algorithms
are presented in the Figure 1. We observe that the joint distribution of the angular momenta is in
fact discriminative. Sample size varies from 1000 to 12000. The Smooth CF test has signiﬁcantly
higher power than the other tests, including the quadratic-time MMD, which we could only run
on up to 5100 samples due to computational limitations. The leading performance of the Smooth
CF test is especially remarkable given it is several orders of magnitude faster than the quadratic-
time MMD(n), even though we used the fastest quadratic-time MMD implementation, where the
asymptotic distribution is approximated by a Gamma density .
Real Data 2: Amplitude Modulated Music, D = 1000, n = 10000, J = 10. Amplitude mod-
ulation is the earliest technique used to transmit voice over the radio. In the following experiment
observations were one thousand dimensional samples of carrier signals that were modulated with
two different input audio signals from the same album, song P and song Q (further details of these
data are described in [11, Section 5]). To increase the difﬁculty of the testing problem, independent
Gaussian noise of increasing variance (in the range 1 to 4.0) was added to the signals. The results
are presented in the Figure 2. Compared to the other tests, the Mean Embedding and Smooth CF
tests are more robust to the moderate noise contamination.
Simulation 1: High Dimensions, D varies, n = 10000, J = 3. It has recently been shown, in
theory and in practice, that the two-sample problem gets more difﬁcult for an increasing number of
dimensions increases on which the distributions do not differ [22, 23]. In the following experiment,
we study the power of the two-sample tests as a function of dimension of the samples. We run two-
sample tests on two datasets of Gaussian random vectors which differ only in the ﬁrst dimension,

Dataset I: P = N (0D, ID)
Dataset II: P = N (0D, ID)

vs.
vs.

Q = N ((1, 0,··· , 0), ID)
Q = N (0D, diag((2, 1,··· , 1))) ,

7

Figure 2: Music Dataset.Left: Test power vs. added noise. Right: four samples from P and Q.

Figure 3: Power vs. redundant dimensions comparison for tests on high dimensional data.

where 0d is a D-dimensional vector of zeros, ID is a D-dimensional identity matrix, and diag(v)
is a diagonal matrix with v on the diagonal. The number of dimensions (D) varies from 50 to
2500 (Dataset I) and from 50 to 1200 (Dataset II). The power of the different two-sample tests is
presented in Figure 3. The Mean Embedding test yields best performance for both datasets, where
the advantage is especially large for differences in variance.
Simulation 2: Blobs, D = 2, n varies, J = 5. The Blobs dataset is a grid of two dimensional
Gaussian distributions (see Figure 4), which is known to be a challenging two-sample testing task.
The difﬁculty arises from the fact that the difference in distributions is encoded at a much smaller
lengthscale than the overall data. In this experiment both P and Q are four by four grids of Gaus-
sians, where P has unit covariance matrix in each mixture component, while each component of Q
has direction of the largest variance rotated by ⇡/4 and ampliﬁed to 4. It was demonstrated by [11]
that a good choice of kernel is crucial for this task. Figure 4 presents the results of two-sample tests
on the Blobs dataset. The number of samples varies from 50 to 14000 ( MMD(n) reached test power
one with n = 1400). We found that the MMD(n) test has the best power as function of the sample
size, but the worst power/computation tradeoff. By contrast, random distance based tests have the
best power/computation tradeoff.

Acknowledgment. We would like thank Bharath Sriperumbudur and Wittawat Jitkrittum for in-
sightful comments.

Figure 4: Blobs Dataset. Left: test power vs. sample size. Center: test power vs. execution time.
Right: illustration of the blob dataset.

8

[10] A. Gretton, K. Fukumizu, Z. Harchaoui, and B. Sriperumbudur. A fast, consistent kernel two-sample test.

[11] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and K. Fuku-

mizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[12] Z. Harchaoui, F.R. Bach, and E. Moulines. Testing for Homogeneity with Kernel Fisher Discriminant

[13] CE Heathcote. A test of goodness of ﬁt for symmetric random variables. Aust J stat, 14(2):172–181,

[14] CR Heathcote. The integrated squared error estimation of parameters. Biometrika, 64(2):255–264, 1977.
[15] H.-C. Ho and G. Shieh. Two-stage U-statistics for hypothesis testing. Scandinavian Journal of Statistics,

13:723–773, 2012.

In NIPS, 2009.

Analysis. In NIPS. 2008.

1972.

33(4):861–873, 2006.

References
[1] V. Alba Fern´andez, M. Jim´enez-Gamero, and J. Mu˜noz Garcia. A test for the two-sample problem based
on empirical characteristic functions. Computational Statistics and Data Analysis, 52:3730–3748, 2008.

[2] T. W. Anderson. An Introduction to Multivariate Statistical Analysis. Wiley, July 2003.
[3] P. Baldi, P. Sadowski, and D. Whiteson. Searching for exotic particles in high-energy physics with deep

learning. Nature Communications, 5, 2014.

[4] L Baringhaus and C Franz. On a new multivariate two-sample test. J mult anal, 88(1):190–206, 2004.
[5] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statis-

tics, volume 3. Kluwer Academic Boston, 2004.

[6] K.M. Borgwardt, A. Gretton, M.J. Rasch, H.-P. Kriegel, B. Sch¨olkopf, and A. Smola. Integrating struc-

tured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49–e57, 2006.

[7] K. R. Davidson. Pointwise limits of analytic functions. Am math mon, pages 391–394, 1983.
[8] T.W. Epps and K.J. Singleton. An omnibus test for the two-sample problem using the empirical charac-

teristic function. Journal of Statistical Computation and Simulation., 26(3-4):177–203, 1986.

[9] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. Smola. A kernel two-sample test. JMLR,

[16] H. Hotelling. The generalization of student’s ratio. Ann. Math. Statist., 2(3):360–378, 1931.
[17] Q. Le, T. Sarlos, and A. Smola. Fastfood - computing Hilbert space expansions in loglinear time. In

ICML, volume 28, pages 244–252, 2013.

[18] M. Lichman. UCI machine learning repository, 2013.
[19] J.R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. Technical report,

[20] Tom´aˇs Pevn`y and Jessica Fridrich. Benchmarking for steganography. In Information Hiding, pages 251–

2014.

267. Springer, 2008.

[21] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
[22] A. Ramdas, S. Reddi, B. P´oczos, A. Singh, and L. Wasserman. On the decreasing power of kernel- and

distance-based nonparametric hypothesis tests in high dimensions. AAAI, 2015.

[23] S. Reddi, A. Ramdas, B. P´oczos, A. Singh, and L. Wasserman. On the high-dimensional power of linear-

time kernel two-sample testing under mean-difference alternatives. AISTATS, 2015.

[24] Walter Rudin. Real and complex analysis. Tata McGraw-Hill Education, 1987.
[25] D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-based and

RKHS-based statistics in hypothesis testing. Annals of Statistics, 41(5):2263–2291, 2013.

[26] B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, characteristic kernels and RKHS em-

bedding of measures. JMLR, 12:2389–2410, 2011.

[27] B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Sch¨olkopf. Hilbert space embeddings

and metrics on probability measures. JMLR, 11:1517–1561, 2010.

[28] I. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media, 2008.
[29] I. Steinwart, D. Hush, and C. Scovel. An explicit description of the reproducing kernel hilbert spaces of

gaussian rbf kernels. Information Theory, IEEE Transactions on, 52(10):4635–4643, 2006.

[30] Hong-Wei Sun and Ding-Xuan Zhou. Reproducing kernel hilbert spaces associated with analytic
translation-invariant mercer kernels. Journal of Fourier Analysis and Applications, 14(1):89–101, 2008.

[31] GJ Sz´ekely. E-statistics: The energy of statistical samples. Technical report, 2003.
[32] W. Zaremba, A. Gretton, and M. Blaschko. B-test: A non-parametric, low variance kernel two-sample

test. In NIPS, 2013.

[33] Ji Zhao and Deyu Meng. FastMMD: Ensemble of circular discrepancy for efﬁcient two-sample test.

Neural computation, (27):1345–1372, 2015.

[34] AA Zinger, AV Kakosyan, and LB Klebanov. A characterization of distributions by mean values of

statistics and certain probabilistic metrics. Journal of Mathematical Sciences, 59(4):914–920, 1992.

9

