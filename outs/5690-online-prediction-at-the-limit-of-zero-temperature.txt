Online Prediction at the Limit of Zero Temperature

Mark Herbster

Stephen Pasteris

Department of Computer Science

University College London

London WC1E 6BT, England, UK

{m.herbster,s.pasteris}@cs.ucl.ac.uk

Shaona Ghosh

ECS

University of Southampton
Southampton, UK SO17 1BJ

ghosh.shaona@gmail.com

Abstract

We design an online algorithm to classify the vertices of a graph. Underpinning
the algorithm is the probability distribution of an Ising model isomorphic to the
graph. Each classiﬁcation is based on predicting the label with maximum marginal
probability in the limit of zero-temperature with respect to the labels and vertices
seen so far. Computing these classiﬁcations is unfortunately based on a #P -
complete problem. This motivates us to develop an algorithm for which we give
a sequential guarantee in the online mistake bound framework. Our algorithm is
optimal when the graph is a tree matching the prior results in [1]. For a general
graph, the algorithm exploits the additional connectivity over a tree to provide a
per-cluster bound. The algorithm is efﬁcient, as the cumulative time to sequen-
tially predict all of the vertices of the graph is quadratic in the size of the graph.

Introduction

1
Semi-supervised learning is now a standard methodology in machine learning. A common approach
in semi-supervised learning is to build a graph [2] from a given set of labeled and unlabeled data
with each datum represented as a vertex. The hope is that the constructed graph will capture either
the cluster [3] or manifold [4] structure of the data. Typically, an edge in this graph indicates the
expectation that the joined data points are more likely to have the same label. One method to
exploit this representation is to use the semi-norm induced by the Laplacian of the graph [5, 4, 6, 7].
A shared idea of the Laplacian semi-norm based approaches is that the smoothness of a boolean
labeling of the graph is measured via the “cut”, which is just the number of edges that connect
disagreeing labels. In practice the semi-norm is then used as a regularizer in which the optimization
problem is relaxed from boolean to real values. Our approach also uses the “cut”, but unrelaxed, to
deﬁne an Ising distribution over the vertices of the graph.
Predicting with the vertex marginals of an Ising distribution in the limit of zero temperature was
shown to be optimal in the mistake bound model [1, Section 4.1] when the graph is a tree. The exact
computation of marginal probabilities in the Ising model is intractable on non-trees [8]. However, in
the limit of zero temperature, a rich combinatorial structure called the Picard-Queyranne graph [9]
emerges. We exploit this structure to give an algorithm which 1) is optimal on trees, 2) has a
quadratic cumulative computational complexity, and 3) has a mistake bound on generic graphs that
is stronger than previous bounds in many natural cases.
The paper is organized as follows. In the remainder of this section, we introduce the Ising model
and lightly review previous work in the online mistake bound model for predicting the labeling of a
graph. In Section 2 we review our key technical tool the Picard-Queyranne graph [9] and explain the
required notation. In the body of Section 3 we provide a mistake bound analysis of our algorithm
as well as the intractable 0-Ising algorithm and then conclude with a detailed comparison to the
state of the art. In the appendices we provide proofs as well as preliminary experimental results.
Ising model in the limit zero temperature.
In our setting, the parameters of the Ising model
are an n-vertex graph G = (V (G), E(G)) and a temperature parameter τ > 0, where V (G) =

1

φG(u) := (cid:80)

τ (u) ∝ exp(cid:0)− 1

τ φG(u)(cid:1) where τ > 0 is the temperature parameter. In our online

{1, . . . , n} denotes the vertex set and E(G) denotes the edge set. Each vertex of this graph may
be labeled with one of two states {0, 1} and thus a labeling of a graph may be denoted by a vector
u ∈ {0, 1}n where ui denotes the label of vertex i. The cutsize of a labeling u is deﬁned as
(i,j)∈E(G) |ui − uj|. The Ising probability distribution over labelings of G is then
deﬁned as pG
setting at the beginning of trial t + 1 we will have already received an example sequence, St, of t
vertex-label pairs (i1, y1), . . . , (it, yt) where pair (i, y) ∈ V (G)×{0, 1}. We use pG
τ (uv = y|St) :=
τ (uv = y|ui1 = y1, . . . , uit = yt) to denote the marginal probability that vertex v has label y
pG
given the previously labeled vertices of St. For convenience we also deﬁne the marginalized cutsize
φG(u|St) to be equal to φG(u) if ui1 = y1, . . . , uit = yt and equal to undefined otherwise. Our
prediction ˆyt+1 of vertex it+1 is then the label with maximal marginal probability in the limit of
zero temperature, thus

t+1(it+1|St) := argmax
ˆy0I
y∈{0,1}

lim
τ→0

τ (uit+1 = y|ui1 = y1, . . . , uit = yt) .
pG

[0-Ising]

(1)

Note the prediction is undeﬁned if the labels are equally probable. In low temperatures the mass
of the marginal is dominated by the labelings consistent with St and the proposed label of vertex
it+1 of minimal cut; as we approach zero, ˆyt+1 is the label consistent with the maximum number of
labelings of minimal cut. Thus if k := min

φG(u|S) then we have that

u∈{0,1}n

®

0
1

ˆy0I(v|S) =

|u ∈ {0, 1}n : φG(u|(S, (v, 0))) = k| > |u ∈ {0, 1}n : φG(u|(S, (v, 1))) = k|
|u ∈ {0, 1}n : φG(u|(S, (v, 0))) = k| < |u ∈ {0, 1}n : φG(u|(S, (v, 1))) = k| .
The problem of counting minimum label-consistent cuts was shown to be #P-complete in [10] and
further computing ˆy0I(v|S) is also NP-hard (see Appendix G). In Section 2.1 we introduce the
Picard-Queyranne graph [9] which captures the combinatorial structure of the set of minimum-cuts.
We then use this simplifying structure as a basis to design a heuristic approximation to ˆy0I(v|S) with
a mistake bound guarantee.
Predicting the labelling of a graph in the mistake bound model. We prove performance guaran-
tees for our method in the mistake bound model introduced by Littlestone [11]. On the graph this
model corresponds to the following game. Nature presents a graph G; Nature queries a vertex
i1 ∈ V (G) = INn; the learner predicts the label of the vertex ˆy1 ∈ {0, 1}; nature presents a
label y1; nature queries a vertex i2; the learner predicts ˆy2; and so forth. The learner’s goal is
to minimize the total number of mistakes M = |{t : ˆyt (cid:54)= yt}|. If nature is adversarial, the learner
will always make a “mistake”, but if nature is regular or simple, there is hope that a learner may
incur only a few mistakes. Thus, a central goal of online learning is to design algorithms whose
total mistakes can be bounded relative to the complexity of nature’s labeling. The graph labeling
problem has been studied extensively in the online literature. Here we provide a rough discussion of
the two main approaches for graph label prediction, and in Section 3.3 we provide a more detailed
comparison. The ﬁrst approach is based on the graph Laplacian [12, 13, 14]; it provides bounds that
utilize the additional connectivity of non-tree graphs, which are particularly strong when the graph
contains uniformly-labeled clusters of small (resistance) diameter. The drawbacks of this approach
are that the bounds are weaker on graphs with large diameter and that the computation times are
slower. The second approach is to estimate the original graph with an appropriately selected tree or
“path” graph [15, 16, 1, 17]; this leads to faster computation times, and bounds that are better on
graphs with large diameters. The algorithm treeOpt [1] is optimal on trees. These algorithms may
be extended to non-tree graphs by ﬁrst selecting a spanning tree uniformly at random [16] and then
applying the algorithm to the sampled tree. This randomized approach enables expected mistake
bounds which exploit the cluster structure in the graph.
The bounds we prove for the NP-hard 0-Ising prediction and our heuristic are most similar to the
“small p” bounds proven for the p-seminorm interpolation algorithm [14]. Although these bounds
are not strictly comparable, a key strength of our approach is that the new bounds often improve
when the graph contains uniformly-labeled clusters of varying diameters. Furthermore, when the
graph is a tree we match the optimal bounds of [1]. Finally, the cumulative time required to compute
the complete labeling of a graph is quadratic in the size of the graph for our algorithm, while [14] re-
quires the minimization of a non-strongly convex function (on every trial) which is not differentiable
when p → 1.

2

i=1 so that V (G) := {Vi}N

2 Preliminaries
An (undirected) graph G is a pair of sets (V, E) such that E is a set of unordered pairs of distinct
elements from V . We say that R is a subgraph R ⊆ G iff V (R) ⊆ V (G) and E(R) = {(i, j) :
i, j ∈ V (R), (i, j) ∈ E(G)}. Given any subgraph R ⊆ G, we deﬁne its boundary (or inner border)
∂0(R), its neighbourhood (or exterior border) ∂e(R) respectively as ∂0(R) := {j : i (cid:54)∈ V (R), j ∈
V (R), (i, j) ∈ E(G)}, and ∂e(R) := {i : i (cid:54)∈ V (R), j ∈ V (R), (i, j) ∈ E(G)}, and its exterior
e (R) := {(i, j) : i (cid:54)∈ V (R), j ∈ V (R), (i, j) ∈ E(G)}. The length of a subgraph
edge border ∂E
P is denoted by |P| := |E(P)| and we denote the diameter of a graph by D(G). A pair of vertices
v, w ∈ V (G) are κ-connected if there exist κ edge-disjoint paths connecting them. The connectivity
of a graph, κ(G), is the maximal value of κ such that every pair of points in G is κ-connected. The
atomic number Nκ(G) of a graph at connectivity level κ is the minimum cardinality c of a partition
of G into subgraphs {R1, . . . ,Rc} such that κ(Ri) ≥ κ for all 1 ≤ i ≤ c.
Our results also require the use of directed-, multi-, and quotient- graphs. Every undirected graph
also deﬁnes a directed graph where each undirected edge (i, j) is represented by directed edges (i, j)
and (j, i). An orientation of an undirected graph is an assignment of a direction to each edge, turning
the initial graph into a directed graph. In a multi-graph the edge set is now a multi-set and thus there
may be multiple edges between two vertices. A quotient-graph G is deﬁned from a graph G and
a partition of its vertex set {Vi}N
i=1 (we often call these vertices super-
vertices to emphasize that they are sets) and the multiset E(G) := {(I, J) : I, J ∈ V (G), I (cid:54)= J, i ∈
I, j ∈ J, (i, j) ∈ E(G)}. We commonly construct a quotient-graph G by “merging” a collection of
super-vertices, for example, in Figure 2 from 2a to 2b where 6 and 9 are merged to “6/9” and also
the ﬁve merges that transforms 2c to 2d.
The set of all label-consistent minimum-cuts in a graph with respect to an example sequence S is
U∗
G(S) := argminu∈{0,1}n φG(u|S). The minimum is typically non-unique. For example in Fig-
ure 2a, the vertex sets {v1, . . . , v4},{v5, . . . , v12} correspond to one label-consistent minimum-cut
and {v1, . . . , v5, v7, v8},{v6, v9 . . . , v12} to another (the cutsize is 3). The (uncapacitated) maxi-
mum ﬂow is the number of edge-disjoint paths between a source and target vertex. Thus in Figure 2b
between vertex “1” and vertex “6/9” there are at most 3 simultaneously edge-disjoint paths; these are
also not unique, as one path must pass through either vertices (cid:104)v11, v12(cid:105) or vertices (cid:104)v11, v10, v12(cid:105).
Figure 2c illustrates one such ﬂow F (just the directed edges). For convenience it is natural to view
the maximum ﬂow or the label-consistent minimum-cut as being with respect to only two vertices
as in Figure 2a transformed to Figure 2b so that H ← merge(G,{v6, v9}). The “ﬂow” and the
“cut” are related by Menger’s theorem which states that the minimum-cut with respect to a source
and target vertex is equal to the max ﬂow between them. Given a connected graph H and source
and target vertices s, t the Ford-Fulkerson algorithm [18] can ﬁnd k edge-disjoint paths from s to t
in time O(k|E(H)|) where k is the value of the max ﬂow.
2.1 The Picard-Queyranne graph
Given a set of labels there may be multiple label-consistent minimum-cuts as well as multiple max-
imum ﬂows in a graph. The Picard-Queyranne (PQ) graph [9] reduces this multiplicity as far as is
possible with respect to the indeterminacy of the maximum ﬂow. The vertices of the PQ-graph are
deﬁned as a super-vertex set on a partition of the original graph’s vertex set. Two vertices are con-
tained in the same super-vertex iff they have the same label in every label-consistent minimum-cut.
An edge between two vertices deﬁnes an analogous edge between two super-vertices iff that edge is
conserved in every maximum ﬂow. Furthermore the edges between super-vertices strictly orient the
labels in any label-consistent minimum-cut as may be seen in the formal deﬁnition that follows.
First we introduce the following useful notations: let kG,S := min{φG(u|S) : u ∈ {0, 1}n} denote
the minimum-cutsize of G with respect to S; let i
S∼j denote an equivalence relation between vertices
in V (G) where i
Deﬁnition 1 ([9]). The Picard-Queyranne graph G(G,S) is derived from graph G and non-trivial
example sequence S. The graph is an orientation of the quotient graph derived from the partition
{⊥, I2, . . . , IN−1,(cid:62)} of V (G) induced by S∼. The edge set of G is constructed of kG,S edge-disjoint
paths starting at source vertex ⊥ and terminating at target vertex (cid:62). A labeling u ∈ {0, 1}n is in
U∗
G(S) iff

G(S) : ui = uj; and then we deﬁne,

S∼j iff ∀u ∈ U∗

1. i ∈ ⊥ implies ui = 0 and i ∈ (cid:62) implies ui = 1

3

2. i, j ∈ H implies ui = uj
3. i ∈ I, j ∈ J, (I, J) ∈ E(G), and ui = 1 implies uj = 1

where ⊥ and (cid:62) are the source and target vertices and H, I, J ∈ V (G).
As G(G,S) is a DAG it naturally deﬁnes a partial order (V (G),≤G) on the vertex set where I ≤G J
if there exists a path starting at I and ending at J. The least and greatest elements of the partial order
are ⊥ and (cid:62). The notation ↑R and ↓R denote the up set and down set of R. Given the set U∗ of all
label-consistent minimum-cuts then if u ∈ U∗ there exists an antichain A ⊆ V (G) \ {(cid:62)} such that
ui = 0 when i ∈ I ∈ ↓A otherwise ui = 1; furthermore for every antichain there exists a label-
consistent minimum-cut. The simple structure of G(G,S) was utilized by [9] to enable the efﬁcient
algorithmic enumeration of minimum-cuts. However, the cardinality of this set of all label-consistent
minimum-cuts is potentially exponential in the size of the PQ-graph and the exact computation of
the cardinality was later shown #P-complete in [10]. In Figure 1 we give the algorithm from [9, 19]
PicardQueyranneGraph(graph: G; example sequence: S = (vk, yk)t
1. (H, s, t) ← SourceTargetMerge(G,S)
2. F ← MaxFlow(H, s, t)
3. I ← (V (I), E(I)) where V (I) := V (H) and E(I) := {(i, j) : (i, j) ∈ E(H), (j, i) (cid:54)∈ F}
4. G0 ← QuotientGraph(StronglyConnectedComponents(I),H)
5. E(G) ← E(G0); V (G) ← V (G0) except ⊥(G) ← ⊥(G0) ∪ {vk : k ∈ INt, yk = 0}
and (cid:62)(G) ← (cid:62)(G0) ∪ {vk : k ∈ INt, yk = 1}

k=1)

Return: directed graph: G

Figure 1: Computing the Picard-Queyranne graph

(a) Graph G and S = (cid:104)(v1, 0), (v6, 1), (v9, 1)(cid:105)

(b) Graph H (step 1 in Figure 1)

(c) Graph I (step 3 in Figure 1)

(d) PQ Graph G (step 4 in Figure 1)

Figure 2: Building a Picard-Queyranne graph

to compute a PQ-graph. We illustrate the computation in Figure 2. The algorithm operates ﬁrst on
(G,S) (step 1) by “merging” all vertices which share the same label in S to create H. In step 2 a
max ﬂow graph F ⊆ H is computed by the Ford-fulkerson algorithm. It is well-known in the case
of unweighted graphs that a max ﬂow graph F may be output as a DAG of k edge-disjoint paths
where k is the value of the ﬂow. In step 3 all edges in the ﬂow become directed edges creating I.
The graph G0 is then created in step 4 from I where the strongly connected components become the
super-vertices of G0 and the super-edges correspond to a subset of ﬂow edges from F. Finally, in

4

13241178510612913241178510126/913241178510126/9⊥ABC⊥step 5, we create the PQ-graph G by “ﬁxing” the source and target vertices so that they also have as
elements the original labeled vertices from S which were merged in step 1. The correctness of the
algorithm follows from arguments in [9]; we provide an independent proof in Appendix B.
Theorem 2 ([9]). The algorithm in Figure 1 computes the unique Picard-Queyranne graph G(G,S)
derived from graph G and non-trivial example sequence S.

3 Mistake Bounds Analysis

In this section we analyze the mistakes incurred by the intractable 0-Ising strategy (see (1)) and
the strategy longest-path (see Figure 3). Our analysis splits into two parts. Firstly, we show
(Section 3.1, Theorem 4) for a sufﬁciently regular graph label prediction algorithm, that we may
analyze independently the mistake bound of each uniformly-labeled cluster (connected subgraph).
Secondly, the per-cluster analysis then separates into three cases, the result of which is summarized
in Theorem 10. For a given cluster C when its internal connectivity is larger than the number of
edges in the boundary (κ(C) > |∂E
e (C)|) we will incur no more than one mistake in that cluster. On
the other hand for smaller connectivity clusters (κ(C) ≤ |∂E
e (C)|) we incur up to quadratically in
mistakes via the edge boundary size. When C is a tree we incur O(|∂E
The analysis of smaller connectivity clusters separates into two parts. First, a sequence of trials in
which the label-consistent minimum-cut does not increase, we call a PQ-game (Section 3.2) as in
essence it is played on a PQ-graph. We give a mistake bound for a PQ-game for the intractable
0-Ising prediction and a comparable bound for the strategy longest-path in Theorem 8.
Second, when the label-consistent minimum-cut increases the current PQ-game ends and a new one
begins, leading to a sequence of PQ-games. The mistakes incurred over a sequence of PQ-games is
addressed in the aforementioned Theorem 10 and ﬁnally Section 3.3 concludes with a discussion of
the combined bounds of Theorems 4 and 10 with respect to other graph label prediction algorithms.

e (C)| log D(C)) mistakes.

3.1 Per-cluster mistake bounds for regular graph label prediction algorithms
An algorithm is called regular if it is permutation-invariant, label-monotone, and Markov. An
algorithm is permutation-invariant if the prediction at any time t does not depend on the order of
the examples up to time t; label-monotone if for every example sequence if we insert an example
“between” examples t and t+1 with label y then the prediction at time t+1 is unchanged or changed
to y; and Markov with respect to a graph G if for any disjoint vertex sets P and Q and separating set
R then the predictions in P are independent of the labels in Q given the labels of R. A subgraph is
uniformly-labeled with respect to an example sequence iff the label of each vertex is the same and
these labels are consistent with the example sequence. The following deﬁnition characterizes the
“worst-case” example sequences for regular algorithms with respect to uniformly-labeled clusters.
Deﬁnition 3. Given an online algorithm A and a uniformly-labeled subgraph C⊆G, then BA(C;G)
denotes the maximal mistakes made only in C for the presentation of any permutation of examples
in ∂e(C), each with label y, followed by any permutation of examples in C, each with label 1−y.
The following theorem enables us to analyze the mistakes incurred in each uniformly-labeled sub-
graph C independently of each other and independently of the remaining graph structure excepting
the subgraph’s exterior border ∂e(C).
Theorem 4 (Proof in Appendix D). Given an online permutation-invariant label-monotone Markov
algorithm A and a graph G which is covered by uniformly-labeled subgraphs C1, . . . ,Cc the mistakes

incurred by the algorithm may be bounded by M ≤(cid:80)c

i=1 BA(Ci;G) .

The above theorem paired with Theorem 10 completes the mistake bound analysis of our algorithms.

3.2 PQ-games
Given a PQ-graph G = G(G,S), the derived online PQ-game is played between a player and an
adversary. The aim of the player is to minimize their mistaken predictions; for the adversary
it is to maximize the player’s mistaken predictions. Thus to play the adversary proposes a vertex
z ∈ Z ∈ V (G), the player then predicts a label ˆy ∈ {0, 1}, then the adversary returns a label
y ∈ {0, 1} and either a mistake is incurred or not. The only restriction on the adversary is to not
return a label which increases the label-consistent minimum-cut. As long as the adversary does not
give an example (z ∈ ⊥, 1) or (z ∈ (cid:62), 0), the label-consistent minimum-cut does not increase

5

no matter the value of y; which also implies the player has a trivial strategy to predict the label of
z ∈ ⊥ ∪ (cid:62). After the example is given, we have an updated PQ-graph with new source and target
super-vertices as seen in the proposition below.
Proposition 5. If G(G,S) is a PQ-graph and (z, y = 0) ((z, y = 1)) is an example with
z ∈ Z ∈ V (G) and z (cid:54)∈ (cid:62) (z (cid:54)∈ ⊥) then let Z = ↓{Z} (Z = ↑{Z}) then G(G,(cid:104)S, (z, y)(cid:105)) =
merge(G(G,S), Z).
Thus given the PQ-graph G the PQ-game is independent of G and S, since a “play” z ∈ V (G)
induces a “play” Z ∈ V (G) (with z ∈ Z).
Mistake bounds for PQ-games. Given a single PQ-game, in the following we will discuss the
three strategies fixed-paths, 0-Ising, and longest-path that the player may adopt
for which we prove online mistake bounds. The ﬁrst strategy fixed-paths is merely motiva-
tional: it can be used to play a single PQ-game, but not a sequence. The second strategy 0-Ising
is computationally infeasible. Finally, the longest-path strategy is “dynamically” similar to
fixed-paths but is also permutation-invariant. Common to all our analyses is a k-path cover
P of PQ-graph G which is a partitioning of the edge-set of G into k edge-disjoint directed paths
P := {p1, . . . , pk} from ⊥ to (cid:62). Note that the cover is not necessarily unique; for example, in
Figure 2d, we have the two unique path covers P1 := {(⊥, A,(cid:62)), (⊥, A, B,(cid:62)), (⊥, B, C,(cid:62))} and
P2 := {(⊥, A,(cid:62)), (⊥, A, B, C,(cid:62)), (⊥, B,(cid:62))}. We denote the set of all path covers as P and thus
we have for Figure 2d that P := {P1, P2}. This cover motivates a simple mistake bound and strat-
egy. Suppose we had a single path of length |p| where the ﬁrst and last vertex are the “source”
and “target” vertices. So the minimum label-consistent cut-size is “1” and a natural strategy is sim-
ply to predict with the “nearest-neighbor” revealed label and trivially our mistake bound is log |p|.
Generalizing to multiple paths we have the following strategy.

Strategy fixed-paths(‹P ): Given a PQ-graph choose a path cover {˜p1, . . . , ˜pk} =‹P ∈ P(G).
‹P is not vertex-disjoint and we need to predict a vertex V we may select a path in‹P containing V
“nearest-neighbor” strategy detailed above, achieving the mistake upper bound M ≤(cid:80)k

If the path cover is also vertex-disjoint except for the source and target vertex we may directly use the
i=1 log |˜pi|.
Unsurprisingly, in the vertex-disjoint case it is a mistake-bound optimal [11] algorithm. If, however,

and predict with the nearest neighbour and also obtain the bound above. In this case, however, the
bound may not be “optimal.” Essentially the same technique was used in [20] in a related setting
for learning “directed cuts.” A limitation of the fixed-paths strategy is that it does not seem
possible to extend into a strategy that can play a sequence of PQ-games and still meet the regularity
properties, particularly permutation-invariance as required by Theorem 4.
Strategy 0-Ising: The prediction of the Ising model in the limit of zero temperature (cf. (1)),
is equivalent to those of the well-known Halving algorithm [21, 22] where the hypothesis class U∗
is the set of label-consistent minimum-cuts. The mistake upper bound of the Halving algorithm
is just M ≤ log |U∗| where this bound follows from the observation that whenever a mistake is
(cid:81)k
made at least “half” of concepts in U∗ are no longer consistent. We observe that we may upper
bound |U∗| ≤ argminP∈P(G)
i=1 |pi| since the product of path lengths from any path cover P
is an upper bound on the cardinality of U∗ and hence we have the bound in (2). And in fact this
bound may be a signiﬁcant improvement over the fixed-paths strategy’s bound as seen in the
following proposition.
Proposition 6 (Proof in Appendix C). For every c ≥ 2 there exists a PQ-graph Gc, with a path cover
P (cid:48) ∈ P(Gc) and a PQ-game example sequence such that the mistakes Mfixed-paths(P (cid:48)) = Ω(c2),
while for all PQ-game example sequences on Gc the mistakes M0-Ising = O(c).
Unfortunately the 0-Ising strategy has the drawback that counting label-consistent minimum-cuts
is #P-complete and computing the prediction (see (1)) is NP-hard (see Appendix G).
Strategy longest-path: In our search for an efﬁcient and regular prediction strategy it seems
natural to attempt to “dynamize” the fixed-paths approach and predict with a nearest neigh-
bor along a dynamic path. Two such permutation-invariant methods are the longest-path and
shortest-path strategies. The strategy shortest-path predicts the label of a super-vertex
Z in a PQ-game G as 0 iff the shortest directed path (⊥, . . . , Z) is shorter than the shortest directed
path (Z, . . . ,(cid:62)). The strategy longest-path predicts the label of a super-vertex Z in a PQ-game
G as 0 iff the longest directed path (⊥, . . . , Z) is shorter than the longest directed path (Z, . . . ,(cid:62)).
The strategy shortest-path seems to be intuitively favored over longest-path as it is just

6

Input: Graph: G, Example sequence: S = (cid:104)(i1, 0), (i2, 1), (i3, y3), . . . , (i(cid:96), y(cid:96))(cid:105) ∈ (INn × {0, 1})(cid:96)
Initialization: G3 = PicardQueyranneGraph(G,S2)
for t = 3, . . . , (cid:96) do

ß0 |longest-path(Gt,⊥t, It)|≤|longest-path(Gt, It,(cid:62)t)|

Receive: it ∈ {1, . . . , n}
It = V ∈ V (Gt) with it ∈ V
Predict (longest-path): ˆyt =

ßmerge(Gt,↓{It}) yt = 0

Predict (0-Ising):
Receive: yt
if (it (cid:54)∈ ⊥t or yt (cid:54)= 1) and (it (cid:54)∈ (cid:62)t or yt (cid:54)= 0) then

ˆyt = ˆyI0(it|St−1)

1 otherwise

merge(Gt,↑{It}) yt = 1

Gt+1 =

else

Gt+1 = PicardQueyranneGraph(G,St)

end

% as per equation (1)

% cut unchanged

% cut increases

Figure 3: Longest-path and 0-Ising online prediction

the “nearest-neighbor” prediction with respect to the geodesic distance. However, the following
proposition shows that it is strictly worse than any fixed-paths strategy in the worst case.
Proposition 7 (Proof in Appendix C). For every c ≥ 4 there exists a PQ-graph Gc and a PQ-game
example sequence such that the mistakes Mshortest-path = Ω(c2 log(c)), while for every path cover
P ∈P(Gc) and for all PQ-game example sequences on Gc the mistakes Mfixed-paths(P ) = O(c2).
In contrast, for the strategy longest-paths in the proof of Theorem 8 we show that there al-
lp|.

ways exists some retrospective path cover Plp ∈ P(G) such that Mlongest-paths ≤ (cid:80)k
fixed-paths(‹P ), 0-Ising, and longest-path on PQ-graph G and k-path cover‹P ∈
fixed-paths(‹P )

Computing the “longest-path” has time complexity linear in the number of edges in a DAG.
Summarizing the mistake bounds for the three PQ-game strategies for a single PQ-game we have
the following theorem.
Theorem 8 (Proof in Appendix C). The mistakes, M, of an online PQ-game for player strategies
P(G) is bounded by

i=1 log |pi



(cid:80)k
i=1 log |˜pi|
argminP∈P(G)
argmaxP∈P(G)

(cid:80)k
(cid:80)k
i=1 log |pi|
i=1 log |pi|

M ≤

0-Ising
longest-path

.

(2)

3.3 Global analysis of prediction at zero temperature
In Figure 3 we summarize the prediction protocol for 0-Ising and longest-path. We claim
the regularity properties of our strategies in the following theorem.
Theorem 9 (Proof in Appendix E). The strategies 0-Ising and longest-path are
permutation-invariant, label-monotone, and Markov.

The technical hurdle here is to prove that label-monotonicity holds over a sequence of PQ-games.
For this we need an analog of Proposition 5 to describe how the PQ-graph changes when the label-
consistent minimum-cut increases (see Proposition 19). The application of the following theorem
along with Theorem 4 implies we may bound the mistakes of each uniformly-labeled cluster in
potentially three ways.
Theorem 10 (Proof in Appendix D). Given either the 0-Ising or longest-path strategy A
the mistakes on uniformly-labeled subgraph C ⊆ G are bounded by

e (C)| − κ(C)) log N (C)(cid:1) κ(C) ≤ |∂E

κ(C) > |∂E

e (C)|
e (C)|

C is a tree

e (C)|(1 + |∂E
e (C)| log D(C))

(3)

O(1)
O(cid:0)|∂E

O(|∂E

BA(C;G) ∈

with the atomic number N (C) := N|∂E

e (C)|+1(C) ≤ |V (C)|.

7

First, if the internal connectivity of the cluster is high we will only make a single mistake in that clus-
ter. Second, if the cluster is a tree then we pay the external connectivity of the cluster |∂E
e (C)| times
the log of the cluster diameter. Finally, in the remaining case we pay quadratically in the external
connectivity and logarithmically in the “atomic number” of the cluster. The atomic number captures
the fact that even a poorly connected cluster may have sub-regions of high internal connectivity.
If G is a graph and S an example sequence with a label-consistent
Computational complexity.
minimum-cut of φ then we may implement the longest-path strategy so that it has a cumulative
computational complexity of O(max(φ, n)|E(G)|). This follows because if on a trial the “cut” does
not increase we may implement prediction and update in O(|E(G)|) time. On the other hand if the
“cut” increases by φ(cid:48) we pay O(φ(cid:48)|E(G)|) time. To do so we implement an online “Ford-Fulkerson”
algorithm [18] which starts from the previous “residual” graph to which it then adds the additional
φ(cid:48) ﬂow paths with φ(cid:48) steps of size O(|E(G)|).
Discussion. There are essentially ﬁve dominating mistake bounds for the online graph labeling prob-
lem: (I) the bound of treeOpt [1] on trees, (II) the bound in expectation of treeOpt on a random
spanning tree sampled from a graph [1], (III) the bound of p-seminorm interpolation [14]
tuned for “sparsity” (p < 2), (IV) the bound of p-seminorm interpolation as tuned to be
equivalent to online label propagation [5] (p = 2), (V) this paper’s longest-path strategy.
The algorithm treeOpt was shown to be optimal on trees.
In Appendix F we show that
longest-path also obtains the same optimal bound on trees. Algorithm (II) applies to generic
graphs and is obtained from (I) by sampling a random spanning tree (RST). It is not directly com-
parable to the other algorithms as its bound holds only in expectation with respect to the RST.
We use [14, Corollary 10] to compare (V) to (III) and (IV). We introduce the following simpli-
fying notation to compare bounds. Let C1, . . . ,Cc denote uniformly-labeled clusters (connected
e (Cr)|. We deﬁne Dr(i) to
subgraphs) which cover the graph and set κr := κ(Cr) and φr := |∂E
be the wide diameter at connectivity level i of cluster Cr. The wide diameter Dr(i) is the minimum
value such that for all pairs of vertices v, w ∈ Cr there exists i edge-disjoints of paths from v to w of
length at least Dr(i) in Cr (and if i > κr then Dr(i) := +∞). Thus Dr(1) is the diameter of cluster
Cr and Dr(1) ≤ Dr(2) ≤ ··· . Let φ denote the minimum label-consistent cutsize and observe that

r=1 φr.

if the cardinality of the cover |{C1, . . . ,Cc}| is minimized then we have that 2φ =(cid:80)c
[(cid:80)c

Thus using [14, Corollary 10] we have the following upper bounds of (III): (φ/κ∗)2 log D∗ + c and
(IV): (φ/κ∗)D∗ + c where κ∗ := minr κr and D∗ := maxr Dr(κ∗). In comparison we have (V):
r=1 max(0, φr − κr + 1)φr log Nr] + c with atomic numbers Nr := Nφr+1(Cr). To contrast
the bounds, consider a double lollipop labeled-graph: ﬁrst create a lollipop which is a path of n/4
vertices attached to a clique of n/4 vertices. Label these vertices 1. Second, clone the lollipop
except with labels 0. Finally join the two cliques with n/8 edges arbitrarily. For (III) and (IV)
the bounds are O(n) independent of the choice of clusters. Whereas an upper bound for (V) is the
exponentially smaller O(log n) which is obtained by choosing a four cluster cover consisting of the
two paths and the two cliques. This emphasizes the generic problem of (III) and (IV): parameters
κ∗ and D∗ are deﬁned by the worst clusters; whereas (V) is truly a per-cluster bound. We consider
the previous “constructed” example to be representative of a generic case where the graph contains
clusters of many resistance diameters as well as sparse interconnecting “background” vertices.
On the other hand, there are cases in which (III,IV) improve on (V). For a graph with only small
diameter clusters and if the cutsize exceeds the cluster connectivity then (IV) improves on (III,V)
given the linear versus quadratic dependence on the cutsize. The log-diameter may be arbitrarily
smaller than log-atomic-number ((III) improves on (V)) and also vice-versa. Other subtleties not
accounted for in the above comparison include the fact a) the wide diameter is a crude upper bound
for resistance diameter (cf. [14, Theorem 1]) and b) the clusters of (III,IV) are not required to be
uniformly-labeled. Regarding “a)” replacing “wide” with “resistance” does not change the fact
the bound now holds with respect to the worst resistance diameter and the example above is still
problematic. Regarding “b)” it is a nice property but we do not know how to exploit this to give
an example that signiﬁcantly improves (III) or (IV) over a slightly more detailed analysis of (V).
Finally (III,IV) depend on a correct choice of tunable parameter p.
Thus in summary (V) matches the optimal bound of (I) on trees, and can often improve on (III,IV)
when a graph is naturally covered by label-consistent clusters of different diameters. However
(III,IV) may improve on (V) in a number of cases including when the log-diameter is signiﬁcantly
smaller than log-atomic-number of the clusters.

8

References
[1] Nicol`o Cesa-Bianchi, Claudio Gentile, and Fabio Vitale. Fast and optimal prediction on a labeled tree. In

Proceedings of the 22nd Annual Conference on Learning. Omnipress, 2009.

[2] Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts.

In
Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 19–26,
San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.

[3] Olivier Chapelle, Jason Weston, and Bernhard Sch¨olkopf. Cluster kernels for semi-supervised learning.
In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems
15, pages 601–608. MIT Press, 2003.

[4] Mikhail Belkin and Partha Niyogi. Semi-supervised learning on riemannian manifolds. Mach. Learn.,

56(1-3):209–239, 2004.

[5] Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian ﬁelds

and harmonic functions. In ICML, pages 912–919, 2003.

[6] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch¨olkopf. Learning

with local and global consistency. In NIPS, 2003.

[7] Martin Szummer and Tommi Jaakkola. Partially labeled classiﬁcation with markov random walks. In

NIPS, pages 945–952, 2001.

[8] Leslie Ann Goldberg and Mark Jerrum. The complexity of ferromagnetic ising with local ﬁelds. Combi-

natorics, Probability & Computing, 16(1):43–61, 2007.

[9] Jean-Claude Picard and Maurice Queyranne. On the structure of all minimum cuts in a network and
applications. In V.J. Rayward-Smith, editor, Combinatorial Optimization II, volume 13 of Mathematical
Programming Studies, pages 8–16. Springer Berlin Heidelberg, 1980.

[10] J. Scott Provan and Michael O. Ball. The complexity of counting cuts and of computing the probability

that a graph is connected. SIAM Journal on Computing, 12(4):777–788, 1983.

[11] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.

Machine Learning, 2:285–318, April 1988.

[12] Mark Herbster, Massimiliano Pontil, and Lisa Wainer. Online learning over graphs. In ICML ’05: Pro-
ceedings of the 22nd international conference on Machine learning, pages 305–312, New York, NY, USA,
2005. ACM.

[13] Mark Herbster. Exploiting cluster-structure to predict the labeling of a graph. In Proceedings of the 19th

International Conference on Algorithmic Learning Theory, pages 54–69, 2008.

[14] Mark Herbster and Guy Lever. Predicting the labelling of a graph via minimum p-seminorm interpolation.

In Proceedings of the 22nd Annual Conference on Learning Theory (COLT’09), 2009.

[15] Mark Herbster, Guy Lever, and Massimiliano Pontil. Online prediction on large diameter graphs.

Advances in Neural Information Processing Systems (NIPS 22), pages 649–656. MIT Press, 2009.

In

[16] Nicol`o Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and Giovanni Zappella. Random spanning trees
and the prediction of weighted graphs. In Proceedings of the 27th International Conference on Machine
Learning (27th ICML), pages 175–182, 2010.

[17] Fabio Vitale, Nicol`o Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella. See the tree through the
lines: The shazoo algorithm. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N.
Pereira, and Kilian Q. Weinberger, editors, NIPS, pages 1584–1592, 2011.

[18] L. R. Ford and D. R. Fulkerson. Maximal Flow through a Network. Canadian Journal of Mathematics,

8:399–404, 1956.

[19] Michael O. Ball and J. Scott Provan. Calculating bounds on reachability and connectedness in stochastic

networks. Networks, 13(2):253–278, 1983.

[20] Thomas G¨artner and Gemma C. Garriga. The cost of learning directed cuts. In Proceedings of the 18th

European Conference on Machine Learning, 2007.

[21] J. M. Barzdin and R. V. Frievald. On the prediction of general recursive functions. Soviet Math. Doklady,

13:1224–1228, 1972.

[22] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108(2):212–

261, 1994.

9

