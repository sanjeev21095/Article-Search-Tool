Fast and Accurate Inference of Plackett–Luce Models

Lucas Maystre

EPFL

lucas.maystre@epfl.ch

matthias.grossglauser@epfl.ch

Matthias Grossglauser

EPFL

Abstract

We show that the maximum-likelihood (ML) estimate of models derived from
Luce’s choice axiom (e.g., the Plackett–Luce model) can be expressed as the
stationary distribution of a Markov chain. This conveys insight into several recently
proposed spectral inference algorithms. We take advantage of this perspective and
formulate a new spectral algorithm that is signiﬁcantly more accurate than previous
ones for the Plackett–Luce model. With a simple adaptation, this algorithm can
be used iteratively, producing a sequence of estimates that converges to the ML
estimate. The ML version runs faster than competing approaches on a benchmark
of ﬁve datasets. Our algorithms are easy to implement, making them relevant for
practitioners at large.

1

Introduction

Aggregating pairwise comparisons and partial rankings are important problems with applications in
econometrics [1], psychometrics [2, 3], sports ranking [4, 5] and multiclass classiﬁcation [6]. One
possible approach to tackle these problems is to postulate a statistical model of discrete choice. In
this spirit, Luce [7] stated the choice axiom in a foundational work published over ﬁfty years ago.
Denote p(i | A) the probability of choosing item i when faced with alternatives in the set A. Given
two items i and j, and any two sets of alternatives A and B containing i and j, the axiom posits that

p(i | A)
p(j | A)

=

p(i | B)
p(j | B)

.

In other words, the odds of choosing item i over item j are independent of the rest of the alternatives.
This simple assumption directly leads to a unique parametric choice model, known as the Bradley–
Terry model in the case of pairwise comparisons, and the Plackett–Luce model in the generalized case
of k-way rankings. In this paper, we highlight a connection between the maximum-likelihood (ML)
estimate under these models and the stationary distribution of a Markov chain parametrized by the
observed choices. Markov chains were already used in recent work [8, 9, 10] to aggregate pairwise
comparisons and rankings. These approaches reduce the problem to that of ﬁnding a stationary
distribution. By formalizing the link between the likelihood of observations under the choice model
and a certain Markov chain, we unify these algorithms and explicate them from an ML inference
perspective. We will also take a detour, and use this link in the reverse direction to give an alternative
proof to a recent result on the error rate of the ML estimate [11], by using spectral analysis techniques.
Beyond this, we make two contributions to statistical inference for this model. First, we develop
a simple, consistent and computationally efﬁcient spectral algorithm that is applicable to a wide
range of models derived from the choice axiom. The exact formulation of the Markov chain used
in the algorithm is distinct from related work [9, 10] and achieves a signiﬁcantly better statistical
efﬁciency at no additional computational cost. Second, we observe that with a small adjustment, the
algorithm can be used iteratively, and it then converges to the ML estimate. An evaluation on ﬁve
real-world datasets reveals that it runs consistently faster than competing approaches and has a much
more predictable performance that does not depend on the structure of the data. The key step, ﬁnding
a stationary distribution, can be ofﬂoaded to commonly available linear-algebra primitives, which

1

makes our algorithms scale well. Our algorithms are intuitively pleasing, simple to understand and
implement, and they outperform the state of the art, hence we believe that they will be highly useful
to practitioners.
The rest of the paper is organized as follows. We begin by introducing some notations and presenting
a few useful facts about the choice model and about Markov chains. By necessity, our exposition is
succinct, and the reader is encouraged to consult Luce [7] and Levin et al. [12] for a more thorough
exposition. In Section 2, we discuss related work. In Section 3, we present our algorithms, and in
Section 4 we evaluate them on synthetic and real-world data. We conclude in Section 5.
Discrete choice model. Denote by n the number of items. Luce’s choice axiom implies that each
item i ∈ {1, . . . , n} can be parametrized by a positive strength πi ∈ R>0 such that p(i | A) =
j∈A πj for any A containing i. The strengths π = [πi] are deﬁned up to a multiplicative
i πi = 1. An alternative parametrization of the model is given by

πi/(cid:80)
factor; for identiﬁability, we let(cid:80)

θi = log(πi), in which case the model is sometimes referred to as conditional logit [1].
Markov chain theory. We represent a ﬁnite, stationary, continuous-time Markov chain by a directed
graph G = (V, E), where V is the set of states and E is the set of transitions with positive rate. If G is
strongly connected, the Markov chain is said to be ergodic and admits a unique stationary distribution
π. The global balance equations relate the transition rates λij to the stationary distribution as follows:

πiλij =

πjλji ∀i.

(1)

(cid:88)

j(cid:54)=i

(cid:88)

j(cid:54)=i

The stationary distribution is therefore invariant to changes in the time scale, i.e., to a rescaling of the
transition rates. In the supplementary ﬁle, we brieﬂy discuss how to ﬁnd π given [λij].

2 Related work

Spectral methods applied to ranking and scoring items from noisy choices have a long-standing
history. To the best of our knowledge, Saaty [13] is the ﬁrst to suggest using the leading eigenvector
of a matrix of inconsistent pairwise judgments to score alternatives. Two decades later, Page et al.
[14] developed PageRank, an algorithm that ranks Web pages according to the stationary distribution
of a random walk on the hyperlink graph. In the same vein, Dwork et al. [8] proposed several variants
of Markov chains for aggregating heterogeneous rankings. The idea is to construct a random walk
that is biased towards high-ranked items, and use the ranking induced by the stationary distribution.
More recently, Negahban et al. [9] presented Rank Centrality, an algorithm for aggregating pairwise
comparisons close in spirit to that of [8]. When the data is generated under the Bradley–Terry model,
this algorithm asymptotically recovers model parameters with only ω(n log n) pairwise comparisons.
For the more general case of rankings under the Plackett–Luce model, Azari Souﬁani et al. [10]
propose to break rankings into pairwise comparisons and to apply an algorithm similar to Rank
Centrality. They show that the resulting estimator is statistically consistent. Interestingly, many of
these spectral algorithms can be related to the method of moments, a broadly applicable alternative to
maximum-likelihood estimation.
The history of algorithms for maximum-likelihood inference under Luce’s model goes back even
further. In the special case of pairwise comparisons, the same iterative algorithm was independently
discovered by Zermelo [15], Ford [16] and Dykstra [17]. Much later, this algorithm was explained
by Hunter [18] as an instance of minorization-maximization (MM) algorithm, and extended to the
more general choice model. Today, Hunter’s MM algorithm is the de facto standard for ML inference
in Luce’s model. As the likelihood can be written as a concave function, off-the-shelf optimization
procedures such as the Newton-Raphson method can also be used, although they have been been
reported to be slower and less practical [18]. Recently, Kumar et al. [19] looked at the problem
of ﬁnding the transition matrix of a Markov chain, given its stationary distribution. The problem
of inferring Luce’s model parameters from data can be reformulated in their framework, and the
ML estimate is the solution to the inversion of the stationary distribution. Their work stands out as
the ﬁrst to link ML inference to Markov chains, albeit very differently from the way presented in
our paper. Beyond algorithms, properties of the maximum-likelihood estimator in this model were
studied extensively. Hajek et al. [11] consider the Plackett–Luce model for k-way rankings. They
give an upper bound to the estimation error and show that the ML estimator is minimax-optimal. In
summary, they show that only ω(n/k log n) samples are enough to drive the mean-square error down

2

to zero, as n increases. Rajkumar and Agarwal [20] consider the Bradley–Terry model for pairwise
comparisons. They show that the ML estimator is able to recover the correct ranking, even when
the data is generated as per another model, e.g., Thurstone’s [2], as long as a so-called low-noise
condition is satisﬁed. We also mention that as an alternative to likelihood maximization, Bayesian
inference has also been proposed. Caron and Doucet [21] present a Gibbs sampler, and Guiver and
Snelson [22] propose an approximate inference algorithm based on expectation propagation.
In this work, we provide a unifying perspective on recent advances in spectral algorithms [9, 10] from
a maximum-likelihood estimation perspective. It turns out that this perspective enables us to make
contributions on both sides: On the one hand, we develop an improved and more general spectral
ranking algorithm, and on the other hand, we propose a faster procedure for ML inference by using
this algorithm iteratively.

3 Algorithms

We begin by expressing the ML estimate under the choice model as the stationary distribution
of a Markov chain. We then take advantage of this formulation to propose novel algorithms for
model inference. Although our derivation is made in the general choice model, we will also discuss
implications for the special cases of pairwise data in Section 3.3 and k-way ranking data in Section 3.4.
Suppose that we collect d independent observations in the multiset D = {(c(cid:96), A(cid:96)) | (cid:96) = 1, . . . , d}.
Each observation consists of a choice c(cid:96) among a set of alternatives A(cid:96); we say that i wins over j
and denote by i (cid:31) j whenever i, j ∈ A and c(cid:96) = i. We deﬁne the directed comparison graph as
GD = (V, E), with V = {1, . . . , n} and (j, i) ∈ E if i wins at least once over j in D. In order to
ensure that the ML estimate is well-deﬁned, we make the standard assumption that GD is strongly
connected [16, 18]. In practice, if this assumption does not hold, we can consider each strongly
connected component separately.

3.1 ML estimate as a stationary distribution

For simplicity, we denote the model parameter associated with item c(cid:96) by π(cid:96). The log-likelihood of
parameters π given observations D is

log π(cid:96) − log

d(cid:88)

(cid:96)=1

(cid:88)

j∈A(cid:96)

 .

πj

log L(π | D) =

.
For each item, we deﬁne two sets of indices. Let Wi
= {(cid:96) | i ∈
A(cid:96), c(cid:96) (cid:54)= i} be the indices of the observations where item i wins over and loses against the alternatives,
respectively. The log-likelihood (2) is not concave in π (it can be made strictly concave using a
simple reparametrization), but we brieﬂy show in the supplementary material that it admits a unique
stationary point, at the ML estimate ˆπ. The optimality condition ∇ ˆπ log L = 0 implies

(cid:32)

=

(cid:88)
 (cid:88)

(cid:96)∈Wi

(cid:96)∈Wi∩Lj

1
ˆπi −

ˆπj(cid:80)

t∈A(cid:96)

∂ log L
∂ ˆπi

(cid:88)

j(cid:54)=i

⇐⇒

1(cid:80)

j∈A(cid:96)

ˆπj

.
= {(cid:96) | i ∈ A(cid:96), c(cid:96) = i} and Li
(cid:33)
(cid:88)

(cid:88)
1(cid:80)
ˆπi(cid:80)

= 0 ∀i

 = 0 ∀i.

j∈A(cid:96)

(cid:96)∈Li

−

ˆπj

t∈A(cid:96)

ˆπt

(2)

(3)

(4)

(5)

In order to go from (3) to (4), we multiply by ˆπi and rearrange the terms. To simplify the notation, let
us further introduce the function

which takes observations S ⊆ D and an instance of model parameters π, and returns a non-negative
.
over j. Then (4) can be rewritten as(cid:88)
real number. Let Di(cid:31)j
= {(c(cid:96), A(cid:96)) ∈ D | (cid:96) ∈ Wi ∩ Lj}, i.e., the set of observations where i wins

(cid:88)

ˆπi · f (Dj(cid:31)i, ˆπ) =

ˆπj · f (Di(cid:31)j, ˆπ) ∀i.

j(cid:54)=i

ˆπt −

(cid:96)∈Wj∩Li

(cid:88)

A∈S

1(cid:80)

,

i∈A πi

.
=

f (S, π)

j(cid:54)=i

3

Algorithm 1 Luce Spectral Ranking
Require: observations D
1: λ ← 0n×n
2: for (i, A) ∈ D do
3:
4:
5:
6: end for
7: ¯π ← stat. dist. of Markov chain λ
8: return ¯π

for j ∈ A \ {i} do
λji ← λji + n/|A|
end for

Algorithm 2 Iterative Luce Spectral Ranking
Require: observations D
(cid:124)
1: π ← [1/n, . . . , 1/n]
2: repeat
3:
4:
5:
6:
7:
8:
9:
10: until convergence

λ ← 0n×n
for (i, A) ∈ D do
for j ∈ A \ {i} do
end for

end for
π ← stat. dist. of Markov chain λ

λji ← λji + 1/(cid:80)

t∈A πt

This formulation conveys a new viewpoint on the ML estimate. It is easy to recognize the global
balance equations (1) of a Markov chain on n states (representing the items), with transition rates
λji = f (Di(cid:31)j, ˆπ) and stationary distribution ˆπ. These transition rates have an interesting inter-
pretation: f (Di(cid:31)j, π) is the count of how many times i wins over j, weighted by the strength of
the alternatives. At this point, it is useful to observe that for any parameters π, f (Di(cid:31)j, π) ≥ 1
if (j, i) ∈ E, and 0 otherwise. Combined with the assumption that GD is strongly connected, it
follows that any π parametrizes the transition rates of an ergodic (homogeneous) Markov chain. The
ergodicity of the inhomogeneous Markov chain, where the transition rates are constantly updated to
reﬂect the current distribution over states, is shown by the following theorem.
Theorem 1. The Markov chain with inhomogeneous transition rates λji = f (Di(cid:31)j, π) converges to
the maximum-likelihood estimate ˆπ, for any initial distribution in the open probability simplex.

Proof (sketch). By (5), ˆπ is the unique invariant distribution of the Markov chain. In the supplemen-
tary ﬁle, we look at an equivalent uniformized discrete-time chain. Using the contraction mapping
principle, one can show that this chain converges to the invariant distribution.

3.2 Approximate and exact ML inference

We approximate the Markov chain described in (5) by considering a priori that all alternatives have
(cid:124)
.
= f (Di(cid:31)j, π) by ﬁxing π to [1/n, . . . , 1/n]
equal strength. That is, we set the transition rates λji
.
For i (cid:54)= j, the contribution of i winning over j to the rate of transition λji is n/|A|. In other words,
for each observation, the winning item is rewarded by a ﬁxed amount of incoming rate that is evenly
split across the alternatives (the chunk allocated to itself is discarded.) We interpret the stationary
distribution ¯π as an estimate of model parameters. Algorithm 1 summarizes this procedure, called
Luce Spectral Ranking (LSR.) If we consider a growing number of observations, LSR converges to
the true model parameters π∗, even in the restrictive case where the sets of alternatives are ﬁxed.
Theorem 2. Let A = {A(cid:96)} be a collection of sets of alternatives such that for any partition of A into
two non-empty sets S and T , (∪A∈SA) ∩ (∪A∈T A) (cid:54)= ∅1. Let d(cid:96) be the number of choices observed
over alternatives A(cid:96). Then ¯π → π∗ as d(cid:96) → ∞ ∀(cid:96).
Proof (sketch). The condition on A ensures that asymptotically GD is strongly connected. Let
d → ∞ be a shorthand for d(cid:96) → ∞ ∀(cid:96). We can show that if items i and j are compared in at least one
set of alternatives, the ratio of transition rates satisﬁes limd→∞ λij/λji = π∗
i . It follows that in
the limit of d → ∞, the stationary distribution is π∗. A rigorous proof is given in the supplementary
ﬁle.

j /π∗

Starting from the LSR estimate, we can iteratively reﬁne the transition rates of the Markov chain and
obtain a sequence of estimates. By (5), the only ﬁxed point of this iteration is the ML estimate ˆπ. We
call this procedure I-LSR and describe it in Algorithm 2.
LSR (or one iteration of I-LSR) entails (a) ﬁlling a matrix of (weighted) pairwise counts and
(b) ﬁnding a stationary distribution. Let D
(cid:96) |A(cid:96)|, and let S be the running time of ﬁnding the
stationary distribution. Then LSR has running time O(D + S). As a comparison, one iteration of

=(cid:80)

.

1 This is equivalent to stating that the hypergraph H = (V,A) is connected.

4

the MM algorithm [18] is O(D). Finding the stationary distribution can be implemented in different
ways. For example, in a sparse regime where D (cid:28) n2, the stationary distribution can be found with
the power method in a few O(D) sparse matrix multiplications. In the supplementary ﬁle, we give
more details about possible implementations. In practice, whether D or S turns out to be dominant in
the running time is not a foregone conclusion.

3.3 Aggregating pairwise comparisons

A widely-used special case of Luce’s choice model occurs when all sets of alternatives contain exactly
two items, i.e., when the data consists of pairwise comparisons. This model was proposed by Zermelo
[15], and later by Bradley and Terry [3]. As the stationary distribution is invariant to changes in the
.
= |Di(cid:31)j| when using LSR on pairwise
time-scale, we can rescale the transition rates and set λji
data. Let S be the set containing the pairs of items that have been compared at least once. In the
case where each pair (i, j) ∈ S has been compared exactly p times, LSR is strictly equivalent to a
continuous-time Markov-chain formulation of Rank Centrality [9]. In fact, our derivation justiﬁes
Rank Centrality as an approximate ML inference algorithm for the Bradley–Terry model. Furthermore,
we provide a principled extension of Rank Centrality to the case where the number of comparisons
observed is unbalanced. Rank Centrality considers transition rates proportional to the ratio of wins,
whereas (5) justiﬁes making transition rates proportional to the count of wins.
Negahban et al. [9] also provide an upper bound on the error rate of Rank Centrality, which essentially
shows that it is minimax-optimal. Because the two estimators are equivalent in the setting of balanced
pairwise comparisons, the bound also applies to LSR. More interestingly, the expression of the ML
estimate as a stationary distribution enables us to reuse the same analytical techniques to bound the
error of the ML estimate. In the supplementary ﬁle, we therefore provide an alternative proof of the
recent result of Hajek et al. [11] on the minimax-optimality of the ML estimate.

3.4 Aggregating partial rankings

Another case of interest is when observations do not consist of only a single choice, but of a ranking
over the alternatives. We now suppose m observations consisting of k-way rankings, 2 ≤ k ≤ n.
For conciseness, we suppose that k is the same for all observations. Let one such observation
be σ(1) (cid:31) . . . (cid:31) σ(k), where σ(p) is the item with p-th rank. Luce [7] and later Plackett [4]
independently proposed a model of rankings where

k(cid:89)

r=1

πσ(r)(cid:80)k

p=r πσ(p)

.

P (σ(1) (cid:31) . . . (cid:31) σ(k)) =

In this model, a ranking can be interpreted as a sequence of k − 1 independent choices: Choose the
ﬁrst item, then choose the second among the remaining alternatives, etc. With this point of view in
mind, LSR and I-LSR can easily accommodate data consisting of k-way rankings, by decomposing
the m observations into d = m(k − 1) choices.
Azari Souﬁani et al. [10] provide a class of consistent estimators for the Plackett–Luce model, using
the idea of breaking rankings into pairwise comparisons. Although they explain their algorithms from
a generalized-method-of-moments perspective, it is straightforward to reinterpret their estimators as
stationary distributions of particular Markov chains. In fact, for k = 2, their algorithm GMM-F is

identical to LSR. When k > 2 however, breaking a ranking into(cid:0)k

(cid:1) pairwise comparisons implicitly

makes the (incorrect) assumption that these comparisons are statistically independent. The Markov
chain that LSR builds breaks rankings into pairwise rate contributions, but weights the contributions
differently depending on the rank of the winning item. In Section 4, we show that this weighting
turns out to be crucial. Our approach yields a signiﬁcant improvement in statistical efﬁciency, yet
keeps the same attractive computational cost and ease of use.

2

3.5 Applicability to other models

Several other variants and extensions of Luce’s choice model have been proposed. For example, Rao
and Kupper [23] extend the Bradley–Terry model to the case where a comparison between two items
can result in a tie. In the supplementary ﬁle, we show that the ML estimate in the Rao–Kupper model
can also be formulated as a stationary distribution, and we provide corresponding adaptations of LSR

5

and I-LSR. We believe that our algorithms can be generalized to further models that are based on the
choice axiom. However, this axiom is key, and other choice models (such as Thurstone’s [2]) do not
admit the stationary-distribution interpretation we derive here.

4 Experimental evaluation

In this section, we compare LSR and I-LSR to other inference algorithms in terms of (a) statistical
efﬁciency, and (b) empirical performance. In order to understand the efﬁciency of the estimators,
we generate synthetic data from a known ground truth. Then, we look at ﬁve real-world datasets
and investigate the practical performance of the algorithms in terms of accuracy, running time and
convergence rate.
Error metric. As the probability of i winning over j depends on the ratio of strengths πi/πi, the
strengths are typically logarithmically spaced. In order to evaluate the accuracy of an estimate π to
ground truth parameters π∗, we therefore use a log transformation, reminiscent of the random-utility-
.
theoretic formulation of the choice model [1, 11]. Deﬁne θ
= [log πi − t], with t chosen such that

i θi = 0. We will consider the root-mean-squared error (RMSE)

(cid:80)

4.1 Statistical efﬁciency

ERMS = (cid:107)θ − θ∗

(cid:107)2/√n.

To assess the statistical efﬁciency of LSR and other algorithms, we follow the experimental procedure
of Hajek et al. [11]. We consider n = 1024 items, and draw θ∗ uniformly at random in [−2, 2]n.
We generate d = 64 full rankings over the n items from a Plackett-Luce model parametrized with
π ∝ [eθi]. For a given k ∈ {21, . . . , 210}, we break down each of the full rankings as follows. First,
we partition the items into n/k subsets of size k uniformly at random. Then, we store the k-way
rankings induced by the full ranking on each of those subsets. As a result, we obtain m = dn/k
statistically independent k-way partial rankings. For a given estimator, this data produces an estimate
θ, for which we record the root-mean-square error to θ∗. We consider four estimators. The ﬁrst two
(LSR and ML) work on the ranking data directly. The remaining two follow Azari Souﬁani et al. [10],

who suggest breaking down k-way rankings into(cid:0)k

(cid:1) pairwise comparisons. These comparisons are

then used by LSR, resulting in Azari Souﬁani et al.’s GMM-F estimator, and by an ML estimator
(ML-F.) In short, the four estimators vary according to (a) whether they use as-is rankings or derived
comparisons, and (b) whether the model is ﬁtted using an approximate spectral algorithm or using
exact maximum likelihood. Figure 1 plots ERMS for increasing sizes of partial rankings, as well as
a lower bound to the error of any estimator for the Plackett-Luce model (see Hajek et al. [11] for
details.) We observe that breaking the rankings into pairwise comparisons (*-F estimators) incurs a
signiﬁcant efﬁciency loss over using the k-way rankings directly (LSR and ML.) We conclude that by
correctly weighting pairwise rates in the Markov chain, LSR distinctly outperforms the rank-breaking
approach as k increases. We also observe that the ML estimate is always more efﬁcient. Spectral
estimators such as LSR provide a quick, asymptotically consistent estimate of parameters, but this
observation justiﬁes calling them approximate inference algorithms.

2

4.2 Empirical performance

We investigate the performance of various inference algorithms on ﬁve real-world datasets. The
NASCAR [18] and sushi [24] datasets contain multiway partial rankings. The YouTube, GIFGIF
and chess datasets2 contain pairwise comparisons. Among those, the chess dataset is particular in
that it features 45% of ties; in this case we use the extension of the Bradley–Terry model proposed
by Rao and Kupper [23]. We preprocess each dataset by discarding items that are not part of the
largest strongly connected component in the comparison graph. The number of items n, the number
of rankings m, as well as the size of a partial rankings k for each dataset are given in Table 1.
Additional details on the experimental setup are given in the supplementary material. We ﬁrst
compare the estimates produced by three approximate ML inference algorithms, LSR, GMM-F and
Rank Centrality (RC.) Note that RC applies only to pairwise comparisons, and that LSR is the only

2

See https://archive.ics.uci.edu/ml/machine-learning-databases/00223/,

http://www.gif.gf/ and https://www.kaggle.com/c/chess.

6

Figure 1: Statistical efﬁciency of different estimators for increasing sizes of partial rankings. As k
grows, breaking rankings into pairwise comparisons becomes increasingly inefﬁcient. LSR remains
efﬁcient at no additional computational cost.

algorithm able to infer the parameters in the Rao-Kupper model. Also note that in the case of pairwise
comparisons, GMM-F and LSR are strictly equivalent. In Table 1, we report the root-mean-square
deviation to the ML estimate ˆθ and the running time T of the algorithm.

Table 1: Performance of approximate ML inference algorithms

Dataset
NASCAR
Sushi
YouTube
GIFGIF
Chess

16 187
5 503

6 174

1 128 704
95 281

63 421

n

83
100

m k

36
5 000

LSR

GMM-F

RC

ERMS

0.194
0.034

0.417
1.286

0.420

T [s]
0.03
0.22

34.18
1.90

2.90

ERMS

0.751
0.130

0.417
1.286
—

T [s] ERMS
—
0.06
—
0.19
0.432
1.295
—

34.18
1.90
—

T [s]
—
—
41.91
2.84
—

43
10

2
2

2

The smallest value of ERMS is highlighted in bold for each dataset. We observe that in the case of
multiway partial rankings, LSR is almost four times more accurate than GMM-F on the datasets
considered. In the case of pairwise comparisons, RC is slightly worse than LSR and GMM-F, because
the number of comparisons per pair is not homogeneous (see Section 3.3.) The running time of the
three algorithms is comparable.
Next, we turn our attention to ML inference and consider three iterative algorithms: I-LSR, MM and
Newton-Raphson. For Newton-Raphson, we use an off-the-shelf solver. Each algorithm is initialized
(cid:124)
with π(0) = [1/n, . . . , 1/n]
, and convergence is declared when ERMS < 0.01. In Table 2, we report
the number of iterations I needed to reach convergence, as well as the total running time T of the
algorithm.

Table 2: Performance of iterative ML inference algorithms.

Dataset
γD
NASCAR 0.832
Sushi
0.890
YouTube
GIFGIF
Chess

0.002
0.408

0.007

I

3
2

12
10

15

I-LSR

T [s]
0.08
0.42

I

4
4

414.44
22.31

8 680
119

MM

Newton
T [s]
I
0.10 —
3
1.09
22 443.88 —
5

T [s]
—
10.45
—
72.38

109.62

43.69

181

55.61

3

49.37

The smallest total running time T is highlighted in bold for each dataset. We observe that Newton-
Raphson does not always converge, despite the log-likelihood being strictly concave3. I-LSR consis-
3 On the NASCAR dataset, this has also been noted by Hunter [18]. Computing the Newton step appears to
be severely ill-conditioned for many real-world datasets. We believe that it can be addressed by a careful choice

7

212223242526272829210k0.10.20.4RMSElowerboundML-FGMM-FMLLSRtently outperforms MM and Newton-Raphson in running time. Even if the average running time per
iteration is in general larger than that of MM, it needs considerably fewer iterations: For the YouTube
dataset, I-LSR yields an increase in speed of over 50 times.
The slow convergence of minorization-maximization algorithms is known [18], yet the scale of the
issue and its apparent unpredictability is surprising. In Hunter’s MM algorithm, updating a given πi
involves only parameters of items to which i has been compared. Therefore, we speculate that the
convergence rate of MM is dependent on the expansion properties of the comparison graph GD. As
an illustration, we consider the sushi dataset. To quantify the expansion properties, we look at the
spectral gap γD of a simple random walk on GD; intuitively, the larger the spectral gap is, the better
the expansion properties are [12]. The original comparison graph is almost complete, and γD = 0.890.
By breaking each 10-way ranking into 5 independent pairwise comparisons, we effectively sparsify
the comparison graph. As a result, the spectral gap decreases to 0.784. In Figure 2, we show the
convergence rate of MM and I-LSR for the original (k = 10) and modiﬁed (k = 2) datasets. We
observe that both algorithms display linear convergence, however the rate at which MM converges
appears to be sensitive to the structure of the comparison graph. In contrast, I-LSR is robust to
changes in the structure. The spectral gap of each dataset is listed in Table 2.

Figure 2: Convergence rate of I-LSR and MM on the sushi dataset. When partial rankings (k = 10)
are broken down into independent comparisons (k = 2), the comparison graph becomes sparser.
I-LSR is robust to this change, whereas the convergence rate of MM signiﬁcantly decreases.

5 Conclusion

In this paper, we develop a stationary-distribution perspective on the maximum-likelihood estimate
of Luce’s choice model. This perspective explains and uniﬁes several recent spectral algorithms from
an ML inference point of view. We present our own spectral algorithm that works on a wider range of
data, and show that the resulting estimate signiﬁcantly outperforms previous approaches in terms of
accuracy. We also show that this simple algorithm, with a straighforward adaptation, can produce a
sequence of estimates that converge to the ML estimate. On real-world datasets, our ML algorithm is
always faster than the state of the art, at times by up to two orders of magnitude.
Beyond statistical and computational performance, we believe that a key strength of our algorithms
is that they are simple to implement. As an example, our implementation of LSR ﬁts in ten lines
of Python code. The most complex operation—ﬁnding a stationary distribution—can be readily
ofﬂoaded to commonly available and highly optimized linear-algebra primitives. As such, we believe
that our work is very useful for practitioners.

Acknowledgments

We thank Holly Cogliati-Bauereis, Stratis Ioannidis, Ksenia Konyushkova and Brunella Spinelli for
careful proofreading and comments on the text.

of starting point, step size, or by monitoring the numerical stability; however, these modiﬁcations are non-trivial
and impose an additional burden on the practitioner.

8

12345678910iteration10010−210−410−610−810−1010−12RMSEMM,k=10MM,k=2I-LSR,k=10I-LSR,k=2References

[1] D. McFadden. Conditional logit analysis of qualitative choice behavior. In P. Zarembka, editor, Frontiers

in Econometrics, pages 105–142. Academic Press, 1973.

[2] L. Thurstone. The method of paired comparisons for social values. Journal of Abnormal and Social

Psychology, 21(4):384–400, 1927.

[3] R. A. Bradley and M. E. Terry. Rank Analysis of Incomplete Block Designs: I. The Method of Paired

Comparisons. Biometrika, 39(3/4):324–345, 1952.

[4] R. L. Plackett. The Analysis of Permutations. Journal of the Royal Statistical Society, Series C (Applied

Statistics), 24(2):193–202, 1975.

[5] A. Elo. The Rating Of Chess Players, Past & Present. Arco, 1978.
[6] T. Hastie and R. Tibshirani. Classiﬁcation by pairwise coupling. The Annals of Statistics, 26(2):451–471,

1998.

[7] R. D. Luce. Individual Choice behavior: A Theoretical Analysis. Wiley, 1959.
[8] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank Aggregation Methods for the Web. In Proceedings

of the 10th international conference on World Wide Web (WWW 2001), Hong Kong, China, 2001.

[9] S. Negahban, S. Oh, and D. Shah. Iterative Ranking from Pair-wise Comparisons. In Advances in Neural

Information Processing Systems 25 (NIPS 2012), Lake Tahoe, CA, 2012.

[10] H. Azari Souﬁani, W. Z. Chen, D. C. Parkes, and L. Xia. Generalized Method-of-Moments for Rank
Aggregation. In Advances in Neural Information Processing Systems 26 (NIPS 2013), Lake Tahoe, CA,
2013.

[11] B. Hajek, S. Oh, and J. Xu. Minimax-optimal Inference from Partial Rankings. In Advances in Neural

Information Processing Systems 27 (NIPS 2014), Montreal, QC, Canada, 2014.

[12] D. A. Levin, Y. Peres, and E. L. Wilmer. Markov Chains and Mixing Times. American Mathematical

Society, 2008.

[13] T. L. Saaty. The Analytic Hierarchy Process: Planning, Priority Setting, Resource Allocation. McGraw-Hill,

1980.

[14] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank Citation Ranking: Bringing Order to the

Web. Technical report, Stanford University, 1998.

[15] E. Zermelo. Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeits-

rechnung. Mathematische Zeitschrift, 29(1):436–460, 1928.

[16] L. R. Ford, Jr. Solution of a Ranking Problem from Binary Comparisons. The American Mathematical

Monthly, 64(8):28–33, 1957.

[17] O. Dykstra, Jr. Rank Analysis of Incomplete Block Designs: A Method of Paired Comparisons Employing

Unequal Repetitions on Pairs. Biometrics, 16(2):176–188, 1960.

[18] D. R. Hunter. MM algorithms for generalized Bradley–Terry models. The Annals of Statistics, 32(1):

384–406, 2004.

[19] R. Kumar, A. Tomkins, S. Vassilvitskii, and E. Vee. Inverting a Steady-State. In Proceedings of the 8th

International Conference on Web Search and Data Mining (WSDM 2015), pages 359–368, 2015.

[20] A. Rajkumar and S. Agarwal. A Statistical Convergence Perspective of Algorithms for Rank Aggregation
from Pairwise Data. In Proceedings of the 31st International Conference on Machine Learning (ICML
2014), Beijing, China, 2014.

[21] F. Caron and A. Doucet. Efﬁcient Bayesian Inference for Generalized Bradley–Terry models. Journal of

Computational and Graphical Statistics, 21(1):174–196, 2012.

[22] J. Guiver and E. Snelson. Bayesian inference for Plackett–Luce ranking models. In Proceedings of the

26th International Conference on Machine Learning (ICML 2009), Montreal, Canada, 2009.

[23] P. V. Rao and L. L. Kupper. Ties in Paired-Comparison Experiments: A Generalization of the Bradley-Terry

Model. Journal of the American Statistical Association, 62(317):194–204, 1967.

[24] T. Kamishima and S. Akaho. Efﬁcient Clustering for Orders. In Mining Complex Data, pages 261–279.

Springer, 2009.

9

