Fast Randomized Kernel Ridge Regression with

Statistical Guarantees∗

† Electrical Engineering and Computer Sciences

Ahmed El Alaoui †
Michael W. Mahoney ‡
‡ Statistics and International Computer Science Institute
University of California, Berkeley, Berkeley, CA 94720.

{elalaoui@eecs,mmahoney@stat}.berkeley.edu

Abstract

One approach to improving the running time of kernel-based methods is to build
a small sketch of the kernel matrix and use it in lieu of the full matrix in the
machine learning task of interest. Here, we describe a version of this approach
that comes with running time guarantees as well as improved guarantees on its
statistical performance. By extending the notion of statistical leverage scores to
the setting of kernel ridge regression, we are able to identify a sampling distribu-
tion that reduces the size of the sketch (i.e., the required number of columns to
be sampled) to the effective dimensionality of the problem. This latter quantity is
often much smaller than previous bounds that depend on the maximal degrees of
freedom. We give an empirical evidence supporting this fact. Our second contri-
bution is to present a fast algorithm to quickly compute coarse approximations to
these scores in time linear in the number of samples. More precisely, the running
time of the algorithm is O(np2) with p only depending on the trace of the kernel
matrix and the regularization parameter. This is obtained via a variant of squared
length sampling that we adapt to the kernel setting. Lastly, we discuss how this
new notion of the leverage of a data point captures a ﬁne notion of the difﬁculty
of the learning problem.

1

Introduction

We consider the low-rank approximation of symmetric positive semi-deﬁnite (SPSD) matrices that
arise in machine learning and data analysis, with an emphasis on obtaining good statistical guaran-
tees. This is of interest primarily in connection with kernel-based machine learning methods. Recent
work in this area has focused on one or the other of two very different perspectives: an algorith-
mic perspective, where the focus is on running time issues and worst-case quality-of-approximation
guarantees, given a ﬁxed input matrix; and a statistical perspective, where the goal is to obtain good
inferential properties, under some hypothesized model, by using the low-rank approximation in
place of the full kernel matrix. The recent results of Gittens and Mahoney [2] provide the strongest
example of the former, and the recent results of Bach [3] are an excellent example of the latter.
In this paper, we combine ideas from these two lines of work in order to obtain a fast randomized
kernel method with statistical guarantees that are improved relative to the state-of-the-art.
To understand our approach, recall that several papers have established the crucial importance—
from the algorithmic perspective—of the statistical leverage scores, as they capture structural non-
uniformities of the input matrix and they can be used to obtain very sharp worst-case approximation
guarantees. See, e.g., work on CUR matrix decompositions [5, 6], work on the the fast approxi-
mation of the statistical leverage scores [7], and the recent review [8] for more details. Here, we

∗A technical report version of this conference paper is available at [1].

1

†
k)i, where Kk is the best rank k approximation of K and where K

simply note that, when restricted to an n × n SPSD matrix K and a rank parameter k, the statistical
leverage scores relative to the best rank-k approximation to K, call them (cid:96)i, for i ∈ {1, . . . , n}, are
the diagonal elements of the projection matrix onto the best rank-k approximation of K. That is,
†
(cid:96)i = diag(KkK
k is the Moore-
Penrose inverse of Kk. The recent work by Gittens and Mahoney [2] showed that qualitatively
improved worst-case bounds for the low-rank approximation of SPSD matrices could be obtained
in one of two related ways: either compute (with the fast algorithm of [7]) approximations to the
leverage scores, and use those approximations as an importance sampling distribution in a random
sampling algorithm; or rotate (with a Gaussian-based or Hadamard-based random projection) to a
random basis where those scores are uniformized, and sample randomly in that rotated basis.
In this paper, we extend these ideas, and we show that—from the statistical perspective—we are
able to obtain a low-rank approximation that comes with improved statistical guarantees by using
a variant of this more traditional notion of statistical leverage. In particular, we improve the recent
bounds of Bach [3], which provides the ﬁrst known statistical convergence result when substitut-
ing the kernel matrix by its low-rank approximation. To understand the connection, recall that a
key component of Bach’s approach is the quantity dmof = n(cid:107) diag( K(K + nλI)−1)(cid:107)∞, which he
calls the maximal marginal degrees of freedom.1 Bach’s main result is that by constructing a low-
rank approximation of the original kernel matrix by sampling uniformly at random p = O(dmof/)
columns, i.e., performing the vanilla Nystr¨om method, and then by using this low-rank approxima-
tion in a prediction task, the statistical performance is within a factor of 1 +  of the performance
when the entire kernel matrix is used. Here, we show that this uniform sampling is suboptimal. We
do so by sampling with respect to a coarse but quickly-computable approximation of a variant to the
statistical leverage scores, given in Deﬁnition 1 below, and we show that we can obtain similar 1 + 
guarantees by sampling only O(deff/) columns, where deff = Tr(K(K + nλI)−1) < dmof. The
quantity deff is called the effective dimensionality of the learning problem, and it can be interpreted
as the implicit number of parameters in this nonparametric setting [9, 10].
We expect that our results and insights will be useful much more generally. As an example of this,
we can directly compare the Nystr¨om sampling method to a related divide-and-conquer approach,
thereby answering an open problem of Zhang et al. [9]. Recall that the Zhang et al. divide-and-
conquer method consists of dividing the dataset {(xi, yi)}n
i=1 into m random partitions of equal size,
computing estimators on each partition in parallel, and then averaging the estimators. They prove
the minimax optimality of their estimator, although their multiplicative constants are suboptimal;
and, in terms of the number of kernel evaluations, their method requires m× (n/m)2, with m in the
order of n/d2
eff) evaluations. They noticed that the scaling
of their estimator was not directly comparable to that of the Nystr¨om sampling method (which was
proven to only require O(ndmof) evaluations, if the sampling is uniform [3]), and they left it as an
open problem to determine which if either method is fundamentally better than the other. Using
our Theorem 3, we are able to put both results on a common ground for comparison. Indeed, the
estimator obtained by our non-uniform Nystr¨om sampling requires only O(ndeff) kernel evaluations
(compared to O(nd2
eff) and O(ndmof)), and it obtains the same bound on the statistical predictive
performance as in [3]. In this sense, our result combines “the best of both worlds,” by having the
reduced sample complexity of [9] and the sharp approximation bound of [3].

eff, which gives a total number of O(nd2

2 Preliminaries and notation
Let {(xi, yi)}n
space. The kernel-based learning problem can be cast as the following minimization problem:

i=1 be n pairs of points in X × Y, where X is the input space and Y is the response

n(cid:88)

i=1

min
f∈F

1
n

(cid:96)(yi, f (xi)) +

(cid:107)f(cid:107)2F ,

λ
2

(1)

where F is a reproducing kernel Hilbert space and (cid:96) : Y × Y → R is a loss function. We denote by
k : X ×X → R the positive deﬁnite kernel corresponding to F and by φ : X → F a corresponding
feature map. That is, k(x, x(cid:48)) = (cid:104)φ(x), φ(x(cid:48))(cid:105)F for every x, x(cid:48) ∈ X . The representer theorem
[11, 12] allows us to reduce Problem (1) to a ﬁnite-dimensional optimization problem, in which

1We will refer to it as the maximal degrees of freedom.

2

n(cid:88)

i=1

case Problem (1) boils down to ﬁnding the vector α ∈ Rn that solves

min
α∈Rn

1
n

(cid:96)(yi, (Kα)i) +

α(cid:62)Kα,

λ
2

(2)

where Kij = k(xi, xj). We let U ΣU(cid:62) be the eigenvalue decomposition of K, with Σ =
Diag(σ1,··· , σn), σ1 ≥ ··· ≥ σn ≥ 0, and U an orthogonal matrix. The underlying data model is

yi = f∗(xi) + σ2ξi

i = 1,··· , n

with f∗ ∈ F, (xi)1≤i≤n a deterministic sequence and ξi are i.i.d. standard normal random variables.
We consider (cid:96) to be the squared loss, in which case we will be interested in the mean squared error
as a measure of statistical risk: for any estimator ˆf, let

R( ˆf ) :=

Eξ(cid:107) ˆf − f∗(cid:107)2

2

1
n

(3)

be the risk function of ˆf where Eξ denotes the expectation under the randomness induced by ξ. In
this setting the problem is called Kernel Ridge Regression (KRR). The solution to Problem (2) is
α = (K + nλI)−1y, and the estimate of f∗ at any training point xi is given by ˆf (xi) = (Kα)i.
We will use ˆfK as a shorthand for the vector ( ˆf (xi))1≤i≤n ∈ Rn when the matrix K is used as a
kernel matrix. This notation will be used accordingly for other kernel matrices (e.g. ˆfL for a matrix
L). Recall that the risk of the estimator ˆfK can then be decomposed into a bias and variance term:

R( ˆfK) =

1
n
1
n

Eξ(cid:107)K(K + nλI)−1(f∗ + σ2ξ) − f∗(cid:107)2
(cid:107)(K(K + nλI)−1 − I)f∗(cid:107)2
σ2
n
variance(K).

=
= nλ2(cid:107)(K + nλI)−1f∗(cid:107)2

bias(K)2

σ2
n

2 +

:=

+

2

Eξ(cid:107)K(K + nλI)−1ξ(cid:107)2

2

2 +
Tr(K 2(K + nλI)−2)

(4)

Solving Problem (2), either by a direct method or by an optimization algorithm needs at least a
quadratic and often cubic running time in n which is prohibitive in the large scale setting. The
so-called Nytr¨om method approximates the solution to Problem (2) by substituting K with a low-
rank approximation to K. In practice, this approximation is often not only fast to construct, but
the resulting learning problem is also often easier to solve [13, 14, 15, 2]. The method operates
as follows. A small number of columns K1,··· , Kp are randomly sampled from K.
If we let
C = [K1,··· , Kp] ∈ Rn×p denote the matrix containing the sampled columns, W ∈ Rp×p the
overlap between C and C(cid:62) in K, then the Nystr¨om approximation of K is the matrix

L = CW †C(cid:62).

More generally, if we let S ∈ Rn×p be an arbitrary sketching matrix, i.e., a tall and skinny matrix
that, when left-multiplied by K, produces a “sketch” of K that preserves some desirable properties,
then the Nystr¨om approximation associated with S is

L = KS(S(cid:62)KS)†S(cid:62)K.

For instance, for random sampling algorithms, S would contain a non-zero entry at position (i, j) if
the i-th column of K is chosen at the j-th trial of the sampling process. Alternatively, S could also
be a random projection matrix; or S could be constructed with some other (perhaps deterministic)
method, as long as it veriﬁes some structural properties, depending on the application [8, 2, 6, 5].
We will focus in this paper on analyzing this approximation in the statistical prediction context
related to the estimation of f∗ by solving Problem (2). We proceed by revisiting and improving
upon prior results from three different areas. The ﬁrst result (Theorem 1) is on the behavior of the
bias of ˆfL, when L is constructed using a general sketching matrix S. This result underlies the
statistical analysis of the Nystr¨om method. To see this, ﬁrst, it is not hard to prove that L (cid:22) K
in the sense of usual the order on the positive semi-deﬁnite cone. Second, one can prove that the
variance is matrix-increasing, hence the variance will decrease when replacing K by L. On the other

3

hand, the bias (while not matrix monotone in general) can be proven to not increase too much when
replacing K by L. This latter statement will be the main technical difﬁculty for obtaining a bound
on R( ˆfL) (see Appendix A). A form of this result is due to Bach [3] in the case where S is a uniform
sampling matrix. The second result (Theorem 2) is a concentration bound for approximating matrix
multiplication when the rank-one components of the product are sampled non uniformly. This result
is derived from the matrix Bernstein inequality, and yields a sharp quantiﬁcation of the deviation
of the approximation from the true product. The third result (Deﬁnition 1) is an extension of the
deﬁnition of the leverage scores to the context of kernel ridge regression. Whereas the notion of
leverage is established as an algorithmic tool in randomized linear algebra, we introduce a natural
counterpart of it to this statistical setting. By combining these contributions, we are able to give a
sharp statistical statement on the behavior of the Nystr¨om method if one is allowed to sample non
uniformly. All the proofs are deferred to the appendix (or see [1]).
3 Revisiting prior work and new results

3.1 A structural result

We begin by stating a “structural” result that upper-bounds the bias of the estimator constructed
using the approximation L. This result is deterministic: it only depends on the properties of the
input data, and holds for any sketching matrix S that satisﬁes certain conditions. This way the
randomness of the construction of S is decoupled from the rest of the analysis. We highlight the fact
that this view offers a possible way of improving the current results since a better construction of S
-whether deterministic or random- satisfying the data-related conditions would immediately lead to
down stream algorithmic and statistical improvements in this setting.
Theorem 1. Let S ∈ Rn×p be a sketching matrix and L the corresponding Nystr¨om approxi-
Φ −
mation. For γ > 0, let Φ = Σ(Σ + nγI)−1.
, where λmax denotes the

Φ1/2U(cid:62)SS(cid:62)U Φ1/2(cid:17) ≤ t for t ∈ (0, 1) and λ ≥ 1

(cid:16)

maximum eigenvalue and (cid:107) · (cid:107)op is the operator norm then

op · λmax(K)

If the sketching matrix S satisﬁes λmax
1−t(cid:107)S(cid:107)2
(cid:19)

n

bias(K).

(5)

(cid:18)

bias(L) ≤

1 +

γ/λ
1 − t

√
pn in every column with p the
In the special case where S contains one non zero entry equal to 1/
number of sampled columns, the result and its proof can be found in [3] (appendix B.2), although
we believe that their argument contains a problematic statement. We propose an alternative and
complete proof in Appendix A. The subsequent analysis unfolds in two steps: (1) assuming the
sketching matrix S satisﬁes the conditions stated in Theorem 1, we will have R( ˆfL) (cid:46) R( ˆfK), and
(2) matrix concentration is used to show that an appropriate random construction of S satisﬁes the
said conditions. We start by stating the concentration result that is the source of our improvement
(section 3.2), deﬁne a notion of statistical leverage scores (section 3.3), and then state and prove
the main statistical result (Theorem 3 section 3.4). We then present our main algorithmic result
consisting of a fast approximation to this new notion of leverage scores (section 3.5).

3.2 A concentration bound on matrix multiplication
Next, we state our result for approximating matrix products of the form ΨΨ(cid:62) when a few columns
from Ψ are sampled to form the approximate product ΨI Ψ(cid:62)
I where ΨI contains the chosen columns.
The proof relies on a matrix Bernstein inequality (see e.g. [16]) and is presented at the end of the
paper (Appendix B).
Theorem 2. Let n, m be positive integers. Consider a matrix Ψ ∈ Rn×m and denote by ψi the ith
column of Ψ. Let p ≤ m and I = {i1,··· , ip} be a subset of {1,··· , m} formed by p elements
chosen randomly with replacement, according to the distribution

∀i ∈ {1,··· , m} Pr(choosing i) = pi ≥ β

4

(cid:107)ψi(cid:107)2
(cid:107)Ψ(cid:107)2

2

F

(6)

for some β ∈ (0, 1]. Let S ∈ Rn×p be a sketching matrix such that Sij = 1/
and 0 elsewhere. Then
−pt2/2
λmax(ΨΨ(cid:62))((cid:107)Ψ(cid:107)2

(cid:0)ΨΨ(cid:62) − ΨSS(cid:62)Ψ(cid:62)(cid:1) ≥ t

(cid:17) ≤ n exp

(cid:18)

λmax

(cid:16)

Pr

√

p · pij only if i = ij

(cid:19)

F /β + t/3)

.

(7)

Remarks: 1. This result will be used for Ψ = Φ1/2U(cid:62), in conjunction with Theorem 1 to prove
our main result in Theorem 3. Notice that Ψ(cid:62) is a scaled version of the eigenvectors, with a scaling
given by the diagonal matrix Φ = Σ(Σ + nγI)−1 which should be considered as “soft projection”
matrix that smoothly selects the top part of the spectrum of K. The setting of Gittens et al. [2], in
which Φ is a 0-1 diagonal is the closest analog of our setting.

(cid:107)ψi(cid:107)2
(cid:107)Ψ(cid:107)2

2

F

2. It is known that pi =
is the optimal sampling distribution in terms of minimizing the
expected error E(cid:107)ΨΨ(cid:62) − ΨSS(cid:62)Ψ(cid:62)(cid:107)2
F [17]. The above result exhibits a robustness property by
allowing the chosen sampling distribution to be different from the optimal one by a factor β.2 The
sub-optimality of such a distribution is reﬂected in the upper bound (7) by the ampliﬁcation of the
squared Frobenius norm of Ψ by a factor 1/β. For instance, if the sampling distribution is chosen
to be uniform, i.e. pi = 1/m, then the value of β for which (6) is tight is
, in which
case we recover a concentration result proven by Bach [3]. Note that Theorem 2 is derived from
one of the state-of-the-art bounds on matrix concentration, but it is one among many others in the
literature; and while it constitutes the base of our improvement, it is possible that a concentration
bound more tailored to the problem might yield sharper results.

m maxi (cid:107)ψi(cid:107)2

(cid:107)Ψ(cid:107)2

F

2

3.3 An extended deﬁnition of leverage

We introduce an extended notion of leverage scores that is speciﬁcally tailored to the ridge regression
problem, and that we call the λ-ridge leverage scores.
Deﬁnition 1. For λ > 0, the λ-ridge leverage scores associated with the kernel matrix K and the
parameter λ are

∀i ∈ {1,··· , n},

li(λ) =

U 2
ij.

(8)

n(cid:88)

σj

σj + nλ

j=1

Note that li(λ) is the ith diagonal entry of K(K + nλI)−1. The quantities (li(λ))1≤i≤n are in this
setting the analogs of the so-called leverage scores in the statistical literature, as they characterize the
data points that “stick out”, and consequently that most affect the result of a statistical procedure.
They are classically deﬁned as the row norms of the left singular matrix U of the input matrix,
and they have been used in regression diagnostics for outlier detection [18], and more recently in
randomized matrix algorithms as they often provide an optimal importance sampling distribution
for constructing random sketches for low rank approximation [17, 19, 5, 6, 2] and least squares
regression [20] when the input matrix is tall and skinny (n ≥ m). In the case where the input matrix
is square, this deﬁnition is vacuous as the row norms of U are all equal to 1. Recently, Gittens and
Mahoney [2] used a truncated version of these scores (that they called leverage scores relative to the
best rank-k space) to obtain the best algorithmic results known to date on low rank approximation
of positive semi-deﬁnite matrices. Deﬁnition 1 is a weighted version of the classical leverage scores,
where the weights depend on the spectrum of K and a regularization parameter λ. In this sense, it is
an interpolation between Gittens’ scores and the classical (tall-and-skinny) leverage scores, where
the parameter λ plays the role of a rank parameter. In addition, we point out that Bach’s maximal
degrees of freedom dmof is to the λ-ridge leverage scores what the coherence is to Gittens’ leverage
scores, i.e. their (scaled) maximum value: dmof/n = maxi li(λ); and that while the sum of Gittens’
scores is the rank parameter k, the sum of the λ-ridge leverage scores is the effective dimensionality
deff. We argue in the following that Deﬁnition 1 provides a relevant notion of leverage in the context
of kernel ridge regression. It is the natural counterpart of the algorithmic notion of leverage in the
prediction context. We use it in the next section to make a statistical statement on the performance
of the Nystr¨om method.

2In their work [17], Drineas et al. have a comparable robust statement for controlling the expected error.

Our result is a robust quantiﬁcation of the tail probability of the error, which is a much stronger statement.

5

3.4 Main statistical result: an error bound on approximate kernel ridge regression

Now we are able to give an improved version of a theorem by Bach [3] that establishes a performance
guaranty on the use of the Nystr¨om method in the context of kernel ridge regression. It is improved
in the sense that the sufﬁcient number of columns that should be sampled in order to incur no
(or little) loss in the prediction performance is lower. This is due to a more data-sensitive way of
sampling the columns of K (depending on the λ-ridge leverage scores) during the construction of
the approximation L. The proof is in Appendix C.
Theorem 3. Let λ,  > 0, ρ ∈ (0, 1/2), n ≥ 2 and L be a Nystr¨om approximation of K by choosing
p columns randomly with replacement according to a probability distribution (pi)1≤i≤n such that

∀i ∈ {1,··· , n}, pi ≥ β · li(λ)/(cid:80)n
(cid:19)
with deff =(cid:80)n

(cid:18) deff

p ≥ 8

1
6

+

β

i=1 li(λ) = Tr(K(K + nλI)−1) then

ρ

(cid:18) n

i=1 li(λ) for some β ∈ (0, 1]. Let l ≤ mini li(λ). If
log

and λ ≥ 2

(cid:19) λmax(K)

(cid:19)

(cid:18)

1 +

1
l

,

n

R( ˆfL) ≤ (1 + 2)2R( ˆfK)

with probability at least 1 − 2ρ, where (li)i are introduced in Deﬁnition 1 and R is deﬁned in (3).
Theorem 3 asserts that substituting the kernel matrix K by a Nystr¨om approximation of rank p in the
KRR problem induces an arbitrarily small prediction loss, provided that p scales linearly with the
3 and that λ is not too small4. The leverage-based sampling appears to be
effective dimensionality deff
crucial for obtaining this dependence, as the λ-ridge leverage scores provide information on which
columns -and hence which data points- capture most of the difﬁculty of the estimation problem.
Also, as a sanity check, the smaller the target accuracy , the higher deff, and the more uniform the
sampling distribution (li(λ))i becomes. In the limit  → 0, p is in the order of n and the scores
are uniform, and the method is essentially equivalent to using the entire matrix K. Moreover, if
the sampling distribution (pi)i is a factor β away from optimal, a slight oversampling (i.e. increase
p by 1/β) achieves the same performance. In this sense, the above result shows robustness to the
sampling distribution. This property is very beneﬁcial from an implementation point of view, as
the error bounds still hold when only an approximation of the leverage scores is available. If the
columns are sampled uniformly, a worse lower bound on p that depends on dmof is obtained [3].

3.5 Main algorithmic result: a fast approximation to the λ-ridge leverage scores

Although the λ-ridge leverage scores can be naively computed using SVD, the exact computation is
as costly as solving the original Problem (2). Therefore, the central role they play in the above result
motivates the problem of a fast approximation, in a similar way the importance of the usual leverage
scores has motivated Drineas et al. to approximate them is random projection time [7]. A success in
this task will allow us to combine the running time beneﬁts with the improved statistical guarantees
we have provided.
Algorithm:

{1, 2,···}, λ > 0,  ∈ (0, 1/2).

• Inputs: data points (xi)1≤i≤n, probability vector (pi)1≤i≤n, sampling parameter p ∈
• Output: (˜li)1≤i≤n -approximations to (li(λ))1≤i≤n.
1. Sample p data points from (xi)1≤i≤n with replacement with probabilities (pi)1≤i≤n.
2. Compute the corresponding columns K1,··· , Kp of the kernel matrix.
3. Construct C = [K1,··· , Kp] ∈ Rn×p and W ∈ Rp×p as presented in Section 2.
4. Construct B ∈ Rn×p such that BB(cid:62) = CW †C(cid:62).
5. For every i ∈ {1,··· , n}, set

˜li = B(cid:62)

i (B(cid:62)B + nλI)−1Bi

(9)

where Bi is the i-th row of B, and return it.

effective dimensionality [10, 9, 3] However, the following bound holds: deff ≤ 1

3Note that deff depends on the precision parameter , which is absent in the classical deﬁnition of the
4This condition on λ is not necessary if one constructs L as KS(S(cid:62)KS + nλI)−1S(cid:62)K (see proof).

 Tr(K(K + nλI)−1).

6

p ≥ 8

then we have ∀i ∈ {1,··· , n}

(cid:18) Tr(K)

nλ

(cid:18) n

(cid:19)

ρ

(cid:19)

+

1
6

log

(cid:16) σn − nλ

(cid:17)

σn + nλ

(additive error bound)

li(λ) − 2 ≤ ˜li ≤ li(λ)

Running time: The running time of the above algorithm is dominated by steps 4 and 5. Indeed,
constructing B can be done using a Cholesky factorization on W and then a multiplication of C by
the inverse of the obtained Cholesky factor, which yields a running time of O(p3 +np2). Computing
the approximate leverage scores (˜li)1≤i≤n in step 5 also runs in O(p3 + np2). Thus, for p (cid:28) n,
the overall algorithm runs in O(np2). Note that formula (9) only involves matrices and vectors of
size p (everything is computed in the smaller dimension p), and the fact that this yields a correct
approximation relies on the matrix inversion lemma (see proof in Appendix D). Also, only the
relevant columns of K are computed and we never have to form the entire kernel matrix. This
improves over earlier models [2] that require that all of K has to be written down in memory. The
improved running time is obtained by considering the construction (9) which is quite different from
the regular setting of approximating the leverage scores of a rectangular matrix [7]. We now give
both additive and multiplicative error bounds on its approximation quality.
Theorem 4. Let  ∈ (0, 1/2), ρ ∈ (0, 1) and λ > 0. Let L be a Nystr¨om approximation of K by
choosing p columns at random with probabilities pi = Kii/Tr(K), i = 1,··· , n. If

and

(multiplicative error bound)

li(λ) ≤ ˜li ≤ li(λ)

with probability at least 1 − ρ.
Remarks: 1. Theorem 4 states that if the columns of K are sampled proportionally to Kii then
nλ ) is a sufﬁcient number of samples. Recall that Kii = (cid:107)φ(xi)(cid:107)2F , so our procedure is akin
O( Tr(K)
to sampling according to the squared lengths of the data vectors, which has been extensively used in
different contexts of randomized matrix approximation [21, 17, 19, 8, 2].
2. Due to how λ is deﬁned in eq. (1) the n in the denominator is artiﬁcial: nλ should be thought of as
a “rescaled” regularization parameter λ(cid:48). In some settings, the λ that yields the best generalization
√
error scales like O(1/
n) is sufﬁcient. On the other hand, if the columns
are sampled uniformly, one would get p = O(dmof) = O(n maxi li(λ)).
4 Experiments

√
n), hence p = O(Tr(K)/

We test our results based on several datasets: one synthetic regression problem from [3] to illus-
trate the importance of the λ-ridge leverage scores, the Pumadyn family consisting of three datasets
pumadyn-32fm, pumadyn-32fh and pumadyn-32nh 5 and the Gas Sensor Array Drift Dataset from
the UCI database6. The synthetic case consists of a regression problem on the interval X = [0, 1]
where, given a sequence (xi)1≤i≤n and a sequence of noise (i)1≤i≤n, we observe the sequence

yi = f (xi) + σ2i,

i ∈ {1,··· , n}.

(2β)! B2β(x−y−(cid:98)x−y(cid:99))
The function f belongs to the RKHS F generated by the kernel k(x, y) = 1
where B2β is the 2β-th Bernoulli polynomial [3]. One important feature of this regression problem
is the distribution of the points (xi)1≤i≤n on the interval X : if they are spread uniformly over the
interval, the λ-ridge leverage scores (li(λ))1≤i≤n are uniform for every λ > 0, and uniform column
n for i = 1,··· , n, the kernel matrix K is
sampling is optimal in this case. In fact, if xi = i−1
a circulant matrix [3], in which case, we can prove that the λ-ridge leverage scores are constant.
Otherwise, if the data points are distributed asymmetrically on the interval, the λ-ridge leverage
scores are non uniform, and importance sampling is beneﬁcial (Figure 1). In this experiment, the
data points xi ∈ (0, 1) have been generated with a distribution symmetric about 1
2, having a high
density on the borders of the interval (0, 1) and a low density on the center of the interval. The
number of observations is n = 500. On Figure 1, we can see that there are few data points with

5http://www.cs.toronto.edu/˜delve/data/pumadyn/desc.html
6https://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array+Drift+Dataset

7

Figure 1: The λ-ridge leverage scores for the synthetic Bernoulli data set described in the text (left) and
the MSE risk vs. the number of sampled columns used to construct the Nystr¨om approximation for different
sampling methods (right).
high leverage, and those correspond to the region that is underrepresented in the data sample (i.e. the
region close to the center of the interval since it is the one that has the lowest density of observations).
The λ-ridge leverage scores are able to capture the importance of these data points, thus providing a
way to detect them (e.g. with an analysis of outliers), had we not known their existence.
For all datasets, we determine λ and the band width of k by cross validation, and we compute the
effective dimensionality deff and the maximal degrees of freedom dmof. Table 1 summarizes the
It is often the case that deff (cid:28) dmof and R( ˆfL)/R( ˆfK) (cid:39) 1, in agreement with
experiments.
Theorem 3.

kernel
Bern

Linear

RBF

dataset
Synth
Gas2
Gas3

n
500
1244
1586
Pum-32fm 2000
2000
Pum-32fh
2000
Pum-32nh
1244
1586
Pum-32fm 2000
2000
Pum-32fh
Pum-32nh
2000

Gas2
Gas3

nb. feat

-
128
128
32
32
32
-
-
-
-
-

band width

-
-
-
-
-
-
1
1
5
5
5

λ
1e−6
1e−3
1e−3
1e−3
1e−3
1e−3
4.5e−4
5e−4
0.5
5e−2
1.3e−2

deff
24
126
125
31
31
32
1135
1450
142
747
1337

dmof
500
1244
1586
2000
2000
2000
1244
1586
1897
1989
1997

risk ratio R( ˆfL)/R( ˆfK)

(p = 2deff)
1.01
1.10 (p = 2deff)
1.09 (p = 2deff)
(p = 2deff)
0.99
(p = 2deff)
0.99
(p = 2deff)
0.99
1.56
(p = deff)
(p = deff)
1.50
(p = deff)
1.00
(p = deff)
1.00
0.99
(p = deff)

Table 1: Parameters and quantities of interest for the different datasets and using different kernels: the synthetic
dataset using the Bernoulli kernel (denoted by Synth), the Gas Sensor Array Drift Dataset (batches 2 and 3,
denoted by Gas2 and Gas3) and the Pumadyn datasets (Pum-32fm, Pum-32fh, Pum-32nh) using linear and
RBF kernels.
5 Conclusion

We showed in this paper that in the case of kernel ridge regression, the sampling complexity of the
Nystr¨om method can be reduced to the effective dimensionality of the problem, hence bridging and
improving upon different previous attempts that established weaker forms of this result. This was
achieved by deﬁning a natural analog to the notion of leverage scores in this statistical context, and
using it as a column sampling distribution. We obtained this result by combining and improving
upon results that have emerged from two different perspectives on low rank matrix approximation.
We also present a way to approximate these scores that is computationally tractable, i.e. runs in time
O(np2) with p depending only on the trace of the kernel matrix and the regularization parameter.
One natural unanswered question is whether it is possible to further reduce the sampling complexity,
or is the effective dimensionality also a lower bound on p? And as pointed out by previous work
[22, 3], it is likely that the same results hold for smooth losses beyond the squared loss (e.g. logistic
regression). However the situation is unclear for non-smooth losses (e.g. support vector regression).
Acknowledgements: We thank Xixian Chen for pointing out a mistake in an earlier draft of this
paper [1]. We thank Francis Bach for stimulating discussions and for contributing to a rectiﬁed proof
of Theorem 1. We thank Jason Lee and Aaditya Ramdas for fruitful discussions regarding the proof
of Theorem 1. We thank Yuchen Zhang for pointing out the connection to his work.

8

References
[1] Ahmed El Alaoui and Michael W Mahoney. Fast randomized kernel methods with statistical

guarantees. arXiv preprint arXiv:1411.0306, 2014.

[2] Alex Gittens and Michael W Mahoney. Revisiting the Nystr¨om method for improved large-
In Proceedings of The 30th International Conference on Machine

scale machine learning.
Learning, pages 567–575, 2013.

[3] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Proceedings of

The 26th Conference on Learning Theory, pages 185–209, 2013.

[4] Francis Bach. Personal communication, October 2015.
[5] Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error CUR matrix de-

compositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–881, 2008.

[6] Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data

analysis. Proceedings of the National Academy of Sciences, 106(3):697–702, 2009.

[7] Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast
approximation of matrix coherence and statistical leverage. The Journal of Machine Learning
Research, 13(1):3475–3506, 2012.

[8] Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and Trends

in Machine Learning, 3(2):123–224, 2011.

[9] Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regres-

sion. In Proceedings of The 26th Conference on Learning Theory, pages 592–617, 2013.

[10] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning,

volume 1. Springer series in statistics Springer, Berlin, 2001.

[11] George Kimeldorf and Grace Wahba. Some results on Tchebychefﬁan spline functions. Jour-

nal of Mathematical Analysis and Applications, 33(1):82–95, 1971.

[12] Bernhard Sch¨olkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In

Computational Learning Theory, pages 416–426. Springer, 2001.

[13] Shai Fine and Katya Scheinberg. Efﬁcient SVM training using low-rank kernel representations.

The Journal of Machine Learning Research, 2:243–264, 2002.

[14] Christopher Williams and Matthias Seeger. Using the Nystr¨om method to speed up kernel
machines. In Proceedings of the 14th Annual Conference on Neural Information Processing
Systems, pages 682–688, 2001.

[15] Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Sampling techniques for the Nystr¨om
method. In International Conference on Artiﬁcial Intelligence and Statistics, pages 304–311,
2009.

[16] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Compu-

tational Mathematics, 12(4):389–434, 2012.

[17] Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for
matrices I: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132–
157, 2006.

[18] Samprit Chatterjee and Ali S Hadi. Inﬂuential observations, high leverage points, and outliers

in linear regression. Statistical Science, pages 379–393, 1986.

[19] Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for
matrices II: Computing a low-rank approximation to a matrix. SIAM Journal on Computing,
36(1):158–183, 2006.

[20] Petros Drineas, Michael W Mahoney, S Muthukrishnan, and Tam´as Sarl´os. Faster least squares

approximation. Numerische Mathematik, 117(2):219–249, 2011.

[21] Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for ﬁnding

low-rank approximations. Journal of the ACM (JACM), 51(6):1025–1041, 2004.

[22] Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics,

4:384–414, 2010.

9

