Mixed Robust/Average Submodular Partitioning:
Fast Algorithms, Guarantees, and Applications

Kai Wei1

Rishabh Iyer1

Shengjie Wang2

Wenruo Bai1

Jeff Bilmes1

1 Department of Electrical Engineering, University of Washington

2 Department of Computer Science, University of Washington

{kaiwei, rkiyer, wangsj, wrbai, bilmes}@u.washington.edu

Abstract

We investigate two novel mixed robust/average-case submodular data partitioning
problems that we collectively call Submodular Partitioning. These problems gen-
eralize purely robust instances of the problem, namely max-min submodular fair
allocation (SFA) [12] and min-max submodular load balancing (SLB) [25], and
also average-case instances, that is the submodular welfare problem (SWP) [26]
and submodular multiway partition (SMP) [5]. While the robust versions have been
studied in the theory community [11, 12, 16, 25, 26], existing work has focused
on tight approximation guarantees, and the resultant algorithms are not generally
scalable to large real-world applications. This is in contrast to the average case,
where most of the algorithms are scalable. In the present paper, we bridge this gap,
by proposing several new algorithms (including greedy, majorization-minimization,
minorization-maximization, and relaxation algorithms) that not only scale to large
datasets but that also achieve theoretical approximation guarantees comparable
to the state-of-the-art. We moreover provide new scalable algorithms that apply
to additive combinations of the robust and average-case objectives. We show that
these problems have many applications in machine learning (ML), including data
partitioning and load balancing for distributed ML, data clustering, and image seg-
mentation. We empirically demonstrate the efﬁcacy of our algorithms on real-world
problems involving data partitioning for distributed optimization (of convex and
deep neural network objectives), and also purely unsupervised image segmentation.

1

Introduction

The problem of data partitioning is of great importance to many machine learning (ML) and data
science applications as is evidenced by the wealth of clustering procedures that have been and continue
to be developed and used. Most data partitioning problems are based on expected, or average-case,
utility objectives where the goal is to optimize a sum of cluster costs, and this includes the ubiquitous
k-means procedure [1]. Other algorithms are based on robust objective functions [10], where the
goal is to optimize the worst-case cluster cost. Such robust algorithms are particularly important
in mission critical applications, such as parallel and distributed computing, where one single poor
partition block can signiﬁcantly slow down an entire parallel machine (as all compute nodes might
need to spin while waiting for a slow node to complete a round of computation). Taking a weighted
combination of both robust and average case objective functions allows one to balance between
optimizing worst-case and overall performance. We are unaware, however, of any previous work that
allows for a mixing between worst- and average-case objectives in the context of data partitioning.
This paper studies two new mixed robust/average-case partitioning problems of the following form:

1

(cid:104)¯λ min

m(cid:88)

λ
m

(cid:105)

(cid:104)¯λ max

i

m(cid:88)

λ
m

(cid:105)

i

,

j=1

j=1

1 , Aπ

fi(Aπ

i ) +

fi(Aπ

i ) +

fj(Aπ
j )

fj(Aπ
j )

i ∩ Aπ

2 ,··· , Aπ

, Prob. 2: min
π∈Π

i = V and ∀i (cid:54)= j, Aπ

Prob. 1: max
π∈Π
where 0 ≤ λ ≤ 1, ¯λ (cid:44) 1 − λ, the set of sets π = (Aπ
m) is a partition of a ﬁnite
set V (i.e, ∪iAπ
j = ∅), and Π refers to the set of all partitions of
V into m blocks. The parameter λ controls the objective: λ = 1 is the average case, λ = 0 is
the robust case, and 0 < λ < 1 is a mixed case. In general, Problems 1 and 2 are hopelessly
intractable, even to approximate, but we assume that the f1, f2,··· , fm are all monotone non-
decreasing (i.e., fi(S) ≤ fi(T ) whenever S ⊆ T ), normalized (fi(∅) = 0), and submodular [9] (i.e.,
∀S, T ⊆ V , fi(S) + fi(T ) ≥ fi(S ∪ T ) + fi(S ∩ T )). These assumptions allow us to develop fast,
simple, and scalable algorithms that have approximation guarantees, as is done in this paper. These
assumptions, moreover, allow us to retain the naturalness and applicability of Problems 1 and 2 to
a wide variety of practical problems. Submodularity is a natural property in many real-world ML
applications [20, 15, 18, 27]. When minimizing, submodularity naturally model notions of interacting
costs and complexity, while when maximizing it readily models notions of diversity, summarization
quality, and information. Hence, Problem 1 asks for a partition whose blocks each (and that
collectively) are a good, say, summary of the whole. Problem 2 on the other hand, asks for a partition
whose blocks each (and that collectively) are internally homogeneous (as is typical in clustering).
Taken together, we call Problems 1 and 2 Submodular Partitioning. We further categorize these
problems depending on if the fi’s are identical to each other (homogeneous) or not (heterogeneous).1
The heterogeneous case clearly generalizes the homogeneous setting, but as we will see, the additional
homogeneous structure can be exploited to provide more efﬁcient and/or tighter algorithms.

Problem 1 (Max-(Min+Avg))

Problem 2 (Min-(Max+Avg))

λ = 0, BINSRCH [16]
λ = 0, MATCHING [12]

λ = 1, GREEDWELFARE [8]

λ = 0, ELLIPSOID [11]
λ = 0, GREEDSAT∗
λ = 0, MMAX∗

λ = 0, GREEDMAX†∗

0 < λ < 1, COMBSFASWP∗

0 < λ < 1, GENERALGREEDSAT∗

Approximation factor

1/(2m − 1)
1/(n − m + 1)
1
4 log n log
nm

√

O(

3
2 m)

O(min

i

1/2
(1/2 − δ,
|√

|Aˆπ
i
1/m
max{ βα
¯λβ+α
λ/2

δ

1/2+δ )
1
m log3 m

)

, λβ}

λ = 0, Hardness
λ = 1, Hardness

1/2 [12]

1 − 1/e [26]

λ = 0, BALANCED† [25]
λ = 0, SAMPLING [25]

λ = 0, ELLIPSOID [11]

λ = 1, GREEDSPLIT† [29, 22]

λ = 1, RELAX [5]
λ = 0, MMIN∗

λ = 0, LOV ´ASZ ROUND∗
0 < λ < 1, COMBSLBSMP∗

0 < λ < 1, GENERALLOV ´ASZ ROUND∗

λ = 0, Hardness∗
λ = 1, Hardness

Approximation factor
min{m, n/m}
n log n)
O(

√
√

O(

n log n)
2

O(log n)
|Aπ∗

2max

|

i

i

min{ mα
m¯λ+λ

m
, β(m¯λ + λ)}
m

m

2 − 2/m [7]

Table 1: Summary of our contributions and existing work on Problems 1 and 2.2 See text for details.

Previous work: Special cases of Problems 1 and 2 have appeared previously. Problem 1 with λ = 0
is called submodular fair allocation (SFA), and Problem 2 with λ = 0 is called submodular load
balancing (SLB), robust optimization problems both of which previously have been studied. When
fi’s are all modular, SLB is called minimum makespan scheduling. An LP relaxation algorithm
provides a 2-approximation for the heterogeneous setting [19]. When the objectives are submodular,
the problem becomes much harder. Even in the homogeneous setting, [25] show that the problem
partitioning algorithm yielding a factor of min{m, n/m} under the homogeneous setting. They also

is information theoretically hard to approximate within o((cid:112)n/ log n). They provide a balanced
give a sampling-based algorithm achieving O((cid:112)n/ log n) for the homogeneous setting. However,

the sampling-based algorithm is not practical and scalable since it involves solving, in the worst-case,
O(n3 log n) instances of submodular function minimization each of which requires O(n5γ + n6)
computation [23], where γ is the cost of a function valuation. Another approach approximates
each submodular function by its ellipsoid approximation (again non-scalable) and reduces SLB
√
to its modular version (minimum makespan scheduling) leading to an approximation factor of
n log n) [11]. SFA, on the other hand, has been studied mostly in the heterogeneous setting.
O(
√
When fi’s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving
m log3 m)) approximation [2], whereas the problem is NP-hard to 1/2 +  approximate for
O(1/(
any  > 0 [12]. When fi’s are submodular, [12] gives a matching-based algorithm with a factor
1/(n − m + 1) approximation that performs poorly when m (cid:28) n. [16] proposes a binary search
algorithm yielding an improved factor of 1/(2m− 1). Similar to SLB, [11] applies the same ellipsoid

1Similar sub-categorizations have been called the “uniform” vs. the “non-uniform” case in the past [25, 11].
2Results obtained in this paper are marked as ∗. Methods for only the homogeneous setting are marked as †.

2

j ) of segment j based on the image pixels Aπ

√
nm1/4 log n log3/2 m). These approaches are
approximation techniques leading to a factor of O(
theoretically interesting, but they do not scale to large problems. Problems 1 and 2, when λ = 1, have
also been previously studied. Problem 2 becomes the submodular multiway partition (SMP) for which
one can obtain a relaxation based 2-approximation [5] in the homogeneous case. In the heterogeneous
case, the guarantee is O(log n) [6]. Similarly, [29, 22] propose a greedy splitting 2-approximation
algorithm for the homogeneous setting. Problem 1 becomes the submodular welfare [26] for which a
scalable greedy algorithm achieves a 1/2 approximation [8]. Unlike the worst case (λ = 0), many of
the algorithms proposed for these problems are scalable. The general case (0 < λ < 1) of Problems 1
and 2 differs from either of these extreme cases since we wish both for a robust (worst-case) and
average case partitioning, and controlling λ allows one to trade off between the two. As we shall see,
the ﬂexibility of a mixture can be more natural in certain applications.
Applications: There are a number of applications of submodular partitioning in ML as outlined below.
Some of these we evaluate in Section 4. Submodular functions naturally capture notions of interacting
cooperative costs and homogeneity and thus are useful for clustering and image segmentation [22,
17]. While the average case instance has been used before, a more worst-case variant (i.e., Problem 2
with λ ≈ 0) is useful to produce balanced clusterings (i.e., the submodular valuations of all the
blocks should be similar to each other). Problem 2 also addresses a problem in image segmentation,
namely how to use only submodular functions (which are instances of pseudo-Boolean functions) for
multi-label (i.e., non-Boolean) image segmentation. Problem 2 addresses this problem by allowing
each segment j to have its own submodular function fj, and the objective measures the homogeneity
j assigned to it. Moreover, by combining the average
fj(Aπ
case and the worst case objectives, one can achieve a tradeoff between the two. Empirically, we
evaluate our algorithms on unsupervised image segmentation (Section 4) and ﬁnd that it outperforms
other clustering methods including k-means, k-medoids, spectral clustering, and graph cuts.
Submodularity also accurately represents computational costs in distributed systems, as shown
in [20]. In fact, [20] considers two separate problems: 1) text data partitioning for balancing memory
demands; and 2) parameter partitioning for balancing communication costs. Both are treated by
solving an instance of SLB (Problem 2, λ = 0) where memory costs are modeled using a set-cover
submodular function and the communication costs are modeled using a modular (additive) function.
Another important ML application, evaluated in Section 4, is distributed training of statistical
models. As data set sizes grow, the need for statistical training procedures tolerant of the distributed
data partitioning becomes more important. Existing schemes are often developed and performed
assuming data samples are distributed in an arbitrary or random fashion. As an alternate strategy,
if the data is intelligently partitioned such that each block of samples can itself lead to a good
approximate solution, a consensus amongst the distributed results could be reached more quickly
than when under a poor partitioning. Submodular functions can in fact express the value of a
subset of training data for certain machine learning risk functions, e.g., [27]. Using these functions
within Problem 1, one can expect a partitioning (by formulating the problem as an instance of
Problem 1, λ ≈ 0) where each block is a good representative of the entire set, thereby achieving
faster convergence in distributed settings. We demonstrate empirically, in Section 4, that this provides
better results on several machine learning tasks, including the training of deep neural networks.
Our Contributions: In contrast to Problems 1 and 2 in the average case (i.e., λ = 1), existing
algorithms for the worst case (λ = 0) are not scalable. This paper closes this gap, by proposing
three new classes of algorithmic frameworks to solve SFA and SLB: (1) greedy algorithms; (2)
semigradient-based algorithms; and (3) a Lov´asz extension based relaxation algorithm. For SFA,
when m = 2, we formulate the problem as non-monotone submodular maximization, which can
be approximated up to a factor of 1/2 with O(n) function evaluations [4]. For general m, we give a
simple and scalable greedy algorithm (GREEDMAX), and show a factor of 1/m in the homogeneous
setting, improving the state-of-the-art factor of 1/(2m − 1) under the heterogeneous setting [16].
For the heterogeneous setting, we propose a “saturate” greedy algorithm (GREEDSAT) that iteratively
solves instances of submodular welfare problems. We show GREEDSAT has a bi-criterion guarantee
of (1/2 − δ, δ/(1/2 + δ)), which ensures at least (cid:100)m(1/2 − δ)(cid:101) blocks receive utility at least
δ/(1/2 + δ)OP T for any 0 < δ < 1/2. For SLB, we ﬁrst generalize the hardness result in [25]

and show that it is hard to approximate better than m for any m = o((cid:112)n/ log n) even in the

homogeneous setting. We then give a Lov´asz extension based relaxation algorithm (LOV ´ASZROUND)
yielding a tight factor of m for the heterogeneous setting. As far as we know, this is the ﬁrst algorithm
achieving a factor of m for SLB in this setting. For both SFA and SLB, we also obtain more efﬁcient

3

algorithms with bounded approximation factors, which we call majorization-minimization (MMIN)
and minorization-maximization (MMAX).
Next we show algorithms to handle Problems 1 and 2 with general 0 < λ < 1. We ﬁrst give two sim-
ple and generic schemes (COMBSFASWP and COMBSLBSMP), both of which efﬁciently combines an
algorithm for the worst-case problem (special case with λ = 0), and an algorithm for the average case
(special case with λ = 1) to provide a guarantee interpolating between the two bounds. For Problem 1
we generalize GREEDSAT leading to GENERALGREEDSAT, whose guarantee smoothly interpolates in
terms of λ between the bi-criterion factor by GREEDSAT in the case of λ = 0 and the constant factor of
1/2 by the greedy algorithm in the case of λ = 1. For Problem 2 we generalize LOV ´ASZROUND to ob-
tain a relaxation algorithm (GENERALLOV ´ASZROUND) that achieves an m-approximation for general
λ. The theoretical contributions and the existing work for Problems 1 and 2 are summarized in Table 1.
Lastly, we demonstrate the efﬁcacy of Problem 2 on unsupervised image segmentation, and the
success of Problem 1 to distributed machine learning, including ADMM and neural network training.

2 Robust Submodular Partitioning (Problems 1 and 2 when λ = 0)
Notation: we deﬁne f (j|S) (cid:44) f (S ∪ j) − f (S) as the gain of j ∈ V in the context of S ⊆ V . We
assume w.l.o.g. that the ground set is V = {1, 2,··· , n}.
2.1 Approximation Algorithms for SFA (Problem 1 with λ = 0)

For general m, we approach SFA from the perspective of the greedy algorithms.

We ﬁrst study approximation algorithms for SFA. When m = 2, the problem becomes maxA⊆V g(A)
where g(A) = min{f1(A), f2(V \ A)} and is submodular thanks to Theorem 2.1.
Theorem 2.1. If f1 and f2 are monotone submodular, min{f1(A), f2(V \ A)} is also submodular.
Proofs for all theorems in this paper are given in [28]. The simple bi-directional randomized greedy
algorithm [4] therefore approximates SFA with m = 2 to a factor of 1/2 matching the problem’s
hardness.
In
this work we introduce two variants of a greedy algorithm – GREEDMAX (Alg. 1) and GREEDSAT
(Alg. 2), suited to the homogeneous and heterogeneous settings, respectively.
GREEDMAX: The key idea of GREEDMAX (see Alg. 1) is to greedily add an item with the
maximum marginal gain to the block whose current solution is minimum. Initializing {Ai}m
i=1 with
the empty sets, the greedy ﬂavor also comes from that it incrementally grows the solution by greedily
improving the overall objective mini=1,...,m fi(Ai) until {Ai}m
i=1 forms a partition. Besides its
simplicity, Theorem 2.2 offers the optimality guarantee.
Theorem 2.2. GREEDMAX achieves a guarantee of 1/m under the homogeneous setting.

m min{fi(A), c} (Line 2). The parameter c controls the saturation in each block. f c

one proposed in [18] GREEDSAT deﬁnes an intermediate objective ¯F c(π) =(cid:80)m

By assuming the homogeneity of the fi’s, we obtain a very simple 1/m-approximation algorithm
improving upon the state-of-the-art factor 1/(2m − 1) [16]. Thanks to the lazy evaluation trick as
described in [21], Line 5 in Alg. 1 need not to recompute the marginal gain for every item in each
round, leading GREEDMAX to scale to large data sets.
GREEDSAT: Though simple and effective in the homogeneous setting, GREEDMAX performs
arbitrarily poorly under the heterogeneous setting. To this end we provide another algorithm –
“Saturate” Greedy (GREEDSAT, see Alg. 2). The key idea of GREEDSAT is to relax SFA to a much
simpler problem – Submodular Welfare (SWP), i.e., Problem 1 with λ = 0. Similar in ﬂavor to the
i ), where
i sat-
i (A) = 1
f c
isﬁes submodularity for each i. Unlike SFA, the combinatorial optimization problem maxπ∈Π ¯F c(π)
(Line 6) is much easier and is an instance of SWP. In this work, we solve Line 6 by the efﬁcient
greedy algorithm as described in [8] with a factor 1/2. One can also use a more computationally
expensive multi-linear relaxation algorithm as given in [26] to solve Line 6 with a tight factor
α = (1− 1/e). Setting the input argument α as the approximation factor for Line 6, the essential idea
of GREEDSAT is to perform a binary search over the parameter c to ﬁnd the largest c∗ such that the
) ≥ αc∗. GREEDSAT terminates after
returned solution ˆπc∗
solving O(log( mini fi(V )
)) instances of SWP. Theorem 2.3 gives a bi-criterion optimality guarantee.
Theorem 2.3. Given  > 0, 0 ≤ α ≤ 1 and any 0 < δ < α, GREEDSAT ﬁnds a partition such that
at least (cid:100)m(α − δ)(cid:101) blocks receive utility at least

for the instance of SWP satisﬁes ¯F c∗

i=1 f c

i (Aπ

(ˆπc∗



δ

1−α+δ (maxπ∈Π mini fi(Aπ

i ) − ).

4

i=1.

i=1, m, V , partition π0.

i ), c}.

i=1, m, V , partition π0.

for i = 1, . . . , m do

for fi.

i=1, { ˜fi}m
i }m

i=1.

i=1, m, V , α.

for i = 1, . . . , m do

Pick a subgradient hi at Aπt
i

for fi.

¯F c(π)

2 (cmax + cmin)

c = 1
ˆπc ∈ argmaxπ∈Π
if ¯F c(ˆπc) < αc then

else

cmax = c
cmin = c; ˆπ ← ˆπc

j∗ ∈ argminj f (Aj );
a∗ ∈ argmaxa∈R f (a|Aj∗ )
Aj∗ ← Aj∗ ∪ {a∗}; R ← R \ a∗

Algorithm 4: GREEDMIN
1: Input: f, m, V ;
2: Let A1 =, . . . , = Am = ∅; R = V .
3: while R (cid:54)= ∅ do
4:
5:
6:
7: end while
8: Output {Ai}m

j∗ ∈ argminj f (Aj )
a∗ ∈ mina∈R f (a|Aj∗ )
Aj∗ ← Aj∗ ∪ a∗; R ← R \ a∗

Algorithm 6: MMAX
1: Input: {fi}m
2: Let t = 0.
3: repeat
4:
5:
6:
end for
πt+1 ∈ argmaxπ∈Π mini hi(Aπ
7:
i )
8:
t = t + 1;
9: until πt = πt−1
10: Output: πt.

Algorithm 5: MMIN
1: Input: {fi}m
2: Let t = 0
3: repeat
4:
Pick a supergradient mi at Aπt
5:
i
6:
end for
πt+1 ∈ argminπ∈Π maxi mi(Aπ
7:
i )
8:
t = t + 1;
9: until πt = πt−1
10: Output: πt.

Algorithm 1: GREEDMAX
1: Input: f, m, V .
2: Let A1 =, . . . , = Am = ∅; R = V .
3: while R (cid:54)= ∅ do
4:
5:
6:
7: end while
8: Output {Ai}m
Algorithm 2: GREEDSAT
(cid:80)m
1: Input: {fi}m
i=1 min{fi(Aπ
2: Let ¯F c(π) = 1
m
3: Let cmin = 0, cmax = mini fi(V )
4: while cmax − cmin ≥  do
5:
6:
7:
8:
9:
10:
11:
end if
12: end while
13: Output: ˆπ.
Algorithm 3: LOV ´ASZ ROUND
1: Input: {fi}m
i=1, m, V .
2: Solve for {x∗
i=1 via convex relaxation.
3: Rounding: Let A1 =, . . . , = Am = ∅.
4: for j = 1, . . . , n do
i (j); Aˆi = Aˆi ∪ j
ˆi ∈ argmaxi x∗
5:
6: end for
7: Output {Ai}m
i=1.
For any 0 < δ < α Theorem 2.3 ensures that the top (cid:100)m(α − δ)(cid:101) valued blocks in the partition
returned by GREEDSAT are (δ/(1−α+δ)−)-optimal. δ controls the trade-off between the number of
top valued blocks to bound and the performance guarantee attained for these blocks. The smaller δ is,
the more top blocks are bounded, but with a weaker guarantee. We set the input argument α = 1/2 (or
α = 1 − 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical
analysis follows. However, the worst-case is often achieved only by very contrived submodular
functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution
([18] and our own observations). Setting α as the actual performance guarantee for SWP (often very
close to 1) can improve the empirical bound, and we, in practice, typically set α = 1 to good effect.
MMAX: Lastly, we introduce another algorithm for the heterogeneous setting, called minorization-
maximization (MMAX, see Alg. 6). Similar to the one proposed in [14], the idea is to iteratively
maximize tight lower bounds of the submodular functions. Submodular functions have tight modular
lower bounds, which are related to the subdifferential ∂f (Y ) of the submodular set function f at a set
Y ⊆ V [9]. Denote a subgradient at Y by hY ∈ ∂f (Y ), the extreme points of ∂f (Y ) may be com-
puted via a greedy algorithm: Let σ be a permutation of V that assigns the elements in Y to the ﬁrst
|Y | positions (σ(i) ∈ Y if and only if i ≤ |Y |). Each such permutation deﬁnes a chain with elements
0 = ∅, Sσ
Y of ∂f (Y ) has each entry
Sσ
Y forms a lower bound of f, tight at Y — i.e.,
as hσ
Y (Y ) = f (Y ). The idea of MMAX is to consider a
hσ
modular lower bound tight at the set corresponding to each block of a partition. In other words, at iter-
ation t + 1, for each block i, we approximate fi with its modular lower bound tight at Aπt
i and solve a
modular version of Problem 1 (Line 7), which admits efﬁcient approximation algorithms [2]. MMAX
is initialized with a partition π0, which is obtained by solving Problem 1, where each fi is replaced
with a simple modular function f(cid:48)
Theorem 2.4. MMAX achieves a worst-case guarantee of O(mini
ˆπ = (Aˆπ
[0, 1] is the curvature of a submodular function f at A ⊆ V .

i |−1)(1−κfi (Aˆπ
1+(|Aˆπ
i |√
i ))
), where
|Aˆπ
f (v|A\v)
m) is the partition obtained by the algorithm, and κf (A) = 1− minv∈V
f (v) ∈

i = {σ(1), σ(2), . . . , σ(i)}, and Sσ|Y | = Y . An extreme point hσ

i ) − f (Sσ
Y (j) ≤ f (X),∀X ⊆ V and hσ

i−1). Deﬁned as above, hσ

Y (X) =(cid:80)

Y (σ(i)) = f (Sσ
j∈X hσ

a∈A fi(a). The following worst-case bound holds:

i (A) =(cid:80)

1 ,··· , Aˆπ

m log3 m

5

2.2 Approximation Algorithms for SLB (Problem 2 with λ = 0)

We next investigate SLB, where existing hardness results [25] are o((cid:112)n/ log n), which is independent
of m and implicitly assumes that m = Θ((cid:112)n/ log n). However, applications for SLB are often
m = o((cid:112)n/ log n) with polynomial number of queries even under the homogeneous setting.
For the rest of the paper, we assume m = o((cid:112)n/ log n) for SLB, unless stated otherwise.

dependent on m with m (cid:28) n. We hence offer hardness analysis in terms of m in the following.
Theorem 2.5. For any  > 0, SLB cannot be approximated to a factor of (1 − )m for any

i f (Aπ(cid:48)

i ) ≤ f (V ) ≤(cid:80)

GREEDMIN: Theorem 2.5 implies that SLB is hard to approximate better than m. However, an
arbitrary partition π ∈ Π already achieves the best approximation factor of m that one can hope
i ) ≤ m maxi f (Aπ(cid:48)
for under the homogeneous setting, since maxi f (Aπ
i )
for any π(cid:48) ∈ Π. In practice, one can still implement a greedy style heuristic, which we refer to as
GREEDMIN (Alg. 4). Very similar to GREEDMAX, GREEDMIN only differs in Line 5, where the
item with the smallest marginal gain is added. Since the functions are all monotone, any additions
to a block can (if anything) only increase its value, so we choose to add to the minimum valuation
block in Line 4 to attempt to keep the maximum valuation block from growing further.
LOV ´ASZ ROUND: Next we consider the heterogeneous setting, for which we propose a tight
algorithm – LOV ´ASZ ROUND (see Alg. 3). The algorithm proceeds as follows: (1) apply the Lov´asz
extension of submodular functions to relax SLB to a convex program, which is exactly solved to
a fractional solution (Line 2); (2) map the fractional solution to a partition using the θ-rounding
technique as proposed in [13] (Line 3 - 6). The Lov´asz extension, which naturally connects a
submodular function f with its convex relaxation ˜f, is deﬁned as follows: given any x ∈ [0, 1]n,
we obtain a permutation σx by ordering its elements in non-increasing order, and thereby a chain of
j = {σx(1), . . . , σx(j)} for j = 1, . . . , n. The Lov´asz extension ˜f
sets Sσx
j−1)).

for f is the weighted sum of the ordered entries of x: ˜f (x) =(cid:80)n

0 ⊂, . . . ,⊂ Sσx

j=1 x(σx(j))(f (Sσx

j ) − f (Sσx

n with Sσx

min

Given the convexity of the ˜fi’s , SLB is relaxed to the following convex program:
xi(j) ≥ 1, for j = 1, . . . , n
(1)
m}, the θ-rounding step simply maps each
1, . . . , x∗
i (j) . The bound for LOV ´ASZ ROUND is as follows:

˜fi(xi), s.t
Denoting the optimal solution for Eqn 1 as {x∗
item j ∈ V to a block ˆi such that ˆi ∈ argmaxi x∗
Theorem 2.6. LOV ´ASZROUND achieves a worst-case approximation factor m.

x1,...,xm∈[0,1]n

max

m(cid:88)

i=1

i

j∈X\Y

j∈Y \X

j∈X\Y

j∈Y \X

We remark that, to the best of our knowledge, LOV ´ASZROUND is the ﬁrst algorithm that is tight
and that gives an approximation in terms of m for the heterogeneous setting.
MMIN: Similar to MMAX for SFA, we propose Majorization-Minimization (MMIN, see Alg. 5)
for SLB. Here, we iteratively choose modular upper bounds, which are deﬁned via superdifferentials
∂f (Y ) of a submodular function [15] at Y . Moreover, there are speciﬁc supergradients [14] that
deﬁne the following two modular upper bounds (when referring to either one, we use mf
f (j|V \j) +
mf

X,2(Y ) (cid:44) f (X) −(cid:88)

X,1(Y ) (cid:44) f (X) −(cid:88)

f (j|∅), mf

f (j|X\j) +

(cid:88)

(cid:88)

X):

f (j|X).

X,1(Y ) ≥ f (Y ) and mf

X,2(Y ) ≥ f (Y ),∀Y ⊆ V and mf

Then mf
X,2(X) = f (X). At
iteration t + 1, for each block i, MMIN replaces fi with a choice of its modular upper bound mi tight
at Aπt
i and solves a modular version of Problem 2 (Line 7), for which there exists an efﬁcient LP relax-
ation based algorithm [19]. Similar to MMAX, the initial partition π0 is obtained by solving Problem
2, where each fi is substituted with f(cid:48)
a∈A fi(a). The following worst-case bound holds:

i (A) =(cid:80)

X,1(X) = mf

Theorem 2.7. MMIN achieves a worst-case guarantee of (2 maxi
π∗ = (Aπ∗

m ) denotes the optimal partition.

1 ,··· , Aπ∗

1+(|Aπ∗

i

6

|

|Aπ∗
|−1)(1−κfi (Aπ∗

i

i

), where

))

(a)20Newsgroups

(b) MNIST

(c) TIMIT

Figure 1: Comparison between submodular and random partitions for distributed ML, including
ADMM (Fig 1a) and distributed neural nets (Fig 1b) and (Fig 1c). For the box plots, the central mark
is the median, the box edges are 25th and 75th percentiles, and the bars denote the best and worst cases.

3 General Submodular Partitioning (Problems 1 and 2 when 0 < λ < 1)
In this section we study Problem 1 and Problem 2, in the most general case, i.e., 0 < λ < 1. We ﬁrst
propose a simple and general “extremal combination” scheme that works both for problem 1 and 2.
It naturally combines an algorithm for solving the worst-case problem (λ = 0) with an algorithm for
solving the average case (λ = 1). We use Problem 1 as an example, but the same scheme easily works
for Problem 2. Denote ALGWC as the algorithm for the worst-case problem (i.e. SFA), and ALGAC
as the algorithm for the average case (i.e., SWP). The scheme is to ﬁrst obtain a partition ˆπ1 by running
ALGWC on the instance of Problem 1 with λ = 0 and a second partition ˆπ2 by running ALGAC with
λ = 1. Then we output one of ˆπ1 and ˆπ2, with which the higher valuation for Problem 1 is achieved.
We call this scheme COMBSFASWP. Suppose ALGWC solves the worst-case problem with a factor
α ≤ 1 and ALGAC for the average case with β ≤ 1. When applied to Problem 2 we refer to this
scheme as COMBSLBSMP (α ≥ 1 and β ≥ 1). The following guarantee holds for both schemes:
Theorem 3.1. For any λ ∈ (0, 1) COMBSFASWP solves Problem 1 with a factor max{ βα
in the heterogeneous case, and max{min{α, 1
m}, βα
COMBSLBSMP solves Problem 2 with a factor min{ mα
and min{m, mα
m¯λ+λ , β(m¯λ + λ)} in the homogeneous case.

¯λβ+α , λβ}
¯λβ+α , λβ} in the homogeneous case. Similarly,
m¯λ+λ , β(m¯λ + λ)} in the heterogeneous case,

(cid:80)m

j=1 fj(Aπ

λ(π) = 1
m

(cid:80)m
i=1 min{¯λfi(Aπ

The drawback of COMBSFASWP and COMBSLBSMP is that they do not explicitly exploit the trade-
off between the average-case and worst-case objectives in terms of λ. To obtain more practically
interesting algorithms, we also give GENERALGREEDSAT that generalizes GREEDSAT to solve Prob-
lem 1. Similar to GREEDSAT we deﬁne an intermediate objective: ¯F c
i )+
j ), c} in GENERALGREEDSAT. Following the same algorithmic design as in GREED-
λ 1
m
SAT, GENERALGREEDSAT only differs from GREEDSAT in Line 6, where the submodular welfare
problem is deﬁned on the new objective ¯F c
λ(π). In [28] we show that GENERALGREEDSAT gives λ/2
approximation, while also yielding a bi-criterion guarantee that generalizes Theorem 2.3. In particular
GENERALGREEDSAT recovers the bicriterion guarantee as shown in Theorem 2.3 when λ = 0. In
the case of λ = 1, GENERALGREEDSAT recovers the 1/2-approximation guarantee of the greedy
algorithm for solving the submodular welfare problem, i.e., the average-case objective. Moreover an
improved guarantee is achieved by GENERALGREEDSAT as λ increases. Details are given in [28].
To solve Problem 2 we generalize LOV ´ASZ ROUND leading to GENERALLOV ´ASZ ROUND. Similar
to LOV ´ASZ ROUND we relax each submodular objective as its convex relaxation using the Lov´asz
extension. Almost the same as LOV ´ASZ ROUND, GENERALLOV ´ASZ ROUND only differs in Line 2,
where Problem 2 is relaxed as the following convex program: minx1,...,xm∈[0,1]n ¯λ maxi
˜fi(xi) +

7

Number of iterations5 101520253035Test accuracy (%)79808182838485865-Partition on 20newsgroup with ADMMSubmodular partitionRandom partitionNumber of iterations5 101520253035Test accuracy (%)74767880828410-Partition on 20newsgroup with ADMMSubmodular partitionAdversarial partitionRandom partitionNumber of iterations5 101520Test accuracy (%)98.398.498.598.698.798.898.99999.15-Partition on MNIST with Distributed NNSubmodular partitionRandom partitionNumber of iterations5 101520Test accuracy (%)97.89898.298.498.698.89999.210-Partition on MNIST with Distributed NNSubmodular partitionRandom partitionNumber of iterations5 10152025303540455055Test accuracy (%)152025303540455030-Partition on TIMITSubmodular partitionRandom partitionNumber of iterations5 10152025303540455055Test accuracy (%)10152025303540455040-Block Partition on TIMIT with Distributed NNSubmodular partitionRandom partition(cid:80)m

˜fj(xj), s.t (cid:80)m

i=1 xi(j) ≥ 1, for j = 1, . . . , n. Following the same rounding procedure
λ 1
m
as LOV ´ASZ ROUND, GENERALLOV ´ASZ ROUND is guaranteed to give an m-approximation for
Problem 2 with general λ. Details are given in [28].

j=1

4 Experiments and Conclusions
We conclude in this section by empirically evaluating the algorithms proposed for Problems 1 and 2
on real-world data partitioning applications including distributed ADMM, distributed deep neural
network training, and lastly unsupervised image segmentation tasks.
ADMM: We ﬁrst consider data partitioning for distributed convex optimization. The evaluation
task is text categorization on the 20 Newsgroup data set, which consists of 18,774 articles divided
almost evenly across 20 classes. We formulate the multi-class classiﬁcation as an (cid:96)2 regularized
logistic regression, which is solved by ADMM implemented as [3]. We run 10 instances of random
partitioning on the training data as a baseline. In this case, we use the feature based function (same as
the one used in [27]), in the homogeneous setting of Problem 1 (with λ = 0). We use GREEDMAX
as the partitioning algorithm. In Figure 1a, we observe that the resulting partitioning performs much
better than a random partitioning (and signiﬁcantly better than an adversarial partitioning, formed
by grouping similar items together). More details are given in [28].
Distributed Deep Neural Network (DNN) Training: Next we evaluate our framework on dis-
tributed deep neural network (DNN) training. We test on two tasks: 1) handwritten digit recognition
on the MNIST database, which consists of 60,000 training and 10,000 test samples; 2) phone
classiﬁcation on the TIMIT data, which has 1,124,823 training and 112,487 test samples. A 4-layer
DNN model is applied to the MNIST experiment, and we train a 5-layer DNN for TIMIT. For both
experiments the submodular partitioning is obtained by solving the homogeneous case of Problem 1
(λ = 0) using GREEDMAX on a form of clustered facility location (as proposed and used in [27]).
We perform distributed training using an averaging stochastic gradient descent scheme, similar to the
one in [24]. We also run 10 instances of random partitioning as a baseline. As shown in Figure 1b
and 1c, the submodular partitioning outperforms the random baseline. An adversarial partitioning,
which is formed by grouping items with the same class, in either case, cannot even be trained.
Unsupervised Image Seg-
mentation: We test
the
efﬁcacy of Problem 2 on
unsupervised image seg-
mentation over the GrabCut
data set (30 color images
and their ground truth fore-
ground/background labels).
By “unsupervised”, we
mean that no labeled data
at any time in supervised
or semi-supervised training,
nor any kind of interactive
segmentation, was used in
forming or optimizing the
objective. The submodular
partitioning for each image
is obtained by solving the
homogeneous case of Problem 2 (λ = 0.8) using a modiﬁed variant of GREEDMIN on the facility lo-
cation function. We compare our method against the other unsupervised methods k-means, k-medoids,
spectral clustering, and graph cuts. Given an m-partition of an image and its ground truth labels, we
assign each of the m blocks either to the foreground or background label having the larger intersection.
In Fig. 2 we show example segmentation results after this mapping on several example images as well
as averaged F-measure (relative to ground truth) over the whole data set. More details are given in [28].
Acknowledgments: This material is based upon work supported by the National Science Foundation
under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by
a Google, a Microsoft, and an Intel research award. R. Iyer acknowledges support from a Microsoft
Research Ph.D Fellowship. This work was supported in part by TerraSwarm, one of six centers of
STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.

Figure 2: Unsupervised image segmentation (right: some examples).

8

OriginalF-measure onall of GrabCut1.00.8100.8230.8540.870GroundTruthk-meansk-medoidsSpectralClustering0.853GraphCutSubmodularPartitioningReferences
[1] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In SODA, 2007.
[2] A. Asadpour and A. Saberi. An approximation algorithm for max-min fair allocation of indivisible goods.

In SICOMP, 2010.

[3] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 2011.
[4] N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. A tight linear time (1/2)-approximation for

unconstrained submodular maximization. In FOCS, 2012.

[5] C. Chekuri and A. Ene. Approximation algorithms for submodular multiway partition. In FOCS, 2011.
[6] C. Chekuri and A. Ene. Submodular cost allocation problem and applications. In Automata, Languages

and Programming, pages 354–366. Springer, 2011.

[7] A. Ene, J. Vondr´ak, and Y. Wu. Local distribution and the symmetry gap: Approximability of multiway

partitioning problems. In SODA, 2013.

[8] M. Fisher, G. Nemhauser, and L. Wolsey. An analysis of approximations for maximizing submodular set

functions—II. In Polyhedral combinatorics, 1978.

[9] S. Fujishige. Submodular functions and optimization, volume 58. Elsevier, 2005.
[10] L. A. Garc´ıa-Escudero, A. Gordaliza, C. Matr´an, and A. Mayo-Iscar. A review of robust clustering methods.

Advances in Data Analysis and Classiﬁcation, 4(2-3):89–109, 2010.

[11] M. Goemans, N. Harvey, S. Iwata, and V. Mirrokni. Approximating submodular functions everywhere. In

SODA, 2009.

[12] D. Golovin. Max-min fair allocation of indivisible goods. Technical Report CMU-CS-05-144, 2005.
[13] R. Iyer, S. Jegelka, and J. Bilmes. Monotone closure of relaxed constraints in submodular optimization:

Connections between minimization and maximization: Extended version.

[14] R. Iyer, S. Jegelka, and J. Bilmes. Fast semidifferential based submodular function optimization. In ICML,

2013.

[15] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In

CVPR, 2011.

[16] S. Khot and A. Ponnuswami. Approximation algorithms for the max-min allocation problem. In APPROX,

2007.

[17] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? In TPAMI, 2004.
[18] A. Krause, B. McMahan, C. Guestrin, and A. Gupta. Robust submodular observation selection. In JMLR,

2008.

[19] J. K. Lenstra, D. B. Shmoys, and ´E. Tardos. Approximation algorithms for scheduling unrelated parallel

machines. In Mathematical programming, 1990.

[20] M. Li, D. Andersen, and A. Smola. Graph partitioning via parallel submodular approximation to accelerate

distributed machine learning. In arXiv preprint arXiv:1505.04636, 2015.

[21] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization

Techniques, 1978.

[22] M. Narasimhan, N. Jojic, and J. A. Bilmes. Q-clustering. In NIPS, 2005.
[23] J. Orlin. A faster strongly polynomial time algorithm for submodular function minimization. Mathematical

Programming, 2009.

[24] D. Povey, X. Zhang, and S. Khudanpur. Parallel training of deep neural networks with natural gradient and

parameter averaging. arXiv preprint arXiv:1410.7455, 2014.

[25] Z. Svitkina and L. Fleischer. Submodular approximation: Sampling-based algorithms and lower bounds.

In FOCS, 2008.

[26] J. Vondr´ak. Optimal approximation for the submodular welfare problem in the value oracle model. In

STOC, 2008.

[27] K. Wei, R. Iyer, and J. Bilmes. Submodularity in data subset selection and active learning. In ICML, 2015.
[28] K. Wei, R. Iyer, S. Wang, W. Bai, and J. Bilmes. Mixed robust/average submodular partitioning: Fast

algorithms, guarantees, and applications: NIPS 2015 Extended Supplementary.

[29] L. Zhao, H. Nagamochi, and T. Ibaraki. On generalized greedy splitting algorithms for multiway partition

problems. Discrete applied mathematics, 143(1):130–143, 2004.

9

