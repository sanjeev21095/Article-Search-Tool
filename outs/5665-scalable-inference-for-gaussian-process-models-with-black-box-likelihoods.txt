Scalable Inference for Gaussian Process Models with

Black-Box Likelihoods

Amir Dezfouli

The University of New South Wales

akdezfuli@gmail.com

Edwin V. Bonilla

The University of New South Wales

e.bonilla@unsw.edu.au

Abstract

We propose a sparse method for scalable automated variational inference (AVI) in
a large class of models with Gaussian process (GP) priors, multiple latent func-
tions, multiple outputs and non-linear likelihoods. Our approach maintains the
statistical efﬁciency property of the original AVI method, requiring only expec-
tations over univariate Gaussian distributions to approximate the posterior with a
mixture of Gaussians. Experiments on small datasets for various problems includ-
ing regression, classiﬁcation, Log Gaussian Cox processes, and warped GPs show
that our method can perform as well as the full method under high sparsity levels.
On larger experiments using the MNIST and the SARCOS datasets we show that
our method can provide superior performance to previously published scalable
approaches that have been handcrafted to speciﬁc likelihood models.

1

Introduction

Developing automated yet practical approaches to Bayesian inference is a problem that has attracted
considerable attention within the probabilisitic machine learning community (see e.g. [1, 2, 3, 4]).
In the case of models with Gaussian process (GP) priors, the main challenge is that of dealing
with a large number of highly-coupled latent variables. Although promising directions within the
sampling community such as Elliptical Slice Sampling (ESS, [5]) have been proposed, they have
been shown to be particularly slow compared to variational methods.
In particular, [6] showed
that their automated variational inference (AVI) method can provide posterior distributions that are
practically indistinguishable from those obtained by ESS, while running orders of magnitude faster.
One of the fundamental properties of the method proposed in [6] is its statistical efﬁciency, which
means that, in order to approximate a posterior distribution via the maximization of the evidence
lower bound (ELBO), it only requires expectations over univariate Gaussian distributions regardless
of the likelihood model. Remarkably, this property holds for a large class of models involving
multiple latent functions and multiple outputs. However, this method is still impractical for large
datasets as it inherits the cubic computational cost of GP models on the number of observations (N).
While there have been several approaches to large scale inference in GP models [7, 8, 9, 10, 11],
these have been focused on regression and classiﬁcation problems. The main obstacle to apply these
approaches to inference with general likelihood models is that it is unclear how they can be extended
to frameworks such as those in [6], while maintaining that desirable property of statistical efﬁciency.
In this paper we build upon the inducing-point approach underpinning most sparse approximations
to GPs [12, 13] in order to scale up the automated inference method of [6]. In particular, for models
with multiple latent functions, multiple outputs and non-linear likelihoods (such as in multi-class
classiﬁcation and Gaussian process regression networks [14]) we propose a sparse approximation
whose computational complexity is O(M 3) in time, where M (cid:28) N is the number of inducing
points. This approximation maintains the statistical efﬁciency property of the original AVI method.
As the resulting ELBO decomposes over the training data points, our method can scale up to a very

1

large number of observations and is amenable to stochastic optimization and parallel computation.
Moreover, it can, in principle, approximate arbitrary posterior distributions as it uses a Mixture-of-
Gaussians (MoG) as the family of approximate posteriors. We refer to our method as SAVIGP, which
stands for scalable automated variational inference for Gaussian process models.
Our experiments on small datasets for problems including regression, classiﬁcation, Log Gaussian
Cox processes, and warped GPs [15] show that SAVIGP can perform as well as the full method un-
der high levels of sparsity. On a larger experiment on the MNIST dataset, our approach outperforms
the distributed variational inference method in [9], who used a class-conditional density modeling
approach. Our method, unlike [9], uses a single discriminative multi-class framework. Finally, we
use SAVIGP to do inference for the Gaussian process regression network model [14] on the SAR-
COS dataset concerning an inverse robot dynamics problem [16]. We show that we can outperform
previously published scalable approaches that used likelihood-speciﬁc inference algorithms.

2 Related work

There has been a long-standing interest in the GP community to overcome the cubic scaling of
inference in standard GP models [17, 18, 12, 13, 8]. However, none of these approaches actually
dealt with the harder tasks of developing scalable inference methods for multi-output problems and
general likelihood models. The former (multiple output problem) has been addressed, notably, by
[19] and [20] using the convolution process formalism. Nevertheless, such approaches were speciﬁc
to regression problems. The latter problem (general likelihood models) has been tackled from a
sampling perspective [5] and within an optimization framework using variational inference [21].
In particular, the work of [21] proposes an efﬁcient full Gaussian posterior approximation for GP
models with iid observations. Our work pushes this breakthrough further by allowing multiple latent
functions, multiple outputs, and more importantly, scalability to large datasets.
A related area of research is that of modeling complex data with deep belief networks based on
Gaussian process mappings [22]. Unlike our approach, these models target the unsupervised prob-
lem of discovering structure in high-dimensional data, do not deal with black-box likelihoods, and
focus on small-data applications. Finally, very recent developments in speeding-up probabilistic ker-
nel machines [9, 23, 24] show that the types of problems we are addressing here are highly relevant
to the machine learning community. In particular, [23] has proposed efﬁcient inference methods for
large scale GP classiﬁcation and [9] has developed a distributed variational approach for GP models,
with a focus on regression and classiﬁcation problems. Our work, unlike these approaches, allows
practitioners and researchers to investigate new models with GP priors and complex likelihoods for
which currently there is no machinery that can scale to very large datasets.

3 Gaussian Process priors and multiple-output nonlinear likelihoods
We are given a dataset D = {xn, yn}N
n=1, where xn is a D-dimensional input vector and yn is
a P -dimensional output. Our goal is to learn the mapping from inputs to outputs, which can be
established via Q underlying latent functions {fj}Q
j=1. A sensible modeling approach to the above
problem is to assume that the Q latent functions {fj} are uncorrelated a priori and that they are
drawn from Q zero-mean Gaussian processes [25]:

p(f ) =

N (f·j; 0, Kj),
where f is the set of all latent function values; f·j = {fj(xn)}N
n=1 denotes the values of latent
function j; and Kj is the covariance matrix induced by the covariance function κj(·,·), evaluated
at every pair of inputs. Along with the prior in Equation (1), we can also assume that our multi-
dimensional observations {yn} are iid given the corresponding set of latent functions {fn}:

p(f·j) =

(1)

Q(cid:89)

j=1

Q(cid:89)

j=1

N(cid:89)

p(y|f ) =

p(yn|fn·),

(2)

where y is the set of all output observations; yn is the nth output observation; and fn· =
{fj(xn)}Q
j=1 is the set of latent function values which yn depends upon. In short, we are inter-

n=1

2

ested in models for which the following criteria are satisﬁed: (i) factorization of the prior over the
latent functions; and (ii) factorization of the conditional likelihood over the observations given the
latent functions. Interestingly, a large class of problems can be well modeled with the above assump-
tions: binary classiﬁcation [7, 26], warped GPs [15], log Gaussian Cox processes [27], multi-class
classiﬁcation [26], and multi-output regression [14] all belong to this family of models.

3.1 Automated variational inference

One of the key inference challenges in the above models is that of computing the posterior distribu-
tion over the latent functions p(f|y). Ideally, we would like an efﬁcient method that does not need to
know the details of the likelihood in order to carry out posterior inference. This is exactly the main
result in [6], which approximates the posterior with a mixture-of-Gaussians within a variational in-
ference framework. This entails the optimization of an evidence lower bound, which decomposes
as a KL-divergence term and an expected log likelihood (ELL) term. As the KL-divergence term is
relatively straightforward to deal with, we focus on their main result regarding the ELL term:
[6], Th. 1: “The expected log likelihood and its gradients can be approximated using samples from
univariate Gaussian distributions”. More generally, we say that the ELL term and its gradients can
be estimated using expectations over univariate Gaussian distributions. We refer to this result as
that of statistical efﬁciency. One of the main limitations of this method is its poor scalability to
large datasets, as it has a cubic time complexity on the number of data points, i.e. O(N 3). In the
next section we describe our inference method that scales up to large datasets while maintaining the
statistical efﬁciency property of the original model.

4 Scalable inference

p(u) =

Q(cid:89)

j=1

In order to make inference scalable we redeﬁne our prior to be sparse by conditioning the latent
j=1, which lie in the same space as {f·j} and are
processes on a set of inducing variables {u·j}Q
drawn from the same zero-mean GP priors. As before, we assume factorization of the prior across
the Q latent functions. Hence the resulting sparse prior is given by:

N (f·j; ˜µj,(cid:101)Kj),
N (u·j; 0, κ(Zj, Zj)),
(cid:101)Kj = κj(X, X) − Ajκ(Zj, X) with Aj = κ(X, Zj)κ(Zj, Zj)−1,
˜µj = κ(X, Zj)κ(Zj, Zj)−1u·j,

(4)
(5)
where u·j are the inducing variables for latent process j; u is the set of all the inducing variables; Zj
are all the inducing inputs (i.e. locations) for latent process j; X is the matrix of all input locations
{xi}; and κ(U, V) is the covariance matrix induced by evaluating the covariance function κj(·,·) at
all pairwise vectors of matrices U and V. We note that while each of the inducing variables in u·j
lies in the same space as the elements in f·j, each of the M inducing inputs in Zj lies in the same
space as each input data point xn. Given the latent function values fn·, the conditional likelihood
factorizes across data points and is given by Equation (2).

Q(cid:89)

j=1

p(f|u) =

(3)

4.1 Approximate posterior

We will approximate the posterior using variational inference. Motivated by the fact that the true
joint posterior is given by p(f , u|y) = p(f|u, y)p(u|y), our approximate posterior has the form:

q(f , u|y) = p(f|u)q(u),

(6)
where p(f|u) is the conditional prior given in Equation (3) and q(u) is our approximate (variational)
posterior. This decomposition has proved effective in problems with a single latent process and a
single output (see e.g. [13]).
Our variational distribution is a mixture of Gaussians (MoG):

q(u|λ) =

πkqk(u|mk, Sk) =

πk

N (u·j; mkj, Skj),

(7)

K(cid:88)

K(cid:88)

Q(cid:89)

k=1

k=1

j=1

3

where λ = {πk, mkj, Skj} are the variational parameters: the mixture proportions {πk}, the pos-
terior means {mkj} and posterior covariances {Skj} of the inducing variables corresponding to
mixture component k and latent function j. We also note that each of the mixture components
qk(u|mk, Sk) is a Gaussian with mean mk and block-diagonal covariance Sk.

5 Posterior approximation via optimization of the evidence lower bound

(cid:90)
(cid:124)

Following variational inference principles, the log marginal likelihood log p(y) (or evidence) is
lower bounded by the variational objective:

log p(y) ≥ Lelbo =

q(u|λ)p(f|u) log p(y|f )dfdu

−KL(q(u|λ)(cid:107)p(u))

,

(8)

(cid:123)(cid:122)

Lell

(cid:124)

(cid:125)

(cid:123)(cid:122)

Lkl

(cid:125)

where the evidence lower bound (Lelbo) decomposes as the sum of an expected log likelihood term
(Lell) and a KL-divergence term (Lkl). Our goal is to estimate our posterior distribution q(u|λ) via
maximization of Lelbo. We consider ﬁrst the Lell term, as it is the most difﬁcult to deal with since
we do not know the details of the implementation of the conditional likelihood p(y|f ).

5.1 Expected log likelihood term
Here we need to compute the expectation of the log conditional likelihood log p(y|f ) over the joint
approximate posterior given in Equation (6). Our goal is to obtain expressions for the Lell term and
its gradients wrt the variational parameters while maintaining the statistical efﬁciency property of
needing only expectations from univariate Gaussians. For this we ﬁrst introduce an intermediate
distribution q(f|λ) that is obtained by integrating out u from the joint approximate posterior:

Lell(λ) =

q(u|λ)p(f|u) log p(y|f )dfdu =

log p(y|f )

p(f|u)q(u|λ)du

f

u

f

(cid:90)

(cid:90)
(cid:124)

u

(cid:123)(cid:122)

q(f|λ)

(cid:90)

(cid:90)

Given our approximate posterior in Equation (7), q(f|λ) can be obtained analytically:

(cid:125)

df.

(9)

(10)

(11)

q(f|λ) =

πkqk(f|λk) =

N (f·j; bkj, Σkj), with

K(cid:88)

k=1

K(cid:88)

K(cid:88)

Q(cid:89)

πk

Σkj = (cid:101)Kj + AjSkjAT

k=1

j=1

N(cid:88)

K(cid:88)

where (cid:101)Kj and Aj are given in Equation (5). Now we can rewrite Equation (9) as:

bkj = Ajmkj,

j ,

Lell(λ) =

πkEqk(f|λk)[log p(y|f )] =

πkEqk(n)(fn·)[log p(yn·|fn·)],

(12)

k=1

n=1

k=1

where Eq(x)[g(x)] denotes the expectation of function g(x) over the distribution q(x). Here we have
used the mixture decomposition of q(f|λ) in Equation (10) and the factorization of the likelihood
over the data points in Equation (2). Now we are ready to state formally our main result.

Theorem 1 For the sparse GP model with prior deﬁned in Equations (3) to (5), and likelihood
deﬁned in Equation (2), the expected log likelihood over the variational distribution in Equation (7)
and its gradients can be estimated using expectations over univariate Gaussian distributions.
Given the result in Equation (12), the proof is trivial for the computation of Lell as we only need
to realize that qk(f|λk) = N (f ; bk, Σk) given in Equation (10) has a block-diagonal covariance
structure. Consequently, qk(n)(fn·) is a Q-dimensional Gaussian with diagonal covariance. For the
gradients of Lell wrt the variational parameters, we use the following identity:

∇λk

Eqk(n)(fn·)[log p(yn|fn·)] = Eqk(n)(fn·)∇λk log qk(n)(fn·) log p(yn|fn·),

(13)
(cid:4)

for λk ∈ {mk, Sk}, and the result for {πk} is straightforward.

4

Explicit computation of Lell
We now provide explicit expressions for the computation of Lell. We know that qk(n)(fn·) is a
Q-dimensional Gaussian with :

(14)
where Σk(n) is a diagonal matrix. The jth element of the mean and the (j, j)th entry of the covari-
ance are given by:

qk(n)(fn·) = N (fn·; bk(n), Σk(n)),

[Σk(n)]j,j = [(cid:101)Kj]n,n + [Aj]n,:Skj[AT

(15)
where [A]n,: and [A]:,n denote the nth row and nth column of matrix A respectively. Hence we

[bk(n)]j = [Aj]n,:mkj,

j ]:,n,

can compute Lell as follows:(cid:110)

(cid:111)S
(cid:98)Lell =

i=1

f (k,i)
n·

∼ N (fn·; bk(n), Σk(n)), k = 1, . . . , K,

N(cid:88)

K(cid:88)

S(cid:88)

n=1

k=1

i=1

1
S

πk

log p(yn·|f (k,i)

n·

).

The gradients of Lell wrt variational parameters are given in the supplementary material.

5.2 KL-divergence term

(16)

(17)

(18)

(19)

We turn now our attention to the KL-divergence term, which can be decomposed as follows:

−KL(q(u|λ)(cid:107)p(u)) = Eq[− log q(u|λ)]

+ Eq[log p(u)]

,

(cid:123)(cid:122)

Lent

(cid:125)

(cid:124)

(cid:123)(cid:122)

Lcross

(cid:125)

where the entropy term (Lent) can be lower bounded using Jensen’s inequality:
π(cid:96)N (mk; m(cid:96), Sk + S(cid:96)) def= ˆLent.

πk log

(cid:124)
K(cid:88)

(cid:96)=1

The negative cross-entropy term (Lcross) can be computed exactly:

Lent ≥ − K(cid:88)
Q(cid:88)

k=1

K(cid:88)

πk

Lcross = − 1
2

[M log 2π + log |κ(Zj, Zj)| + mT

kjκ(Zj, Zj)−1mkj + tr κ(Zj, Zj)−1Skj].
(20)
The gradients of the above terms wrt the variational parameters are given in the supplementary
material.

k=1

j=1

5.3 Hyperparameter learning and scalability to large datasets

For simplicity in the notation we have omitted the parameters of the covariance functions and the
likelihood parameters from the ELBO. However, in our experiments we optimize these along with
the variational parameters in a variational-EM alternating optimization framework. The gradients of
the ELBO wrt these parameters are given in the supplementary material.
The original framework of [6] is completely unfeasible for large datasets, as its complexity is dom-
inated by the inversion of the Gram matrix on all the training data, which is an O(N 3) operation
where N is the number of training points. Our sparse framework makes automated variational in-
ference practical for large datasets as its complexity is dominated by inversions of the kernel matrix
on the inducing points, which is an O(M 3) operation where M is the number of inducing points
per latent process. Furthermore, as the Lell and its gradients decompose over the training points, and
the Lkl term decomposes over the number of latent process, our method is amenable to stochastic
optimization and / or parallel computation, which makes it scalable to very large number of input
observations, output dimensions and latent processes. In our experiments in section 6 we show that
our sparse framework can achieve similar performance to the full method [6] on small datasets un-
der high levels of sparsity. Moreover, we carried out experiments on larger datasets for which is
practically impossible to apply the full (i.e. non-sparse) method.

5

Figure 1: The SSE and NLPD for warped GPs on the Abalone dataset, where lower values on both
measures are better. Three approximate posteriors are used: FG (full Gaussian), MoG1 (diagonal
Gaussian), and MoG2 (mixture of two diagonal Gaussians), along with various sparsity factors (SF
= M/N). The smaller the SF the sparser the model, with SF=1 corresponding to no sparsity.

6 Experiments

Our experiments ﬁrst consider the same six benchmarks with various likelihood models analyzed
by [6]. The number of training points (N) on these benchmarks ranges from 300 to 1233 and their
input dimensionality (D) ranges from 1 to 256. The goal of this ﬁrst set of experiments is to show
that SAVIGP can attain as good performance as the full method under high sparsity levels. We also
carried out experiments at a larger scale using the MNIST dataset and the SARCOS dataset [16]. The
application of the original automated variational inference framework on these datasets is unfeasible.
We refer the reader to the supplementary material for the details of our experimental set-up.
We used two performance measures in each experiment: the standardized squared error (SSE) and
the negative log predictive density (NLPD) for continuous-output problems, and the error rate and
the negative log probability (NLP) for discrete-output problems. We use three versions of SAVIGP:
FG, MoG1, and MoG2, corresponding to a full Gaussian, a diagonal Gaussian, and mixture of di-
agonal Gaussians with 2 components, respectively. We refer to the ratio of the number of inducing
points over the number of training points (M/N) as sparsity factor.

6.1 Small-scale experiments

processes

dataset

[28],

p(yn|fn)

(WGP), Abalone

In this section we describe the results on three (out of six) benchmarks used by [6] and analyze the
performance of SAVIGP. The other three benchmarks are described in the supplementary material.
Warped Gaussian
=
∇ynt(yn)N (t(yn)|fn, σ2).
For this task we used the same neural-net transformation as in
[15] and the results for the Abalone dataset are shown in Figure 1. We see that the performance of
SAVIGP is practically indistinguishable across all sparsity factors for SSE and NLPD. Here we note
that [6] showed that automated variational inference performed competitively when compared to
hand-crafted methods for warped GPs [15].
n exp(−λn)
Log Gaussian Cox process (LGCP), Coal-mining disasters dataset [29], p(yn|fn) = λyn
.
Here we used the LGCP for modeling the number of coal-mining disasters between years 1851 to
1962. We note that [6] reported that automated variational inference (the focus of this paper) pro-
duced practically indistinguishable distributions (but run order of magnitude faster) when compared
to sampling methods such as Elliptical Slice Sampling [5]. The results for our sparse models are
shown in Figure 2, where we see that both models (FG and MoG1) remain mostly unaffected when
using high levels of sparsity. We also conﬁrm the ﬁndings in [6] that the MoG1 model underestimates
the variance of the predictions.
Binary classiﬁcation, Wisconsin breast cancer dataset [28], p(yn = 1) = 1/(1 + exp(−fn)).
Classiﬁcation error rates and the negative log probability (NLP) on the Wisconsin breast cancer
dataset are shown in Figure 3. We see that the error rates are comparable across all models and
sparsity factors. Interestingly, sparser models achieved lower NLP values, suggesting overconﬁdent
predictions by the less sparse models, especially for the mixtures of diagonal Gaussians.

yn!

6

FGMoG1MoG20.250.500.751.00SSEFGMoG1MoG21234NLPDSF0.10.20.51.0Figure 2: Left: the coal-mining disasters data. Right: the posteriors for a Log Gaussian Cox process
on these data when using a full Gaussian (FG) and a diagonal Gaussian (MoG1), for various sparsity
factors (SF = M/N). The smaller the SF the sparser the model, with SF=1 corresponding to no
sparsity. The solid line is the posterior mean and the shading area includes 90% conﬁdence interval.

Figure 3: Error rates and NLP for binary classiﬁcation on the Wisconsin breast cancer dataset. Three
approximate posteriors are used: FG (full Gaussian), MoG1 (diagonal Gaussian), and MoG2 (mixture
of two diagonal Gaussians), along with various sparsity factors (SF = M/N). The smaller the SF the
sparser the model, with SF=1 corresponding to the original model without sparsity. Error bars on
the left plot indicate 95% conﬁdence interval around the mean.

6.2 Large-scale experiments

In this section we show the results of the experiments carried out on larger datasets with non-linear
non-Gaussian likelihoods.
Multi-class classiﬁcation on the MNIST dataset. We ﬁrst considered a multi-class classiﬁcation
task on the MNIST dataset using the softmax likelihood. This dataset has been extensively used by the
machine learning community and contains 50,000 examples for training, 10,000 for validation and
10,000 for testing, with 784-dimensional input vectors. Unlike most previous approaches, we did not
tune additional parameters using the validation set. Instead we used our variational framework for
learning all the model parameters using all the training and validation data. This setting most likely
provides a lower bound on test accuracy but our goal here is simply to show that we can achieve
competitive performance with highly-sparse models as our inference algorithm does not know the
details of the conditional likelihood. Figure 4 (left and middle) shows error rates and NLPs where we
see that, although the performance decreases with sparsity, the method is able to attain an accuracy
of 97.49%, while using only around 2000 inducing points (SF = 0.04).
To the best of our knowledge, we are the ﬁrst to train a multi-class Gaussian process classiﬁer using
a single discriminative probabilistic framework on all classes on MNIST. For example, [17] used a
1-vs-rest approach and [23] focused on the binary classiﬁcation task of distinguishing the odd digits
from the even digits. Finally, [9] trained one model for each digit and used it as a density model,
achieving an error rate of 5.95%. Our experiments show that by having a single discriminative
probabilistic framework, even without exploiting the details of the conditional likelihood, we can
bring this error rate down to 2.51%. As a reference, previous literature reports about 12% error
rate by linear classiﬁers and less than 1% error rate by sate-of-the-art large/deep convolutional nets.

7

0123418501875190019251950timeevent countsSF = 0.1SF = 0.2SF = 0.5SF = 1.00.20.40.60.20.40.6FGMoG118501875190019251950185018751900192519501850187519001925195018501875190019251950intensityFGMoG1MoG20.000.010.020.030.04error rateFGMoG1MoG20.10.2NLPSF0.10.20.51.0Figure 4: Left and middle: classiﬁcation error rates and negative log probabilities (NLP) for the
multi-class problem on MNIST. Here we used the FG (full Gaussian) approximation with various
sparsity factors (SF = M/N). The smaller the SF the sparser the model. Right: the SMSE for a
Gaussian process regression network model on the SARCOS dataset when learning the 4th and 7th
torques (output 1 and output 2) with a FG (full Gaussian) approximation and 0.04 sparsity factor.

Our results show that our method, while solving the harder problem of full posterior estimation, can
reduce the gap between GPs and deep nets.
Gaussian process regression networks on the SARCOS dataset. Here we apply our SAVIGP infer-
ence method to the Gaussian process regression networks (GPRNs) model of [14], using the SARCOS
dataset as a test bed. GPRNs are a very ﬂexible regression approach where P outputs are a linear
combination of Q latent Gaussian processes, with the weights of the linear combination also drawn
from Gaussian processes. This yields a non-linear multiple output likelihood model where the cor-
relations between the outputs can be spatially adaptive, i.e. input dependent. The SARCOS dataset
concerns an inverse dynamics problem of a 7-degrees-of-freedom anthropomorphic robot arm [16].
The data consists of 44,484 training examples mapping from a 21-dimensional input space (7 joint
positions, 7 joint velocities, 7 joint accelerations) to the corresponding 7 joint torques. Similarly to
the work in [10], we consider joint learning for the 4th and 7th torques, which we refer to as output
1 and output 2 respectively, and make predictions on 4,449 test points per output.
Figure 4 (right) shows the standardized mean square error (SMSE) with the full Gaussian approxi-
mation (FG) using SF=0.04, i.e. less than 2000 inducing points. The results are considerably better
than those reported by [10] (0.2631 and 0.0127 for each output respectively), although their setting
was much sparser than ours on the ﬁrst output. This also corroborates previous ﬁndings that, on this
problem, having more data does help [16]. To the best of our knowledge, we are the ﬁrst to perform
inference in GPRNs on problems at this scale.

7 Conclusion

We have presented a scalable approximate inference method for models with Gaussian process (GP)
priors, multiple outputs, and nonlinear likelihoods. One of the key properties of this method is its
statistical efﬁciency in that it requires only expectations over univariate Gaussian distributions to ap-
proximate the posterior with a mixture of Gaussians. Extensive experimental evaluation shows that
our approach can attain excellent performance under high sparsity levels and that it can outperform
previous inference methods that have been handcrafted to speciﬁc likelihood models. Overall, this
work makes a substantial contribution towards the goal of developing generic yet scalable Bayesian
inference methods for models based on Gaussian processes.

Acknowledgments

This work has been partially supported by UNSW’s Faculty of Engineering Research Grant Program
project # PS37866 and an AWS in Education Research Grant award. AD was also supported by a
grant from the Australian Research Council # DP150104878.

8

FG0.000.020.040.060.080.0010.0040.020.04SFerror rateFG0.10.20.30.40.50.0010.0040.020.04SFNLPSF = 0.040.0000.0030.0060.00912outputSMSEReferences
[1] Pedro Domingos, Stanley Kok, Hoifung Poon, Matthew Richardson, and Parag Singla. Unifying logical

and statistical AI. In AAAI, 2006.

[2] Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum.

Church: A language for generative models. In UAI, 2008.

[3] Matthew D. Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths in

Hamiltonian Monte Carlo. JMLR, 15(1):1593–1623, 2014.

[4] Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In AISTATS, 2014.
[5] Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical slice sampling. In AISTATS, 2010.
[6] Trung V. Nguyen and Edwin V. Bonilla. Automated variational inference for Gaussian process models.

In NIPS. 2014.

[7] Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary Gaussian process classiﬁca-

tion. JMLR, 9(10), 2008.

[8] James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. In UAI, 2013.
[9] Yarin Gal, Mark van der Wilk, and Carl Rasmussen. Distributed variational inference in sparse Gaussian

process regression and latent variable models. In NIPS. 2014.

[10] Trung V. Nguyen and Edwin V. Bonilla. Collaborative multi-output Gaussian processes. In UAI, 2014.
[11] Trung V. Nguyen and Edwin V. Bonilla. Fast allocation of Gaussian process experts. In ICML, 2014.
[12] Joaquin Qui˜nonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaus-

sian process regression. JMLR, 6:1939–1959, 2005.

[13] Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In AISTATS,

2009.

[14] Andrew G. Wilson, David A. Knowles, and Zoubin Ghahramani. Gaussian process regression networks.

In ICML, 2012.

[15] Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped Gaussian processes.

NIPS, 2003.

In

[16] Sethu Vijayakumar and Stefan Schaal. Locally weighted projection regression: An O(n) algorithm for

incremental real time learning in high dimensional space. In ICML, 2000.

[17] Neil D Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods: The

informative vector machine. In NIPS, 2002.

[18] Ed Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In NIPS, 2006.
[19] Mauricio A ´Alvarez and Neil D Lawrence. Computationally efﬁcient convolved multiple output Gaussian

processes. JMLR, 12(5):1459–1500, 2011.

[20] Mauricio A. ´Alvarez, David Luengo, Michalis K. Titsias, and Neil D. Lawrence. Efﬁcient multioutput

Gaussian processes through variational inducing kernels. In AISTATS, 2010.

[21] Manfred Opper and C´edric Archambeau. The variational Gaussian approximation revisited. Neural

Computation, 21(3):786–792, 2009.

[22] Andreas Damianou and Neil Lawrence. Deep Gaussian processes. In AISTATS, 2013.
[23] James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process

classiﬁcation. In AISTATS, 2015.

[24] Zichao Yang, Andrew Gordon Wilson, Alexander J. Smola, and Le Song.

kernels. In AISTATS, 2015.

`A la carte — learning fast

[25] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. The

MIT Press, 2006.

[26] Christopher K.I. Williams and David Barber. Bayesian classiﬁcation with Gaussian processes. Pattern

Analysis and Machine Intelligence, IEEE Transactions on, 20(12):1342–1351, 1998.

[27] Jesper Møller, Anne Randi Syversveen, and Rasmus Plenge Waagepetersen. Log Gaussian Cox processes.

Scandinavian journal of statistics, 25(3):451–482, 1998.

[28] K. Bache and M. Lichman. UCI machine learning repository, 2013.
[29] R.G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, 66(1):191–193, 1979.

9

