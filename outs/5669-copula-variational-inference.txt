Copula variational inference

Dustin Tran

Harvard University

David M. Blei

Columbia University

Edoardo M. Airoldi
Harvard University

Abstract

We develop a general variational inference method that preserves dependency
among the latent variables. Our method uses copulas to augment the families of
distributions used in mean-ﬁeld and structured approximations. Copulas model the
dependency that is not captured by the original variational distribution, and thus
the augmented variational family guarantees better approximations to the posterior.
With stochastic optimization, inference on the augmented distribution is scalable.
Furthermore, our strategy is generic: it can be applied to any inference procedure
that currently uses the mean-ﬁeld or structured approach. Copula variational in-
ference has many advantages: it reduces bias; it is less sensitive to local optima;
it is less sensitive to hyperparameters; and it helps characterize and interpret the
dependency among the latent variables.

1

Introduction

Variational inference is a computationally eﬃcient approach for approximating posterior distribu-
tions. The idea is to specify a tractable family of distributions of the latent variables and then to min-
imize the Kullback-Leibler divergence from it to the posterior. Combined with stochastic optimiza-
tion, variational inference can scale complex statistical models to massive data sets [9, 23, 24].
Both the computational complexity and accuracy of variational inference are controlled by the fac-
torization of the variational family. To keep optimization tractable, most algorithms use the fully-
factorized family, also known as the mean-ﬁeld family, where each latent variable is assumed inde-
pendent. Less common, structured mean-ﬁeld methods slightly relax this assumption by preserving
some of the original structure among the latent variables [19]. Factorized distributions enable eﬃ-
cient variational inference but they sacriﬁce accuracy. In the exact posterior, many latent variables
are dependent and mean-ﬁeld methods, by construction, fail to capture this dependency.
To this end, we develop copula variational inference (copula vi). Copula vi augments the traditional
variational distribution with a copula, which is a ﬂexible construction for learning dependencies in
factorized distributions [3]. This strategy has many advantages over traditional vi: it reduces bias;
it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize
and interpret the dependency among the latent variables. Variational inference has previously been
restricted to either generic inference on simple models—where dependency does not make a signif-
icant diﬀerence—or writing model-speciﬁc variational updates. Copula vi widens its applicability,
providing generic inference that ﬁnds meaningful dependencies between latent variables.
In more detail, our contributions are the following.
A generalization of the original procedure in variational inference. Copula vi generalizes vari-
ational inference for mean-ﬁeld and structured factorizations: traditional vi corresponds to running
only one step of our method. It uses coordinate descent, which monotonically decreases the KL
divergence to the posterior by alternating between ﬁtting the mean-ﬁeld parameters and the copula
parameters. Figure 1 illustrates copula vi on a toy example of ﬁtting a bivariate Gaussian.
Improving generic inference. Copula vi can be applied to any inference procedure that currently
uses the mean-ﬁeld or structured approach. Further, because it does not require speciﬁc knowledge

1

Figure 1: Approximations to an elliptical Gaussian. The mean-ﬁeld (red) is restricted to ﬁtting
independent one-dimensional Gaussians, which is the ﬁrst step in our algorithm. The second step
(blue) ﬁts a copula which models the dependency. More iterations alternate: the third reﬁts the mean-
ﬁeld (green) and the fourth reﬁts the copula (cyan), demonstrating convergence to the true posterior.

of the model, it falls into the framework of black box variational inference [15]. An investigator
need only write down a function to evaluate the model log-likelihood. The rest of the algorithm’s
calculations, such as sampling and evaluating gradients, can be placed in a library.
Richer variational approximations. In experiments, we demonstrate copula vi on the standard
example of Gaussian mixture models. We found it consistently estimates the parameters, reduces
sensitivity to local optima, and reduces sensitivity to hyperparameters. We also examine how well
copula vi captures dependencies on the latent space model [7]. Copula vi outperforms competing
methods and signiﬁcantly improves upon the mean-ﬁeld approximation.

2 Background

2.1 Variational inference

Let x be a set of observations, z be latent variables, and λ be the free parameters of a variational
distribution q(z; λ). We aim to ﬁnd the best approximation of the posterior p(z| x) using the vari-
ational distribution q(z; λ), where the quality of the approximation is measured by KL divergence.
This is equivalent to maximizing the quantity

L (λ) = Eq(z;λ)[log p(x, z)] − Eq(z;λ)[log q(z; λ)].

L(λ) is the evidence lower bound (elbo), or the variational free energy [25]. For simpler computa-
tion, a standard choice of the variational family is a mean-ﬁeld approximation

d(cid:89)

q(z; λ) =

qi(zi; λi),

i=1

where z = (z1, . . . , zd). Note this is a strong independence assumption. More sophisticated ap-
proaches, known as structured variational inference [19], attempt to restore some of the dependencies
among the latent variables.
In this work, we restore dependencies using copulas. Structured vi is typically tailored to individual
models and is diﬃcult to work with mathematically. Copulas learn general posterior dependencies
during inference, and they do not require the investigator to know such structure in advance. Further,
copulas can augment a structured factorization in order to introduce dependencies that were not
considered before; thus it generalizes the procedure. We next review copulas.

2.2 Copulas

We will augment the mean-ﬁeld distribution with a copula. We consider the variational family

q(z) =

q(zi)

c(Q(z1), . . . , Q(zd)).

(cid:34) d(cid:89)

(cid:35)

i=1

2

                                                                                                                                                                    Figure 2: Example of a vine V which factorizes a copula density of four random variables
c(u1, u2, u3, u4) into a product of 6 pair copulas. Edges in the tree Tj are the nodes of the lower level
tree Tj+1, and each edge determines a bivariate copula which is conditioned on all random variables
that its two connected nodes share.

Here Q(zi) is the marginal cumulative distribution function (CDF) of the random variable zi, and
c is a joint distribution of [0, 1] random variables.1 The distribution c is called a copula of z: it
is a joint multivariate density of Q(z1), . . . , Q(zd) with uniform marginal distributions [21]. For
any distribution, a factorization into a product of marginal densities and a copula always exists and
integrates to one [14].
Intuitively, the copula captures the information about the multivariate random variable after elimi-
nating the marginal information, i.e., by applying the probability integral transform on each variable.
The copula captures only and all of the dependencies among the zi’s. Recall that, for all random vari-
ables, Q(zi) is uniform distributed. Thus the marginals of the copula give no information.
For example, the bivariate Gaussian copula is deﬁned as

c(u1, u2; ρ) = Φρ(Φ−1(u1), Φ−1(u2)).

If u1, u2 are independent uniform distributed, the inverse CDF Φ−1 of the standard normal trans-
forms (u1, u2) to independent normals. The CDF Φρ of the bivariate Gaussian distribution, with
mean zero and Pearson correlation ρ, squashes the transformed values back to the unit square. Thus
the Gaussian copula directly correlates u1 and u2 with the Pearson correlation parameter ρ.

2.2.1 Vine copulas

It is diﬃcult to specify a copula. We must ﬁnd a family of distributions that is easy to compute with
and able to express a broad range of dependencies. Much work focuses on two-dimensional copulas,
such as the Student-t, Clayton, Gumbel, Frank, and Joe copulas [14]. However, their multivariate ex-
tensions do not ﬂexibly model dependencies in higher dimensions [4]. Rather, a successful approach
in recent literature has been by combining sets of conditional bivariate copulas; the resulting joint is
called a vine [10, 13].
A vine V factorizes a copula density c(u1, . . . , ud) into a product of conditional bivariate copulas,
also called pair copulas. This makes it easy to specify a high-dimensional copula. One need only ex-
press the dependence for each pair of random variables conditioned on a subset of the others.
Figure 2 is an example of a vine which factorizes a 4-dimensional copula into the product of 6 pair
copulas. The ﬁrst tree T1 has nodes 1, 2, 3, 4 representing the random variables u1, u2, u3, u4 respec-
tively. An edge corresponds to a pair copula, e.g., 1, 4 symbolizes c(u1, u4). Edges in T1 collapse
into nodes in the next tree T2, and edges in T2 correspond to conditional bivariate copulas, e.g.,
1, 2|3 symbolizes c(u1, u2|u3). This proceeds to the last nested tree T3, where 2, 4|13 symbolizes
1We overload the notation for the marginal CDF Q to depend on the names of the argument, though we oc-
casionally use Qi(zi) when more clarity is needed. This is analogous to the standard convention of overloading
the probability density function q(·).

3

13241;32;33;4(T1)2;31;33;41;2j31;4j3(T2)1;2j31;4j32;4j13(T3)c(u2, u4|u1, u3). The vine structure speciﬁes a complete factorization of the multivariate copula,
and each pair copula can be of a diﬀerent family with its own set of parameters:
c(u1, u2|u3)c(u1, u4|u3)

c(u1, u2, u3, u4) =
Formally, a vine is a nested set of trees V = {T1, . . . , Td−1} with the following properties:

c(u1, u3)c(u2, u3)c(u3, u4)

c(u2, u4|u1, u3)

(cid:105)(cid:104)

(cid:105)(cid:104)

(cid:104)

(cid:105)

.

1. Tree Tj = {Nj, Ej} has d + 1 − j nodes and d − j edges.
2. Edges in the jth tree Ej are the nodes in the (j + 1)th tree Nj+1.
3. Two nodes in tree Tj+1 are joined by an edge only if the corresponding edges in tree Tj

share a node.

Each edge e in the nested set of trees {T1, . . . , Td−1} speciﬁes a diﬀerent pair copula, and the product
of all edges comprise of a factorization of the copula density. Since there are a total of d(d − 1)/2
edges, V factorizes c(u1, . . . , ud) as the product of d(d − 1)/2 pair copulas.
Each edge e(i, k) ∈ Tj has a conditioning set D(e), which is a set of variable indices 1, . . . , d. We
deﬁne cik|D(e) to be the bivariate copula density for ui and uk given its conditioning set:

cik|D(e) = c

Q(ui|uj : j ∈ D(e)), Q(ui|uj : j ∈ D(e))

.

(1)

(cid:12)(cid:12)(cid:12)uj : j ∈ D(e)
(cid:17)

(cid:16)

Both the copula and the CDF’s in its arguments are conditional on D(e). A vine speciﬁes a factor-
ization of the copula, which is a product over all edges in the d − 1 levels:

c(u1, . . . , ud; η) =

cik|D(e).

(2)

d−1(cid:89)

(cid:89)

j=1

e(i,k)∈Ej

We highlight that c depends on η, the set of all parameters to the pair copulas. The vine construction
provides us with the ﬂexibility to model dependencies in high dimensions using a decomposition of
pair copulas which are easier to estimate. As we shall see, the construction also leads to eﬃcient
stochastic gradients by taking individual (and thus easy) gradients on each pair copula.

3 Copula variational inference

We now introduce copula variational inference (copula vi), our method for performing accurate and
scalable variational inference. For simplicity, consider the mean-ﬁeld factorization augmented with
a copula (we later extend to structured factorizations). The copula-augmented variational family is

q(z; λ, η) =

q(zi; λ)

c(Q(z1; λ), . . . , Q(zd; λ); η)
,

(3)

(cid:123)(cid:122)

copula

(cid:125)

(cid:34) d(cid:89)
(cid:124)

i=1

(cid:123)(cid:122)

mean-ﬁeld

(cid:35)
(cid:125)

(cid:124)

where λ denotes the mean-ﬁeld parameters and η the copula parameters. With this family, we max-
imize the augmented elbo,

L (λ, η) = Eq(z;λ,η)[log p(x, z)] − Eq(z;λ,η)[log q(z; λ, η)].

Copula vi alternates between two steps: 1) ﬁx the copula parameters η and solve for the mean-ﬁeld
parameters λ; and 2) ﬁx the mean-ﬁeld parameters λ and solve for the copula parameters η. This
generalizes the mean-ﬁeld approximation, which is the special case of initializing the copula to be
uniform and stopping after the ﬁrst step. We apply stochastic approximations [18] for each step with
gradients derived in the next section. We set the learning rate ρt ∈ R to satisfy a Robbins-Monro

schedule, i.e.,(cid:80)∞

t=1 ρt = ∞, (cid:80)∞

t < ∞. A summary is outlined in Algorithm 1.

t=1 ρ2

This alternating set of optimizations falls in the class of minorize-maximization methods, which
includes many procedures such as the EM algorithm [1], the alternating least squares algorithm, and
the iterative procedure for the generalized method of moments. Each step of copula vi monotonically
increases the objective function and therefore better approximates the posterior distribution.

4

Algorithm 1: Copula variational inference (copula vi)

Input: Data x, Model p(x, z), Variational family q.
Initialize λ randomly, η so that c is uniform.
while change in elbo is above some threshold do

// Fix η, maximize over λ.
Set iteration counter t = 1.
while not converged do

Draw sample u ∼ Unif([0, 1]d).
Update λ = λ + ρt∇λL. (Eq.5, Eq.6)
Increment t.

end
// Fix λ, maximize over η.
Set iteration counter t = 1.
while not converged do

Draw sample u ∼ Unif([0, 1]d).
Update η = η + ρt∇ηL. (Eq.7)
Increment t.

end

end
Output: Variational parameters (λ, η).

Copula vi has the same generic input requirements as black-box variational inference [15]—the user
need only specify the joint model p(x, z) in order to perform inference. Further, copula variational in-
ference easily extends to the case when the original variational family uses a structured factorization.
By the vine construction, one simply ﬁxes pair copulas corresponding to pre-existent dependence in
the factorization to be the independence copula. This enables the copula to only model dependence
where it does not already exist.
Throughout the optimization, we will assume that the tree structure and copula families are given
and ﬁxed. We note, however, that these can be learned. In our study, we learn the tree structure using
sequential tree selection [2] and learn the families, among a choice of 16 bivariate families, through
Bayesian model selection [6] (see supplement). In preliminary studies, we’ve found that re-selection
of the tree structure and copula families do not signiﬁcantly change in future iterations.

3.1 Stochastic gradients of the elbo

To perform stochastic optimization, we require stochastic gradients of the elbo with respect to both
the mean-ﬁeld and copula parameters. The copula vi objective leads to eﬃcient stochastic gradients
and with low variance.
We ﬁrst derive the gradient with respect to the mean-ﬁeld parameters. In general, we can apply the
score function estimator [15], which leads to the gradient

∇λL = Eq(z;λ,η)[∇λ log q(z; λ, η) · (log p(x, z) − log q(z; λ, η))].

(4)
We follow noisy unbiased estimates of this gradient by sampling from q(·) and evaluating the inner
expression. We apply this gradient for discrete latent variables.
When the latent variables z are diﬀerentiable, we use the reparameterization trick [17] to take ad-
vantage of ﬁrst-order information from the model, i.e.,∇z log p(x, z). Speciﬁcally, we rewrite the
expectation in terms of a random variable u such that its distribution s(u) does not depend on the
variational parameters and such that the latent variables are a deterministic function of u and the
mean-ﬁeld parameters, z = z(u; λ). Following this reparameterization, the gradients propagate

5

inside the expectation,

∇λL = Es(u)[(∇z log p(x, z) − ∇z log q(z; λ, η))∇λz(u; λ)].

(5)
This estimator reduces the variance of the stochastic gradients [17]. Furthermore, with a copula vari-
ational family, this type of reparameterization using a uniform random variable u and a deterministic
function z = z(u; λ, η) is always possible. (See the supplement.)
The reparameterized gradient (Eq.5) requires calculation of the terms ∇zi log q(z; λ, η) and
∇λiz(u; λ, η) for each i. The latter is tractable and derived in the supplement; the former decom-
poses as
∇zi log q(z; λ, η) = ∇zi log q(zi; λi) + ∇Q(zi;λi) log c(Q(z1; λ1), . . . , Q(zd; λd); η)∇ziQ(zi; λi)
(6)

= ∇zi log q(zi; λi) + q(zi; λi)

∇Q(zi;λi) log ck(cid:96)|D(e).

(cid:88)

d−1(cid:88)

j=1

e(k,(cid:96))∈Ej :
i∈{k,(cid:96)}

The summation in Eq.6 is over all pair copulas which contain Q(zi; λi) as an argument. In other
words, the gradient of a latent variable zi is evaluated over both the marginal q(zi) and all pair
copulas which model correlation between zi and any other latent variable zj. A similar derivation
holds for calculating terms in the score function estimator.
We now turn to the gradient with respect to the copula parameters. We consider copulas which are
diﬀerentiable with respect to their parameters. This enables an eﬃcient reparameterized gradient

∇ηL = Es(u)[(∇z log p(x, z) − ∇z log q(z; λ, η))∇ηz(u; λ, η)].

The requirements are the same as for the mean-ﬁeld parameters.
Finally, we note that the only requirement on the model is the gradient ∇z log p(x, z). This can
be calculated using automatic diﬀerentiation tools [22]. Thus Copula vi can be implemented in a
library and applied without requiring any manual derivations from the user.

(7)

3.2 Computational complexity
In the vine factorization of the copula, there are d(d − 1)/2 pair copulas, where d is the number of
latent variables. Thus stochastic gradients of the mean-ﬁeld parameters λ and copula parameters η
require O(d2) complexity. More generally, one can apply a low rank approximation to the copula by
truncating the number of levels in the vine (see Figure 2). This reduces the number of pair copulas
to be Kd for some K > 0, and leads to a computational complexity of O(Kd).
Using sequential tree selection for learning the vine structure [2], the most correlated variables are at
the highest level of the vines. Thus a truncated low rank copula only forgets the weakest correlations.
This generalizes low rank Gaussian approximations, which also have O(Kd) complexity [20]: it is
the special case when the mean-ﬁeld distribution is the product of independent Gaussians, and each
pair copula is a Gaussian copula.

3.3 Related work
Preserving structure in variational inference was ﬁrst studied by Saul and Jordan [19] in the case of
probabilistic neural networks. It has been revisited recently for the case of conditionally conjugate
exponential familes [8]. Our work diﬀers from this line in that we learn the dependency structure
during inference, and thus we do not require explicit knowledge of the model. Further, our augmen-
tation strategy works more broadly to any posterior distribution and any factorized variational family,
and thus it generalizes these approaches.
A similar augmentation strategy is higher-order mean-ﬁeld methods, which are a Taylor series correc-
tion based on the diﬀerence between the posterior and its mean-ﬁeld approximation [11]. Recently,
Giordano et al. [5] consider a covariance correction from the mean-ﬁeld estimates. All these methods
assume the mean-ﬁeld approximation is reliable for the Taylor series expansion to make sense, which
is not true in general and thus is not robust in a black box framework. Our approach alternates the
estimation of the mean-ﬁeld and copula, which we ﬁnd empirically leads to more robust estimates
than estimating them simultaneously, and which is less sensitive to the quality of the mean-ﬁeld
approximation.

6

Figure 3: Covariance estimates from copula variational inference (copula vi), mean-ﬁeld (mf), and
linear response variational Bayes (lrvb) to the ground truth (Gibbs samples). copula vi and lrvb
eﬀectively capture dependence while mf underestimates variance and forgets covariances.

4 Experiments

We study copula vi with two models: Gaussian mixtures and the latent space model [7]. The Gaus-
sian mixture is a classical example of a model for which it is diﬃcult to capture posterior dependen-
cies. The latent space model is a modern Bayesian model for which the mean-ﬁeld approximation
gives poor estimates of the posterior, and where modeling posterior dependencies is crucial for un-
covering patterns in the data.
There are several implementation details of copula vi. At each iteration, we form a stochastic gra-
dient by generating m samples from the variational distribution and taking the average gradient. We
set m = 1024 and follow asynchronous updates [16]. We set the step-size using ADAM [12].

4.1 Mixture of Gaussians

We follow the goal of Giordano et al. [5], which is to estimate the posterior covariance for a Gaussian
mixture. The hidden variables are a K-vector of mixture proportions π and a set of K P -dimensional
multivariate normals N (µk, Λ−1
(a P -vector) and P × P precision
matrix Λk. In a mixture of Gaussians, the joint probability is

k ), each with unknown mean µk

K(cid:89)

N(cid:89)

p(x, z, µ, Λ−1, π) = p(π)

p(µk, Λ−1
k )

p(xn | zn, µzn , Λ−1

zn

)p(zn | π),

k=1

n=1

with a Dirichlet prior p(π) and a normal-Wishart prior p(µk, Λ−1
k ).
We ﬁrst apply the mean-ﬁeld approximation (mf), which assigns independent factors to µ, π, Λ, and
z. We then perform copula vi over the copula-augmented mean-ﬁeld distribution, i.e., one which
includes pair copulas over the latent variables. We also compare our results to linear response varia-
tional Bayes (lrvb) [5], which is a posthoc correction technique for covariance estimation in varia-
tional inference. Higher-order mean-ﬁeld methods demonstrate similar behavior as lrvb. Compar-
isons to structured approximations are omitted as they require explicit factorizations and are not black
box. Standard black box variational inference [15] corresponds to the mf approximation.
We simulate 10, 000 samples with K = 2 components and P = 2 dimensional Gaussians. Figure
3 displays estimates for the standard deviations of Λ for 100 simulations, and plots them against the
ground truth using 500 eﬀective Gibb samples. The second plot displays all oﬀ-diagonal covariance
estimates. Estimates for µ and π indicate the same pattern and are given in the supplement.
When initializing at the true mean-ﬁeld parameters, both copula vi and lrvb achieve consistent
estimates of the posterior variance. mf underestimates the variance, which is a well-known limita-
tion [25]. Note that because the mf estimates are initialized at the truth, copula vi converges to the
true posterior upon one step of ﬁtting the copula. It does not require alternating more steps.

7

0.00.10.20.30.00.10.20.3Gibbs standard deviationEstimated sdmethodCVILRVBMFLambda−0.010.000.01−0.010.000.01Gibbs standard deviationEstimated sdmethodCVILRVBAll off−diagonal covariancesVariational inference methods
Mean-ﬁeld
lrvb
copula vi (2 steps)
copula vi (5 steps)
copula vi (converged)

Predictive Likelihood Runtime
15 min.
-383.2
38 min.
-330.5
32 min.
-303.2
-80.2
1 hr. 17 min.
2 hr.
-50.5

Table 1: Predictive likelihood on the latent space model. Each copula vi step either reﬁts the mean-
ﬁeld or the copula. copula vi converges in roughly 10 steps and already signiﬁcantly outperforms
both mean-ﬁeld and lrvb upon ﬁtting the copula once (2 steps).

Copula vi is more robust than lrvb. As a toy demonstration, we analyze the MNIST data set of
handwritten digits, using 12,665 training examples and 2,115 test examples of 0’s and 1’s. We per-
form "unsupervised" classiﬁcation, i.e., classify without using training labels: we apply a mixture of
Gaussians to cluster, and then classify a digit based on its membership assignment. copula vi reports
a test set error rate of 0.06, whereas lrvb ranges between 0.06 and 0.32 depending on the mean-ﬁeld
estimates. lrvb and similar higher order mean-ﬁeld methods correct an existing mf solution—it is
thus sensitive to local optima and the general quality of that solution. On the other hand, copula vi
re-adjusts both the mf and copula parameters as it ﬁts, making it more robust to initialization.

4.2 Latent space model

We next study inference on the latent space model [7], a Bernoulli latent factor model for network
analysis. Each node in an N-node network is associated with a P -dimensional latent variable z ∼
N (µ, Λ−1). Edges between pairs of nodes are observed with high probability if the nodes are close
to each other in the latent space. Formally, an edge for each pair (i, j) is observed with probability
logit(p) = θ − |zi − zj|, where θ is a model parameter.
We generate an N = 100, 000 node network with latent node attributes from a P = 10 dimensional
Gaussian. We learn the posterior of the latent attributes in order to predict the likelihood of held-out
edges. mf applies independent factors on µ, Λ, θ and z, lrvb applies a correction, and copula vi
uses the fully dependent variational distribution. Table 1 displays the likelihood of held-out edges and
runtime. We also attempted Hamiltonian Monte Carlo but it did not converge after ﬁve hours.
Copula vi dominates other methods in accuracy upon convergence, and the copula estimation with-
out reﬁtting (2 steps) already dominates lrvb in both runtime and accuracy. We note however that
lrvb requires one to invert a O(N K 3) × O(N K 3) matrix. We can better scale the method and
achieve faster estimates than copula vi if we applied stochastic approximations for the inversion.
However, copula vi always outperforms lrvb and is still fast on this 100,000 node network.

5 Conclusion

We developed copula variational inference (copula vi). copula vi is a new variational inference
algorithm that augments the mean-ﬁeld variational distribution with a copula; it captures posterior
dependencies among the latent variables. We derived a scalable and generic algorithm for performing
inference with this expressive variational distribution. We found that copula vi signiﬁcantly reduces
the bias of the mean-ﬁeld approximation, better estimates the posterior variance, and is more accurate
than other forms of capturing posterior dependency in variational approximations.

Acknowledgments

We thank Luke Bornn, Robin Gong, and Alp Kucukelbir for their insightful comments. This work
is supported by NSF IIS-0745520, IIS-1247664, IIS-1009542, ONR N00014-11-1-0651, DARPA
FA8750-14-2-0009, N66001-15-C-4032, Facebook, Adobe, Amazon, and the John Templeton Foun-
dation.

8

References
[1] Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the

EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1).

[2] Dissmann, J., Brechmann, E. C., Czado, C., and Kurowicka, D. (2012). Selecting and estimating regular

vine copulae and application to ﬁnancial returns. arXiv preprint arXiv:1202.2002.

[3] Fréchet, M. (1960). Les tableaux dont les marges sont données. Trabajos de estadística, 11(1):3–18.
[4] Genest, C., Gerber, H. U., Goovaerts, M. J., and Laeven, R. (2009). Editorial to the special issue on modeling
and measurement of multivariate risk in insurance and ﬁnance. Insurance: Mathematics and Economics,
44(2):143–145.

[5] Giordano, R., Broderick, T., and Jordan, M. I. (2015). Linear response methods for accurate covariance

estimates from mean ﬁeld variational Bayes. In Neural Information Processing Systems.

[6] Gruber, L. and Czado, C. (2015). Sequential Bayesian model selection of regular vine copulas. International

Society for Bayesian Analysis.

[7] Hoﬀ, P. D., Raftery, A. E., and Handcock, M. S. (2001). Latent space approaches to social network analysis.

Journal of the American Statistical Association, 97:1090–1098.

[8] Hoﬀman, M. D. and Blei, D. M. (2015). Structured stochastic variational inference. In Artiﬁcial Intelligence

and Statistics.

[9] Hoﬀman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference. Journal of

Machine Learning Research, 14:1303–1347.

[10] Joe, H. (1996). Families of m-variate distributions with given margins and m(m− 1)/2 bivariate depen-

dence parameters, pages 120–141. Institute of Mathematical Statistics.

[11] Kappen, H. J. and Wiegerinck, W. (2001). Second order approximations for probability models. In Neural

Information Processing Systems.

[12] Kingma, D. P. and Ba, J. L. (2015). Adam: A method for stochastic optimization.

Conference on Learning Representations.

In International

[13] Kurowicka, D. and Cooke, R. M. (2006). Uncertainty Analysis with High Dimensional Dependence Mod-

elling. Wiley, New York.

[14] Nelsen, R. B. (2006). An Introduction to Copulas (Springer Series in Statistics). Springer-Verlag New

York, Inc.

[15] Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black box variational inference. In Artiﬁcial Intelli-

gence and Statistics, pages 814–822.

[16] Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic

gradient descent. In Advances in Neural Information Processing Systems, pages 693–701.

[17] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate

inference in deep generative models. In International Conference on Machine Learning.

[18] Robbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical

Statistics, 22(3):400–407.

[19] Saul, L. and Jordan, M. I. (1995). Exploiting tractable substructures in intractable networks. In Neural

Information Processing Systems, pages 486–492.

[20] Seeger, M. (2010). Gaussian covariance and scalable variational inference. In International Conference

on Machine Learning.

[21] Sklar, A. (1959). Fonstions de répartition à n dimensions et leurs marges. Publications de l’Institut de

Statistique de l’Université de Paris, 8:229–231.

[22] Stan Development Team (2015). Stan: A C++ library for probability and sampling, version 2.8.0.
[23] Toulis, P. and Airoldi, E. M. (2014). Implicit stochastic gradient descent. arXiv preprint arXiv:1408.2923.
[24] Tran, D., Toulis, P., and Airoldi, E. M. (2015). Stochastic gradient descent methods for estimation with

large data sets. arXiv preprint arXiv:1509.06459.

[25] Wainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational

inference. Foundations and Trends in Machine Learning, 1(1-2):1–305.

9

