Monotone k-Submodular Function Maximization

with Size Constraints

Naoto Ohsaka

The University of Tokyo

ohsaka@is.s.u-tokyo.ac.jp

Yuichi Yoshida

National Institute of Informatics, and

Preferred Infrastructure, Inc.
yyoshida@nii.ac.jp

Abstract

A k-submodular function is a generalization of a submodular function, where the
input consists of k disjoint subsets, instead of a single subset, of the domain.
Many machine learning problems, including inﬂuence maximization with k kinds
of topics and sensor placement with k kinds of sensors, can be naturally modeled
as the problem of maximizing monotone k-submodular functions. In this paper,
we give constant-factor approximation algorithms for maximizing monotone k-
submodular functions subject to several size constraints. The running time of our
algorithms are almost linear in the domain size. We experimentally demonstrate
that our algorithms outperform baseline algorithms in terms of the solution quality.

1

Introduction

The task of selecting a set of items subject to constraints on the size or the cost of the set is versatile
in machine learning problems. The objective can be often modeled as maximizing a function with
the diminishing return property, where for a ﬁnite set V , a function f : 2V → R satisﬁes the
diminishing return property if

f (S ∪ {e}) − f (S) ≥ f (T ∪ {e}) − f (T )

for any S ⊆ T and e ∈ V \ T . For example, sensor placement [13, 14], inﬂuence maximization
in social networks [11], document summarization [15], and feature selection [12] involve objectives
satisfying the diminishing return property. It is well known that the diminishing return property is
equivalent to submodularity, where a function f : 2V → R is submodular if

f (S) + f (T ) ≥ f (S ∩ T ) + f (S ∪ T )

holds for any S, T ⊆ V . When the objective function is submodular and hence satisﬁes the dimin-
ishing return property, we can ﬁnd in polynomial time a solution with a provable guarantee on its
solution quality even with various constraints [2, 3, 18, 21].
In many practical applications, however, we want to select several disjoint sets of items instead of a
single set. To see this, let us describe two examples:
Inﬂuence maximization: Viral marketing is a cost-effective marketing strategy that promotes prod-
ucts by giving free (or discounted) items to a selected group of highly inﬂuential people in the hope
that, through the word-of-mouth effects, a large number of product adoptions will occur [4, 19].
Suppose that we have k kinds of items, each having a different topic and thus a different word-of-
mouth effect. Then, we want to distribute these items to B people selected from a group V of n
people so as to maximize the (expected) number of product adoptions. It is natural to impose a con-
straint that each person can receive at most one item since giving many free items to one particular
person would be unfair.
Sensor placement: There are k kinds of sensors for different measures such as temperature,
humidity, and illuminance. Suppose that we have Bi many sensors of the i-th kind for each

1

i ∈ {1, 2, . . . , k}, and there is a set V of n locations, each of which can be instrumented with
exactly one sensor. Then, we want to allocate those sensors so as to maximize the information gain.
When k = 1, these problems can be modeled as maximizing monotone submodular functions [11,
14] and admit polynomial-time (1 − 1/e)-approximation [18]. Unfortunately, however, the case of
general k cannot be modeled as maximizing submodular functions, and we cannot apply the methods
in the literature on maximizing submodular functions [2, 3, 18, 21]. We note that the problem of
selecting k disjoint sets can be sometimes modeled as maximizing monotone submodular functions
over the extended domain k × V subject to a partition matroid. Although (1 − 1/e)-approximation
algorithms are known [3, 5], the running time is around O(k8n8) and is prohibitively slow.

Our contributions: To address the problem of selecting k disjoint sets, we use the fact that the
objectives can be often modeled as k-submodular functions. Let (k + 1)V := {(X1, . . . , Xk) |
Xi ⊆ V ∀i ∈ {1, 2, . . . , k}, Xi ∩ Xj = ∅ ∀i (cid:54)= j} be the family of k disjoint sets. Then, a function
f : (k + 1)V → R is called k-submodular [9] if, for any x = (X1, . . . , Xk) and y = (Y1, . . . , Yk)
in (k + 1)V , we have

f (x) + f (y) ≥ f (x (cid:116) y) + f (x (cid:117) y)

where

x (cid:117) y := (X1 ∩ Y1, . . . , Xk ∩ Yk),
x (cid:116) y :=
Xi ∪ Yi

(cid:16)
X1 ∪ Y1 \(cid:0)(cid:91)

(cid:1), . . . , Xk ∪ Yk \(cid:0)(cid:91)

(cid:1)(cid:17)

.

Xi ∪ Yi

i(cid:54)=1

i(cid:54)=k

Roughly speaking, k-submodularity captures the property that, if we choose exactly one set Xe ∈
{X1, . . . , Xk} that an element e can belong to for each e ∈ V , then the resulting function is sub-
modular (see Section 2 for details). When k = 1, k-submodularity coincides with submodularity.
In this paper, we give approximation algorithms for maximizing non-negative monotone k-
submodular functions with several constraints on the sizes of the k sets. Here, we say that f is
monotone if f (x) ≤ f (y) for any x = (X1, . . . , Xk) and y = (Y1, . . . , Yk) with Xi ⊆ Yi for each
i ∈ {1, . . . , k}. Let n = |V | be the size of the domain. For the total size constraint, under which
the total size of the k sets is bounded by B ∈ Z+, we show that a simple greedy algorithm outputs
1/2-approximation in O(knB) time. The approximation ratio of 1/2 is asymptotically tight since
the lower bound of k+1
2k +  for any  > 0 is known even when B = n [10]. Combining the random
sampling technique [17], we also give a randomized algorithm that outputs 1/2-approximation with
probability at least 1 − δ in O(kn log B log(B/δ)) time. Hence, even when B is as large as n, the
running time is almost linear in n. For the individual size constraint, under which the size of the i-th
set is bounded by Bi ∈ Z+ for each i ∈ {1, . . . , k}, we give a 1/3-approximation algorithm with
i=1 Bi. We then give a randomized algorithm that outputs
1/3-approximation with probability at least 1 − δ in O(k2n log(B/k) log(B/δ)) time.
To show the practicality of our algorithms, we apply them to the inﬂuence maximization problem
and the sensor placement problem, and we demonstrate that they outperform previous methods based
on submodular function maximization and several baseline methods in terms of the solution quality.

running time O(knB), where B = (cid:80)k

Related work: When k = 2, k-submodularity is called bisubmodularity, and [20] applied bisub-
modular functions to machine learning problems. However, their algorithms do not have any ap-
proximation guarantee. Huber and Kolmogorov introduced k-submodularity as a generalization of
submodularity and bisubmodularity [9], and minimizing k-submodular functions was successfully
used in a computer vision application [8]. Iwata et al. [10] gave a 1/2-approximation algorithm
2k−1-approximation algorithm for maximizing non-monotone and monotone k-submodular
and a
functions, respectively, when there is no constraint.

k

Organization: The rest of this paper is organized as follows. In Section 2, we review properties
of k-submodular functions. Sections 3 and 4 are devoted to show 1/2-approximation algorithms
for the total size constraint, and 1/3-approximation algorithms for the individual size constraint,
respectively. We show our experimental results in Section 5. We conclude our paper in Section 6.

2

Algorithm 1 k-Greedy-TS
Input: a monotone k-submodular function f : (k + 1)V → R+ and an integer B ∈ Z+.
Output: a vector s with |supp(s)| = B.
1: s ← 0.
2: for j = 1 to B do
3:
4:
5: return s.

(e, i) ← arg maxe∈V \supp(s),i∈[k] ∆e,if (s).
s(e) ← i.

2 Preliminaries
For an integer k ∈ N, [k] denotes the set {1, 2, . . . , k}. We deﬁne a partial order (cid:22) on (k + 1)V so
that, for x = (X1, . . . , Xk) and y = (Y1, . . . , Yk) in (k + 1)V , x (cid:22) y if Xi ⊆ Yi for every i with
i ∈ [k]. We also deﬁne

∆e,if (x) = f (X1, . . . , Xi−1, Xi ∪ {e}, Xi+1, . . . , Xk) − f (X1, . . . , , Xk)

(cid:96)∈[k] X(cid:96), and i ∈ [k], which is the marginal gain when adding e to the
i-th set of x. Then, it is easy to see the monotonicity of f is equivalent to ∆e,if (x) ≥ 0 for any
(cid:96)∈[k] X(cid:96) and i ∈ [k]. Also it is not hard to show (see [22] for details)

that the k-submodularity of f implies the orthant submodularity, i.e.,

for x ∈ (k + 1)V , e /∈ (cid:83)
x = (X1, . . . , Xk) and e (cid:54)∈(cid:83)
for any x, y ∈ (k + 1)V with x (cid:22) y, e /∈(cid:83)
for any x ∈ (k + 1)V , e /∈(cid:83)

(cid:96)∈[k] X(cid:96), and i, j ∈ [k] with i (cid:54)= j. Actually, the converse holds:

Theorem 2.1 (Ward and ˇZivn´y [22]). A function f : (k + 1)V → R is k-submodular if and only if
f is orthant submodular and pairwise monotone.
It is often convenient to identify (k + 1)V with {0, 1 . . . , k}V to analyze k-submodular functions,
Namely, we associate (X1, . . . , Xk) ∈ (k + 1)V with x ∈ {0, 1, . . . , k}V by Xi = {e ∈ V |
x(e) = i} for i ∈ [k]. Hence we sometimes abuse notation, and simply write x = (X1, . . . , Xk)
by regarding a vector x as disjoint k sets of V . We deﬁne the support of x ∈ {0, 1, . . . , k}V as
supp(x) = {e ∈ V | x(e) (cid:54)= 0}. Analogously, for x ∈ {0, 1, . . . , k}V and i ∈ [k], we deﬁne
suppi(x) = {e ∈ V | x(e) = i}. Let 0 be the zero vector in {0, 1, . . . , k}V .

∆e,if (x) ≥ ∆e,if (y)

∆e,if (x) + ∆e,jf (x) ≥ 0

(cid:96)∈[k] Y(cid:96), and i ∈ [k], and the pairwise monotonicity, i.e.,

3 Maximizing k-submodular Functions with the Total Size Constraint

In this section, we give a 1/2-approximation algorithm to the problem of maximizing monotone
k-submodular functions subject to the total size constraint. Namely, we consider
subject to |supp(x)| ≤ B and x ∈ (k + 1)V ,

max f (x)

where f : (k + 1)V → R+ is monotone k-submodular and B ∈ Z+ is a non-negative integer.

3.1 A greedy algorithm

The ﬁrst algorithm we propose is a simple greedy algorithm (Algorithm 1). We show the following:
Theorem 3.1. Algorithm 1 outputs a 1/2-approximate solution by evaluating f O(knB) times,
where n = |V |.
The number of evaluations of f is clearly O(knB). Hence in what follows, we focus on analyzing
the approximation ratio of Algorithm 1. Our analysis is based on the framework of [10].
Consider the j-th iteration of the for loop from Line 2. Let (e(j), i(j)) ∈ V × [k] be the pair greedily
chosen in this iteration, and let s(j) be the solution after this iteration. We deﬁne s(0) = 0. Let o be

3

probability δ > 0.

Algorithm 2 k-Stochastic-Greedy-TS
Input: a monotone k-submodular function f : (k + 1)V → R+, an integer B ∈ Z+, and a failure
Output: a vector s with |supp(s)| = B.
1: s ← 0.
2: for j = 1 to B do
3: R ← a random subset of size min{ n−j+1
4:
5:
6: return s.

(e, i) ← arg maxe∈R,i∈[k] ∆e,if (s).
s(e) ← i.

δ , n} uniformly sampled from V \ supp(s).

B−j+1 log B

the optimal solution. We iteratively deﬁne o(0) = o, o(1), . . . , o(B) as follows. For each j ∈ [B], let
S(j) = supp(o(j−1)) \ supp(s(j−1)). Then, we set o(j) = e(j) if e(j) ∈ S(j), and set o(j) to be an
arbitrary element in S(j) otherwise. Then, we deﬁne o(j−1/2) as the resulting vector obtained from
o(j−1) by assigning 0 to the o(j)-th element, and then deﬁne o(j) as the resulting vector obtained
from o(j−1/2) by assigning i(j) to the e(j)-th element. Note that supp(o(j)) = B holds for every
j ∈ {0, 1, . . . , B} and o(B) = s(B) = s. Moreover, we have s(j−1) (cid:22) o(j−1/2) for every j ∈ [B].

Proof of Theorem 3.1. We ﬁrst show that, for each j ∈ [B],

f (s(j)) − f (s(j−1)) ≥ f (o(j−1)) − f (o(j)).

(1)
For each j ∈ [B], let y(j) = ∆e(j),i(j)f (s(j−1)), a(j−1/2) = ∆o(j),o(j−1)(o(j))f (o(j−1/2)), and
a(j) = ∆e(j),i(j) f (o(j−1/2)). Then, note that f (s(j))−f (s(j−1)) = y(j), and f (o(j−1))−f (o(j)) =
a(j−1/2) − a(j). From the monotonicity of f, it sufﬁces to show that y(j) ≥ a(j−1/2). Since e(j) and
i(j) are chosen greedily, we have y(j) ≥ ∆o(j),o(j−1)(o(j))f (s(j−1)). Since s(j−1) (cid:22) o(j−1/2), we
have ∆o(j),o(j−1)(o(j))f (s(j−1)) ≥ a(j−1/2) from the orthant submodularity. Combining these two
inequalities, we establish (1).
Then, we have

B(cid:88)

(f (o(j−1)) − f (o(j))) ≤ B(cid:88)

f (o) − f (s) =

(f (s(j)) − f (s(j−1))) = f (s) − f (0) ≤ f (s),

j=1

j=1

which implies f (s) ≥ f (o)/2.

3.2 An almost linear-time algorithm by random sampling

δ ),

In this section, we improve the number of evaluations of f from O(knB) to O(kn log B log B
where δ > 0 is a failure probability.
Our algorithm is shown in Algorithm 2. The main difference from Algorithm 1 is that we sample a
sufﬁciently large subset R of V , and then greedily assign a value only looking at elements in R.
We reuse notations e(j), i(j), S(j) and s(j) from Section 3.1, and let R(j) be R in the j-th iteration.
We iteratively deﬁne o(0) = o, o(1), . . . , o(B) as follows. If R(j)∩S(j) is empty, then we regard that
the algorithm failed. Suppose R(j)∩S(j) is non-empty. Then, we set o(j) = e(j) if e(j) ∈ R(j)∩S(j),
and set o(j) to be an arbitrary element in R(j) ∩ S(j) otherwise. Finally, we deﬁne o(j−1/2) and o(j)
as in Section 3.1 using o(j−1), o(j), and e(j).
If the algorithm does not fail and o(1), . . . , o(B) are well deﬁned, or in other words, if R(j) ∩ S(j) is
not empty for every j ∈ [B], then the rest of the analysis is completely the same as in Section 3.1,
and we achieve an approximation ratio of 1/2. Hence, it sufﬁces to show that o(1), . . . , o(B) are
well deﬁned with a high probability.
Lemma 3.2. With probability at least 1 − δ, we have R(j) ∩ S(j) (cid:54)= ∅ for every j ∈ [B].

4

1: s ← 0 and B ←(cid:80)

Algorithm 3 k-Greedy-IS
Input: a monotone k-submodular function f : (k + 1)V → R+ and integers B1, . . . , Bk ∈ Z+.
Output: a vector s with |suppi(s)| = Bi for each i ∈ [k].
2: for j = 1 to B do
3:
4:
5:
6: return s.

I ← {i ∈ [k] | suppi(s) < Bi}.
(e, i) ← arg maxe∈V \supp(s),i∈I ∆e,if (s).
s(e) ← i.

i∈[k] Bi.

(cid:16)

|S(j)|

(cid:17)|R(j)| ≤ e

Proof. Fix j ∈ [B]. If |R(j)| = n, then we clealy have Pr[R(j) ∩ S(j) = ∅] = 0. Otherwise we have

Pr[R(j) ∩ S(j) = ∅] =

1 −

|V \ supp(s(j−1))|
By the union bound over j ∈ [B], the lemma follows.
Theorem 3.3. Algorithm 2 outputs a 1/2-approximate solution with probability at least 1 − δ by
evaluating f at most O(k(n − B) log B log B

δ =

.

δ ) times.

− B−j+1
n−j+1

n−j+1
B−j+1 log B

δ
B

Proof. By Lemma 3.2 and the analysis in Section 3.1, Algorithm 2 outputs a 1/2-approximate
solution with probability at least 1 − δ.
The number of evaluations of f is at most

(cid:88)

k

j∈[B]

(cid:88)

n − j + 1
B − j + 1

log

B
δ

= k

n − B + j

j∈[B]

j

log

B
δ

= O

kn log B log

B
δ

4 Maximizing k-submodular Functions with the Individual Size Constraint

In this section, we consider the problem of maximizing monotone k-submodular functions subject
to the individual size constraint. Namely, we consider

max f (x)

subject to |suppi(x)| ≤ Bi ∀i ∈ [k] and x ∈ (k + 1)V ,

where f : (k + 1)V → R+ is monotone k-submodular, and B1, . . . , Bk ∈ Z+ are non-negative
integers.

4.1 A greedy algorithm

We ﬁrst consider a simple greedy algorithm described in Algorithm 3. We show the following:
Theorem 4.1. Algorithm 3 outputs a 1/3-approximate solution by evaluating f at most O(knB)
times.

It is clear that the number of evaluations of f is O(knB). The analysis of the approximation ratio is
given in Appendix A.

4.2 An almost linear-time algorithm by random sampling

We next improve the number of evaluations of f from O(knB) to O
rithm is given in Algorithm 4. In Appendix A, we show the following.
Theorem 4.2. Algorithm 4 outputs a 1/3-approximate solution with probability at least 1 − δ by
evaluating f at most O

. Our algo-

k2n log B

k2n log B

k log B

times.

(cid:16)

(cid:17)

δ

k log B

δ

(cid:16)

(cid:16)

(cid:17)

.

(cid:17)

5

failure probability δ > 0.

1: s ← 0 and B ←(cid:80)

i∈[k] Bi.

Algorithm 4 k-Stochastic-Greedy-IS
Input: a monotone k-submodular function f : (k + 1)V → R+, integers B1, . . . , Bk ∈ Z+, and a
Output: a vector s with |suppi(s)| = Bi for each i ∈ [k].
2: for j = 1 to B do
3:
4:
5:
6:
7:
8:
9:
10: return s

Add a random element in V \ (supp(s) ∪ R) to R.
(e, i) ← arg maxe∈R,i∈I ∆e,if (s).
if |R| ≥ min{ n−|suppi(s)|
Bi−|suppi(s)| log B

I ← {i ∈ [k] | suppi(s) < Bi} and R ← ∅.
loop

s(e) ← i.
break the loop.

δ , n} then

5 Experiments

In this section, we experimentally demonstrate that our algorithms outperform baseline algorithms
and our almost linear-time algorithms signiﬁcantly improve efﬁciency in practice. We conducted
experiments on a Linux server with Intel Xeon E5-2690 (2.90 GHz) and 264GB of main memory.
We implemented all algorithms in C++. We measured the computational cost in terms of the number
of function evaluations so that we can compare the efﬁciency of different methods independently
from concrete implementations.

5.1

Inﬂuence maximization with k topics under the total size constraint

We ﬁrst apply our algorithms to the problem of maximizing the spread of inﬂuence on several topics.
First we describe our information diffusion model, called the k-topic independent cascade (k-IC)
model, which generalizes the independent cascade model [6, 7]. In the k-IC model, there are k
kinds of items, each having a different topic, and thus k kinds of rumors independently spread
u,v for
through a social network. Let G = (V, E) be a social network with an edge probability pi
each edge (u, v) ∈ E, representing the strength of inﬂuence from u to v on the i-th topic. Given
a seed s ∈ (k + 1)V , for each i ∈ [k], the diffusion process of the rumor about the i-th topic
starts by activating vertices in suppi(s), independently from other topics. Then the process unfolds
in discrete steps according to the following randomizes rule: When a vertex u becomes active in
the step t for the ﬁrst time, it is given a single chance to activate each current inactive vertex v. It
succeeds with probability pi
u,v. If u succeeds, then v becomes active in the step t + 1. Whether or
not u succeeds, it cannot make any further attempt to activate v in subsequent steps. The process
runs until no more activation is possible.
The inﬂuence spread σ : (k + 1)V → R+ in the k-IC model is deﬁned as the expected total number
of vertices who eventually become active in one of the k diffusion processes given a seed s, namely,
, where Ai(suppi(s)) is a random variable representing the set of
σ(s) = E
activated vertices in the diffusion process of the i-th topic. Given a directed graph G = (V, E), edge
u,v ((u, v) ∈ E, i ∈ [k]), and a budget B, the problem is to select a seed s ∈ (k + 1)V
probabilities pi
that maximizes σ(s) subject to |supp(s)| ≤ B. It is easy to see that the inﬂuence spread function σ
is monotone k-submodular (see Appendix B for the proof).

i∈[k] Ai(suppi(s))|(cid:105)

(cid:104)|(cid:83)

Experimental settings: We use a publicly available real-world dataset of a social news website
Digg.1 This dataset consists of a directed graph where each vertex represents a user and each edge
represents the friendship between a pair of users, and a log of user votes for stories. We set the
number of topics k to be 10, and estimated edge probabilities on each topic from the log using the
method of [1]. We set the value of B to 5, 10, . . . , 100 and compared the following algorithms:

1http://www.isi.edu/˜lerman/downloads/digg2009.html

6

Figure 1: Comparison of inﬂuence spreads.

Figure 2: The number of inﬂuence estimations.

• k-Greedy-TS (Algorithm 1).
• k-Stochastic-Greedy-TS (Algorithm 2). We chose δ = 0.1.
• Single(i): Greedily choose B vertices only considering the i-th topic and assign them items
• Degree: Choose B vertices in decreasing order of degrees and assign them items of ran-
• Random: Randomly choose B vertices and assign them items of random topics.

of the i-th topic.

dom topics.

For the ﬁrst three algorithms, we implemented the lazy evaluation technique [16] for efﬁciency. For
k-Greedy-TS, we maintain an upper bound on the gain of inserting each pair (e, i) to apply the lazy
evaluation technique directly. For k-Stochastic-Greedy-TS, we maintain an upper bound on the
gain for each pair (e, i), and we pick up a pair in R with the largest gain for each iteration. During
the process of the algorithms, the inﬂuence spread was approximated by simulating the diffusion
process 100 times. When the algorithms terminate, we simulated the diffusion process 10,000 times
to obtain sufﬁciently accurate estimates of the inﬂuence spread.

Results: Figure 1 shows the inﬂuence spread achieved by each algorithm. We only show Sin-
gle(3) among Single(i) strategies since its inﬂuence spread is the largest. k-Greedy-TS and k-
Stochastic-Greedy-TS clearly outperform the other methods owing to their theoretical guarantee
on the solution quality. Note that our two methods simulated the diffusion process 100 times to
choose a seed set, which is relatively small, because of the high computation cost. Consequently,
the approximate value of the inﬂuence spread has a relatively high variance, and this might have
caused the greedy method to choose seeds with small inﬂuence spreads. Remark that Single(3)
works worse than Degree for B larger than 35, which means that focusing on a single topic may
signiﬁcantly degrade the inﬂuence spread. Random shows a poor performance as expected.
Figure 2 reports the number of inﬂuence estimations of greedy algorithms. We note that k-
Stochastic-Greedy-TS outperforms k-Greedy-TS, which implies that the random sampling tech-
nique is effective even when combined with the lazy evaluation technique. The number of evalu-
ations of k-Greedy-TS drastically increases when B is around 40 since we run out of inﬂuential
vertices and we need to reevaluate the remaining vertices. Indeed, the slope of k-Greedy-TS after
B = 40 is almost constant in Figure 1, which indicates that the remaining vertices have a similar
inﬂuence. Single(3) is faster than our algorithms since it only considers a single topic.

5.2 Sensor placement with k kinds of measures under the individual size constraint

ﬁned as H(S) = −(cid:80)

Next we apply our algorithms for maximizing k-submodular functions with the individual size
constraint to the sensor placement problem that allows multiple kinds of sensors.
In this prob-
lem, we want to determine the placement of multiple kinds of sensors that most effectively re-
duces the expected uncertainty. We need several notions to describe our model. Let Ω =
{X1, X2, . . . , Xn} be a set of discrete random variables. The entropy of a subset S of Ω is de-
s∈dom S Pr[s] log Pr[s]. The conditional entropy of Ω having observed S is
H(Ω | S) := H(Ω) − H(S). Hence, in order to reduce the uncertainty of Ω, we want to ﬁnd a set
S of as a large entropy as possible.
Now we formalize the sensor placement problem. There are k kinds of sensors for different mea-
sures. Suppose that we want to allocate Bi many sensors of the i-th kind for each i ∈ [k], and there

7

k-Greedy-TSk-Stochastic-Greedy-TSSingle(3)DegreeRandom050100150200250300350020406080100Influence SpreadBudget010000200003000040000500006000070000020406080100# of EvaluationsBudgetFigure 3: Comparison of entropy.

Figure 4: The number of entropy evaluations.

e be the
are set V of n locations, each of which can be instrumented with exactly one sensor. Let X i
random variable representing the observation collected from a sensor of the i-th kind if it is installed
e}i∈[k],e∈V . Then, the problem is to select x ∈ (k + 1)V that
at the e-th location, and let Ω = {X i
subject to |suppi(x)| ≤ Bi for each i ∈ [k]. It is easy
maximizes f (x) = H
to see that f is monotone k-submodular (see Appendix B for details).

e∈supp(x){X x(e)

(cid:16)(cid:83)

}(cid:17)

e

Experimental settings: We use the publicly available Intel Lab dataset.2 This dataset contains a
log of approximately 2.3 million readings collected from 54 sensors deployed in the Intel Berkeley
research lab between February 28th and April 5th, 2004. We extracted temperature, humidity, and
light values from each reading and discretized these values into several bins of 2 degrees Celsius
each, 5 points each, and 100 luxes each, respectively. Hence there are k = 3 kinds of sensors to be
allocated to n = 54 locations. Budgets for sensors measuring temperature, humidity, and light are
denoted by B1, B2, and B3. We set B1 = B2 = B3 = b, where b is a parameter varying from 1 to
18. We compare the following algorithms:

• k-Greedy-IS (Algorithm 3).
• k-Stochastic-Greedy-IS (Algorithm 4). We chose δ = 0.1.

• Single(i): Allocate sensors of the i-th kind to greedily chosen(cid:80)

j Bj places.

We again implemented these algorithms with the lazy evaluation technique in a similar way to the
previous experiment. Also note that Single(i) strategies do not satisfy the individual size constraint.

Results: Figure 3 shows the entropy achieved by each algorithm. k-Greedy-IS and k-Stochastic-
Greedy-IS clearly outperform Single(i) strategies. The maximum gap of entropies achieved by
k-Greedy-IS and k-Stochastic-Greedy-IS is only 0.18%.
Figure 4 shows the number of entropy evaluations of each algorithm. We observe that k-Stochastic-
Greedy-IS outperforms k-Greedy-IS. Especially when b = 18, the number of entropy evaluations
is reduced by 31%. Single(i) strategies are faster because they only consider sensors of a ﬁxed kind.

6 Conclusions

Motivated by real-world applications, we proposed approximation algorithms for maximizing mono-
tone k-submodular functions. Our algorithms run in almost linear time and achieve the approxima-
tion ratio of 1/2 for the total size constraint and 1/3 for the individual size constraint. We empir-
ically demonstrated that our algorithms outperform baseline methods for maximizing submodular
functions in terms of the solution quality. Improving the approximation ratio of 1/3 for the individ-
ual size constraint or showing it tight is an interesting open problem.

Acknowledgments

Y. Y. is supported by JSPS Grant-in-Aid for Young Scientists (B) (No. 26730009), MEXT Grant-
in-Aid for Scientiﬁc Research on Innovative Areas (24106003), and JST, ERATO, Kawarabayashi
Large Graph Project. N. O. is supported by JST, ERATO, Kawarabayashi Large Graph Project.

2http://db.csail.mit.edu/labdata/labdata.html

8

k-Greedy-ISk-Stochastic-Greedy-ISSingle(1)Single(2)Single(3) 4 5 6 7 8 9 10 11 0 2 4 6 8 10 12 14 16 18EntropyValue of b 0 200 400 600 800 1000 1200 1400 1600 1800 0 2 4 6 8 10 12 14 16 18# of EvaluationsValue of bReferences
[1] N. Barbieri, F. Bonchi, and G. Manco. Topic-aware social inﬂuence propagation models. In

ICDM, pages 81–90, 2012.

[2] N. Buchbinder, M. Feldman, J. S. Naor, and R. Schwartz. A tight linear time (1/2)-
approximation for unconstrained submodular maximization. In FOCS, pages 649–658, 2012.
[3] G. Calinescu, C. Chekuri, M. P´al, and J. Vondr´ak. Maximizing a monotone submodular func-

tion subject to a matroid constraint. SIAM Journal on Computing, 40(6):1740–1766, 2011.

[4] P. Domingos and M. Richardson. Mining the network value of customers.

57–66, 2001.

In KDD, pages

[5] Y. Filmus and J. Ward. Monotone submodular maximization over a matroid via non-oblivious

local search. SIAM Journal on Computing, 43(2):514–542, 2014.

[6] J. Goldenberg, B. Libai, and E. Muller. Talk of the network: A complex systems look at the

underlying process of word-of-mouth. Marketing Letters, 12(3):211–223, 2001.

[7] J. Goldenberg, B. Libai, and E. Muller. Using complex systems analysis to advance marketing
theory development: Modeling heterogeneity effects on new product growth through stochastic
cellular automata. Academy of Marketing Science Review, 9(3):1–18, 2001.

[8] I. Gridchyn and V. Kolmogorov. Potts model, parametric maxﬂow and k-submodular functions.

In ICCV, pages 2320–2327, 2013.

[9] A. Huber and V. Kolmogorov. Towards minimizing k-submodular functions. In Combinatorial

Optimization, pages 451–462. Springer Berlin Heidelberg, 2012.

[10] S. Iwata, S. Tanigawa, and Y. Yoshida. Improved approximation algorithms for k-submodular

function maximization. In SODA, 2016. to appear.

[11] D. Kempe, J. Kleinberg, and ´E. Tardos. Maximizing the spread of inﬂuence through a social

network. In KDD, pages 137–146, 2003.

[12] C.-W. Ko, J. Lee, and M. Queyranne. An exact algorithm for maximum entropy sampling.

Operations Research, 43(4):684–691, 1995.

[13] A. Krause, H. B. McMahon, C. Guestrin, and A. Gupta. Robust submodular observation

selection. The Journal of Machine Learning Research, 9:2761–2801, 2008.

[14] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in gaussian processes:
Theory, efﬁcient algorithms and empirical studies. The Journal of Machine Learning Research,
9:235–284, 2008.

[15] H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submod-

ular functions. In NAACL/HLT, pages 912–920, 2010.

[16] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Opti-

mization Techniques, 7:234–243, 1978.

[17] B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondr´ak, and A. Krause. Lazier than lazy

greedy. In AAAI, pages 1812–1818, 2015.

[18] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maxi-

mizing submodular set functions—I. Mathematical Programming, 14(1):265–294, 1978.
[19] M. Richardson and P. Domingos. Mining knowledge-sharing sites for viral marketing.

In

KDD, pages 61–70, 2002.

[20] A. P. Singh, A. Guillory, and J. A. Bilmes. On bisubmodular maximization. In AISTATS, pages

1055–1063, 2012.

[21] M. Sviridenko. A note on maximizing a submodular set function subject to a knapsack con-

straint. Operations Research Letters, 32(1):41–43, 2004.

[22] J. Ward and S. ˇZivn´y. Maximizing k-submodular functions and beyond. arXiv:1409.1399v1,

2014, A preliminary version appeared in SODA, pages 1468–1481, 2014.

9

